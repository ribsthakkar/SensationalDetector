Title,Text,Source,Sensationalized
Weak Federal Powers Could Limit Trump‚Äôs Climate-Policy Rollback ,"With Donald J. Trump about to take control of the White House, it would seem a dark time for the renewable energy industry. After all, Mr. Trump has mocked the science of global warming as a Chinese hoax, threatened to kill a global deal on climate change and promised to restore the coal industry to its former glory. So consider what happened in the middle of December, after investors had had a month to absorb the implications of Mr. Trump‚Äôs victory. The federal government opened bidding on a tract of the ocean floor off New York State as a potential site for a huge wind farm. Up, up and away soared the offers  ‚Äî   interest from the bidders was so fevered that the auction went through 33 rounds and spilled over to a second day. In the end, the winning bidder offered the federal Treasury $42 million, more than twice what the government got in August for oil leases  ‚Äî   oil leases  ‚Äî   in the Gulf of Mexico. Who won the bid? None other than Statoil, the Norwegian oil company, which is in the midst of a major campaign to turn itself into a big player in renewable energy. We do not know for sure that the New York wind farm will get built, but we do know this: The energy transition is real, and Mr. Trump is not going to stop it. On a global scale, more than half the investment in new electricity generation is going into renewable energy. That is more than $300 billion a year, a sign of how powerful the momentum has become. Wind power is booming in the United States, with the industry adding manufacturing jobs in the reddest states. When Mr. Trump‚Äôs appointees examine the facts, they will learn that   technician is projected to be the   occupation in America over the next decade. The election of Mr. Trump left climate activists and environmental groups in despair. They had pinned their hopes on a Hillary Clinton victory and a continuation of President Obama‚Äôs strong push to tackle global warming. Now, of course, everything is in flux. In the worst case, with a sufficiently pliant Congress, Mr. Trump could roll back a decade of progress on climate change. Barring some miraculous conversion on Mr. Trump‚Äôs part, his election cannot be interpreted as anything but bad news for the climate agenda. Yet despair might be an overreaction. For starters, when Mr. Trump gets to the White House, he will find that the federal government actually has relatively little control over American energy policy, and particularly over electricity generation. The coal industry has been ravaged in part by cheap natural gas, which is abundant because of technological changes in the way it is produced, and there is no lever in the Oval Office that Mr. Trump can pull to reverse that. The intrinsically weak federal role was a source of frustration for Mr. Obama and his aides, but now it will work to the benefit of environmental advocates. They have already persuaded more than half the states to adopt mandates on renewable energy. Efforts to roll those back have largely failed, with the latest development coming only last week, when Gov. John Kasich of Ohio, a Republican, vetoed a rollback bill. The federal government does offer important subsidies for renewable energy, and they will surely become a target in the new Congress. But those subsidies are already scheduled to fall drastically over five years, in a deal cut a year ago that gave the oil industry some favors and that passed Congress with many Republican votes. If Mr. Trump pushes for an early end to the subsidies, he will find that renewable energy has friends in the Republican Party. Topping that list is Charles E. Grassley, the senior senator from Iowa. That state  ‚Äî     in presidential politics, let us remember  ‚Äî   will soon be getting 40 percent of its electricity from wind power. ‚ÄúSenator Grassley has been and continues to be an extraordinary leader and champion for the wind industry,‚Äù said Tom Kiernan, the head of the American Wind Energy Association, a trade group. When I spoke with him last week, Mr. Kiernan did not sound like a man gnashing his teeth about the impending Trump era. By his group‚Äôs calculations, $80 billion of wind industry investment is in the pipeline for the United States over the next few years. ‚ÄúWe are creating jobs throughout America,   jobs, and we think   Trump will want that to continue,‚Äù he said. If Mr. Trump really wanted to roll back the clock, he could try to get Congress to override all the state mandates, a gross violation of the supposed conservative commitment to federalism. But it would be a titanic fight, some Republican senators would defect on principle, and Mr. Trump would almost certainly lose. So if the damage Mr. Trump can do domestically is limited by circumstance, what about the international effort against global warming? That is the prospect that has David G. Victor most worried. Dr. Victor, a professor at the University of California at San Diego, is one of the closest observers of global climate politics. While the nations of the world agreed a year ago to a landmark deal to tackle global warming, that consensus is fragile, he pointed out. The Paris Agreement is really an outline more promise than reality. Mr. Trump has vowed to withdraw. Right now, other countries are saying they will go forward even if he does so, but it is not hard to imagine the thing unraveling. As part of the negotiations, the Obama administration promised billions of dollars from American taxpayers to help poor countries adjust to the devastation of global warming. ‚ÄúThat‚Äôs a big part of the glue that held the Paris deal together,‚Äù Dr. Victor pointed out. Mr. Trump is considered likely to abandon that pledge. Perhaps the biggest threat to the climate agenda posed by the incoming administration is not anything that Mr. Trump might do, but rather what he will not do. While the energy transition is real, it is still in its earliest stages. Iowa may soon get 40 percent of its power from wind, but for the United States as a whole, the figure is closer to 5 percent. The transition is simply not happening fast enough. The pledges countries made in Paris, even if kept, are not ambitious enough. To meet the climate goals embodied in the Paris Agreement, the world needed an American president who would have pushed hard to accelerate the energy transition. You can debate whether Mrs. Clinton would have been that president, but it is certainly clear that Mr. Trump will not be. So as Washington goes into reverse gear on climate policy, seas will keep rising and heat waves will get worse. Later this month, global monitoring agencies are expected to report that 2016 was the hottest year in the historical record, beating out 2015, which beat out 2014. If nothing else, the next four years may be a fascinating test of just how far politics can become divorced from physical reality.",New York Times,0
6 Ways To Know For Sure That What You‚Äôre Drinking... | ClickHole,"Email Knowledge is power. 
1. Put a pork slice in there and see if it makes that real pork-on-cup sound: Just give it a couple shakes and listen for that unmistakable sound that can only come from cup-to-pork contact. If so, friend: You have a cup on your hands. 
2. Mail a letter to the Department of Drinking Receptacles: This government agency was started by Dwight Eisenhower in 1953 to help citizens identify what items in their homes are cups. It‚Äôs your tax dollars at work, so take advantage! 
3. Apply the helpful acronym C.U.P.: It stands for ‚ÄúCups Usually Pour,‚Äù and it just might save your ass one day. 
4. It‚Äôs big enough to fit your entire tongue into, but small enough that you can‚Äôt swim inside: Remember: You should be able to reach the liquid, but if you feel yourself drowning, you are not drinking from a real cup. 
5. It meets the biblical definition of a cup: Think back to your days in Sunday school to recall that ‚Äúwith hole in top, open to the Lord, and bottom closed to sin, such is the true cup.‚Äù Leviticus 6:19. Who said Church was useless?? 
6. It says ‚ÄúWelcome to Hawaii‚Äù on it: If you don‚Äôt know if it‚Äôs a cup or not by this point, you‚Äôre a lost cause. ",clickhole.com,1
"Blue Privilege in Action: Video Shows Cop Crashing into Cars & Fleeing Police ‚Äî No Arrest, No Charges","Home / Badge Abuse / Blue Privilege in Action: Video Shows Cop Crashing into Cars & Fleeing Police ‚Äî No Arrest, No Charges Blue Privilege in Action: Video Shows Cop Crashing into Cars & Fleeing Police ‚Äî No Arrest, No Charges Jay Syrmopoulos October 28, 2016 Leave a comment 
Oroville, CA ‚Äì In footage that looks like it came straight out of Broken Lizard‚Äôs Super Troopers, a California state parks law enforcement officer was caught on video crashing his police SUV into a power pole and parked vehicle ‚Äì then attempting to flee from responding officers . 
The incident happened just before 2p.m. on Thursday afternoon, with video showing Daniel Kenney, 43, driving a state parks vehicle into a parked car and telephone pole, according to the California Highway Patrol. 
The bizarre scene shows officers around the vehicle after the crash, with one cop eventually smashing out the passenger window. Seemingly in response to the window being broken the vehicle lunges forward, then backward, in an apparent attempt to flee from officers ‚Äì who can be heard yelling ‚Äústop‚Äù repeatedly. 
Kenney then came to a stop at the traffic lights momentarily, as police continued to command him to stop the vehicle. As an officer approached the driver side door, the vehicle abruptly takes off down the street with officers giving chase. 
According to Photography is Not a Crime : 
Daniel Kenney, who is listed as a state park peace officer supervisor, was then handed over to his department, who promptly placed him on paid administrative leave. 
Kenney, a K-9 officer who made more than $95,000 last year, was the recipient of the 2012 Top Dog Award along with his dog Kilo for ‚Äúdemonstrating the best discipline, teamwork, and overall skills.‚Äù 
It is not clear if Kilo was in the car with him as he made his getaway but the video shows the words ‚ÄúK-9 Unit, Stay Back‚Äù on the side of his patrol car. 
Meanwhile, a man sitting inside the parked vehicle that was struck by Kenney‚Äôs patrol car was also transported to the hospital with minor injuries, according to Action News Now. 
The incident was so bizarre that the individual filming can be heard wrongly making the assumption that a civilian was driving the vehicle, most likely believing the vehicle was stolen. Eventually, Kenney was pulled over and placed in handcuffs ‚Äì and after being found to not be under the influence of alcohol or drugs ‚Äì he was released to state park supervisors and taken to a hospital for evaluation. 
The fact that this man was not arrested seemingly speaks to there being a separate set of rules for law enforcement, than for the rest of society. Although perhaps there were mitigating circumstances that haven‚Äôt been made public, there is a stark appearance of impropriety. Kenney has not been arrested or charged with anything, but has been put on paid administrative leave. 
The reality is that if this had been a civilian driving the vehicle the police would likely have shot and killed the driver and claimed they feared for their lives ‚Äî as we have seen happen numerous times . 
Do you think you would receive the same treatment as this man in similar circumstances? Let us know in the comments, and please share this wild video! Share Social Trending",thefreethoughtproject.com,1
UN failed to organize evacuation of civilians from rebel-held Aleppo ‚Äì Russian envoy,"READ MORE: US-led coalition killed 300 Syrian civilians in 11 probed strikes ‚Äì Amnesty 
The ambassador added that the UN work with various opposition groups in Aleppo and the local council was ‚Äúleft to take care of itself.‚Äù He stressed that the UN personnel did not ‚Äúexert the necessary pressure‚Äù on ‚Äúsponsors‚Äù of illegal armed groups to convince them to cooperate with the aid workers on the ground. 
Besides criticizing the UN team, the Russian envoy also accused entities that have influence over fighters in besieged neighborhoods of Aleppo of not applying enough pressure on the militants to make the most of the Russian-Syrian humanitarian pause. 
‚ÄúExternal patrons of entrenched groups in eastern Aleppo could not or did not want positively influence the fighters and convince them to stop the shooting, to release civilians or leave the city themselves,‚Äù Churkin said. 
The ambassador noted that militants in Aleppo continue to get supplies and arms, including portable surface-to-air shoulder launchers (MANPADs) and missiles. 
READ MORE: No Russian, Syrian flights around Aleppo for 8 days ‚Äì Moscow 
The humanitarian pause was introduced in Aleppo on October 20, as Syrian and Russian jets halted all strikes in the vicinity of the city. While only an estimated ten percent of the city‚Äôs populace live in terrorist-held Eastern Aleppo, Moscow is doing everything possible to secure the evacuation of civilians. 
Those civilians who want to leave jihadist-held areas may use six humanitarian corridors. Fighters can also leave the city with their weapons by using two other corridors established by the Russians and the Syrians. However, terrorists have refused to leave and instead resorted to shelling the civilian escape routes. 
Russian and Syrian planes have stayed out of the city for eight consecutive days. In that time, only a few dozen civilians managed to escape the terrorist-held areas. Meanwhile, the Russian reconciliation centers continued to pour aid into Aleppo. Our colleague @ISedkeyICRC shot this video in Ramousseh, on her way to western #Aleppo . The level of destruction is staggering. pic.twitter.com/fqwNChL45E ‚Äî ICRC (@ICRC) October 26, 2016 
During the Security Council session, the UN official in charge of humanitarian aid defended the world organization‚Äôs actions in Syria, laying blame at both the rebels, Damascus, and Moscow for not allowing the UN humanitarian assistance to take place. 
‚ÄúThe United Nations were ready to launch our operations on Sunday, 23 October. However, objections by two non-State armed opposition groups, namely Ahrar as-Sham and Nureddin Zenki, scuppered these plans. The United Nations made every effort to get assurances from all parties, only for the parties to then fail to agree on each other‚Äôs conditions about how evacuations should proceed,‚Äù said Under-Secretary General for Humanitarian Affairs and Emergency Relief Coordinator, Stepen O‚ÄôBrien. 
In the meantime, the International Committee of the Red Cross and the Red Crescent teams working in Aleppo have complained that delivering the humanitarian aid and treating the wounded has been a challenge, as the ICRC failed to ‚Äúsecure the security guarantees of some armed groups.‚Äù ŸÖÿ≠ÿßŸàŸÑÿßÿ™ ÿ•ÿ¨ŸÑÿßÿ° ÿßŸÑÿ¨ÿ±ÿ≠Ÿâ ŸàÿßŸÑŸÖÿ±ÿ∂Ÿâ ŸÖŸÜ #ÿ≠ŸÑÿ® ŸÑŸÖ ÿ™ŸÜÿ¨ÿ≠. Ÿáÿ∞ÿß ŸÖÿß ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ£ŸÜ ÿ™ÿπÿ±ŸÅŸá ÿπŸÜ ÿßŸÑÿ£ŸÖÿ±: pic.twitter.com/hsdfmqLVRK ‚Äî ÿßŸÑŸÑÿ¨ŸÜÿ© ÿßŸÑÿØŸàŸÑŸäÿ© (@ICRC_ar) October 26, 2016 
Back at the UNSC, O'Brien painted a clear picture for the members of the UN Security Council of human suffering in Eastern Aleppo where terrorists use civilians as human shields. 
In a graphic yet poetic account, O‚ÄôBrien said that civilians ‚Äì mostly children and elderly ‚Äì are stuck in basements where ‚Äúthe stench of urine and the vomit caused by unrelieved fear never leaving your nostrils‚Äù is omnipresent. Read more No Russian, Syrian flights around Aleppo for 8 days ‚Äì Moscow 
‚ÄúOr scrabbling with your bare hands in the street above to reach under concrete rubble, lethal steel reinforcing bars jutting at you as you hysterically try to reach your young child screaming unseen in the dust and dirt below your feet, you choking to catch your breath in the toxic dust and the smell of gas ever-ready to ignite and explode over you.‚Äù 
‚ÄúThese are constant, harrowing reports and images of people detained, tortured, forcibly displaced, maimed and executed,‚Äù O‚ÄôBrien added. 
While mentioning the destructive role of terrorist on the ground, the UN envoy to Syria went out of his way to blame Damascus and Moscow for their air raids. 
‚ÄúAleppo has essentially become a kill zone. Since my last report to this Council less than a month ago, 400 more people have been killed and nearly 2,000 injured in eastern Aleppo. So many of them ‚Äì too many of them ‚Äì were children,‚Äù O'Brien said. 
‚ÄúNever has the phrase by poet Robert Burns, of ‚ÄòMan‚Äôs inhumanity to man‚Äô been as apt. It can be stopped but you the Security Council have to choose to make it stop,‚Äù the envoy added. 
Taking the mic at the UNSC meeting, Churkin criticized O'Brien‚Äôs report, which he said lacked factual information and failed to stress the cessation of Syrian and Russian air raids on the city. He asked O‚ÄôBrian not to recite poetry but base his reports on concrete facts. 
‚ÄúIf we wanted to hear a sermon, we would go to church. If we wanted to hear poetry, we would go to a theater,‚Äù Churkin said. 
READ MORE: 60 civilians killed, 200 injured as US-led coalition strikes Mosul residential areas ‚Äì Russian MoD 
Security Council members wanted to hear ‚Äúobjective analysis‚Äù of the situation on the ground from O‚ÄôBrien, the Russian ambassador stressed. 
‚ÄúYou clearly did not achieve this,‚Äù Churkin said, reminding O'Brien that no strikes have been conducted over Aleppo since October 18. Calling O'Brian's statement ‚Äúprovocative and unacceptable,‚Äù Churkin pointed that in the past eight days Syrian and Russian planes had not flown over Aleppo, staying at least 10 km away from the city. 
‚ÄúThis moratorium on the flight lasted eight days [now]. Mr. O 'Brian, you did not mention a single word about it. You have built your speech so to paint a picture that aerial bombardment did not stop for one day and that it is happening now, as we speak,‚Äù said Churkin.",rt.com,1
"First, a Mixtape. Then a Romance.","Just how   is Hillary Kerr, the    founder of a   digital media company in Los Angeles? She can tell you what song was playing five years ago on the jukebox at the bar where she somewhat randomly met the man who became her husband. It was ‚ÄúThese Days,‚Äù the version sung by Nico, the German   made famous by Andy Warhol and the Velvet Underground. Actually, the song had been playing just before she met Jonathan Leahy, now 38, on that December night in 2011 at the 4100 Bar in the Silver Lake district of Los Angeles. Ms. Kerr can‚Äôt remember exactly what was playing when they met because at that moment she was jumping up and down ‚Äúlike Tigger,‚Äù as she put it. In answering the usual     questions, Mr. Leahy told her he was a music supervisor for ‚ÄúGirls,‚Äù the HBO show created by and starring Lena Dunham. That was enough to get Ms. Kerr bouncing. ‚ÄúYour music has changed my life!‚Äù she told Mr. Leahy. Mr. Leahy, who is quiet but not shy (at least he doesn‚Äôt jump up and down upon meeting people) was mesmerized. ‚ÄúMy main reaction,‚Äù he said, ‚Äúwas it‚Äôs a lot easier to talk to beautiful women in a bar when you‚Äôre working on a hit show. ‚Äù They exchanged email addresses, more an act of politeness than promise. Then their soundtrack went quiet for almost a year. Both Mr. Leahy and Ms. Kerr had active social lives, but they were focused on their careers. Mr. Leahy, who grew up in Laconia, N. H. graduated from the College of William and Mary in Williamsburg, Va. in 2000 and landed in Los Angeles later that year. Now he is a music supervisor with Aperture Music and is joined by Manish Raval and Tom Wolfe in being responsible for the music on ‚ÄúGirls. ‚Äù The team has also worked on films including ‚ÄúTrainwreck‚Äù and television series such as ‚ÄúNew Girl. ‚Äù In addition, Mr. Leahy is the music supervisor for ‚ÄúSurvivor‚Äôs Remorse‚Äù on Starz. Ms. Kerr grew up in the La Jolla section of San Diego, graduated from the University of Southern California in 2000 and eventually made her way to New York, where she took a job as an assistant at Elle magazine. In 2005 she moved back to the West Coast, to Los Angeles, and with a fellow Elle alum,  Katherine Power, created a company best known for its website, Who What Wear, which casts an eye on celebrity culture and fashion it now has 13 million monthly unique visitors. Ms. Kerr and Ms. Power also started the Who What Wear clothing and accessories line sold at Target. It was nine months after their initial meeting that Mr. Leahy emailed Ms. Kerr. He had a friend who wanted to get into the fashion industry. Ms. Kerr, Mr. Leahy and his friend met for a long, boozy brunch. They began to email and text a bit. ‚ÄúThere was banter,‚Äù Ms. Kerr said, but neither knew the interest or intention of the other. A few months later, she texted to ask if he could help her score a ticket to see the band Lord Huron. Mr. Leahy happens to be a friend of Ben Schneider, the band‚Äôs lead singer, and had an extra ticket. ‚ÄúThis was one of those moments where the universe conspires to make you seem cooler than you actually are,‚Äù Mr. Leahy said. He and Ms. Kerr met up at the show. That is when Mr. Leahy and Ms. Kerr moved into the ambiguous ‚Äúmixtape era,‚Äù in which for months they emailed and texted each other with coy ‚Äúare we just friends or what‚Äù texts revolving around music. For example, Ms. Kerr was visiting New York and texted Mr. Leahy a request for ‚Äúwalking around SoHo music. ‚Äù He sent her a link to ‚ÄúLove Me Again,‚Äù by John Newman. It has a club vibe but romantic lyrics. ‚ÄúI wanted to read into it,‚Äù Ms. Kerr said, but she figured (correctly, it turns out) that her new friend was a bit of a clueless guy who didn‚Äôt spend much time thinking about how a woman might react to such a song being shared with her. Another time he emailed her a link to a Fleetwood Mac version of ‚ÄúNeed Your Love So Bad. ‚Äù After listening to it, Ms. Kerr said, ‚ÄúI called my friend Katie. ‚Äù ‚ÄúAt that time,‚Äù she continued, ‚ÄúI just referred to him as ‚Äòthe supervisor.‚Äô She knew I had a crush on him. She said, ‚ÄòHow can it not mean something? ‚Äô‚Äù Mr. Leahy acknowledged that it might be difficult for a person to think he was not sending Ms. Kerr a message with this song. ‚ÄúI sort of thought, ‚ÄòMaybe it‚Äôs too much. ‚Äô‚Äù But he shared it with her anyway. (This is the same man who sent her the song ‚ÄúBedBedBedBedBed, Vacationer Remix,‚Äù by Deleted Scenes, ‚Äúduring the friend phase,‚Äù Ms. Kerr said.) Ms. Kerr played the game, too. She made Mr. Leahy a mix CD (handwritten liner notes and all) that she titled ‚ÄúFeynman Diagrams for All,‚Äù after Mr. Leahy told her in a text conversation that he thought the idea of Feynman diagrams  ‚Äî   in which physicists map out the interactions of subatomic particles  ‚Äî   was romantic. On the mix, Ms. Kerr included the Mazzy Star song ‚ÄúI‚Äôve Been Let Down. ‚Äù It was ‚Äúa bit of an Easter egg of my actual feelings,‚Äù she said. Around this time, Ms. Kerr texted Mr. Leahy a photo of the drink menu from a bar,  the Roger Room. She had focused on a drink named for the song ‚ÄúChristmas Card From a Hooker in Minneapolis,‚Äù which happens to be Mr. Leahy‚Äôs favorite Tom Waits tune. ‚ÄúThis made me rethink things a bit,‚Äù he said, adding, ‚ÄúHillary Kerr was clearly not to be trifled with. ‚Äù In early 2014, Mr. Leahy invited her to a Bleachers concert. The band‚Äôs lead singer is Jack Antonoff, who is Ms. Dunham‚Äôs boyfriend. Ms. Dunham was at the concert as well, and on meeting Ms. Kerr, she said, ‚ÄúI‚Äôve heard so much about you. ‚Äù Ms. Kerr and Mr. Leahy shared their first kiss that night. He proposed to her on Polihale Beach in Kauai, Hawaii, on Jan. 1, 2016. On Dec. 10, 125 friends and relatives gathered in Palm Springs, Calif. at the Colony Palms Hotel, which was opened in 1936 by the reputed mobster Al Wertheimer and whose poolside guests have included Frank Sinatra, Ronald Reagan, Kirk Douglas and Zsa Zsa Gabor. Ms. Kerr walked down a grassy aisle in a   courtyard wearing a structured lace Reem Acra dress, strapless with a bustier and a full skirt. Four musicians played ‚ÄúOnce, With Feeling,‚Äù an instrumental song Mr. Leahy wrote for Ms. Kerr. . Just minutes into the cocktail reception, a few of Ms. Kerr‚Äôs best friends descended upon her. Jen Atkin, the celebrity hairstylist and social media star, started fussing with the flower she had sewn into the bride‚Äôs hair. Joey Maalouf, the celebrity makeup artist who is a creator of the       service the Glam App, whipped out a tube of lip gloss and reapplied it to the bride‚Äôs pucker. He had done her makeup. ‚ÄúThe look we went for is sickeningly stunning and perfect,‚Äù he said. Guests mingled over drinks by the pool, which was framed by banquette tables lit from above by strings of bulbs. The sky turned pink before the stars appeared, and guests snapped photos and shared them with the hundreds of thousands who follow these members of the   illuminati (#imwithkerr and #letsgetleahyed). ‚ÄúThis looks like it‚Äôs art directed,‚Äù Eva Chen, the head of fashion partnerships for Instagram, said as she took it all in. She had worked as an assistant at Elle with Ms. Kerr. Friends of both the bride and the groom celebrated what they saw as a great match, based on passion not only for each other but also for music. Leigh Belz Ray, the features and news director at InStyle, was another former Elle colleague who made the trip. ‚ÄúHillary loves music, and it‚Äôs not just a casual thing,‚Äù Ms. Ray said. ‚ÄúWe used to say the ultimate fantasy was to become a music director, and now she‚Äôs married to one. ‚Äù After a romantic first dance to Solomon Burke‚Äôs ‚ÄúIf You Need Me,‚Äù Mr. and Mrs. Leahy (she will use her maiden name professionally) settled into several hours of serious dancing to songs spun by a D. J. And before they left for their Hawaiian honeymoon, Mr. Leahy completed his first important act as husband. He pulled together many of the songs that could be considered the soundtrack to their romance and made his wife a mixtape. When Dec. 10, 2016 Where Colony Palms Hotel, Palm Springs, Calif. Flora The bride and groom were married under a white birch trellis, because white birch is the state tree of New Hampshire, where Mr. Leahy grew up and where his parents, Richard and Marie Leahy, reside. The structure was wrapped in white peonies, Sahara roses and camellia greens. Readings Mr. Leahy‚Äôs family is Roman Catholic Ms. Kerr‚Äôs parents, John and Carole Kerr, are more spiritual than religiously observant. Many of the guests had an artistic bent. The bride and groom planned accordingly. Marshall Goldsmith, an executive coach, author and lifelong family friend of Ms. Kerr, officiated. Friends and relatives stood to read poems from James Kavanaugh and Mary Oliver, as well as a passage from the Supreme Court‚Äôs 2015 ruling legalizing   marriage. ‚ÄúA little Catholic priest, a little lesbian Pulitzer Prize winner, a little equal rights for all,‚Äù Ms. Kerr explained after the ceremony.",New York Times,0
Filtering Spam E-Mail with Generalized Additive Neural Networks,"INTRODUCTION The Internet has promoted new channels of communication which allow an e-mail to be sent to people thousands of kilometers away [1]. Sending messages by e-mail has a number of advantages including high reliability, relatively low transmission costs, generally fast delivery and the ability to be automated [2]. These strengths enable almost free mass e- mailing which can reach out to hundreds of thousands of users within seconds. Unfortunately, this freedom of communication can be exploited. Over the last number of years a situation has been reached where users’ mailboxes have been ﬂooded with unwanted messages. This deluge of spam has escalated to the point where the viability of communication via e-mail is threatened. Although spam messages can be easily recognised, it is difﬁcult to develop an accurate and useful deﬁnition of spam. The Concise Oxford English Dictionary (eleventh edition) deﬁnes spam as “irrelevant or inappropriate messages sent on the Internet to a large number of newsgroups or users”. Spam is primarily used for advertising. Commodities proposed for commercial purposes range from computer software and medical products to investments. Spam is also employed to express religious or political opinions, mislead the target audience with promises of fortune and distribute useless chain letters. Spam causes a number of problems to the Internet commu- nity. It takes time to sort unwanted messages which poses a risk that legitimate mail can be deleted. Delivery of normal mail can be delayed by large amounts of spam-trafﬁc between servers. Users with dial-up Internet access have to waste bandwidth downloading junk mail. Furthermore, some spam are pornographic in nature and should not be revealed to children. Spam is no longer just considered as an invasive annoyance or a problem of convenience but it is regarded and accepted as an issue which poses a considerable security risk to enter- prises. This view is due to the fact that spam is used, amongst other things, for spreading computer viruses and as a deceptive method of obtaining sensitive information. It has already been reported in 2004 that about ninety percent of companies agreed that spam makes their companies more vulnerable to security threats [3]. More recent surveys and reports support this point of view [4]; [5]; [6]. It has therefore become imperative to ensure that proper policies and controls are in place to mitigate the security risks associated with spam. One important control is the detection and management of spam messages. A number of techniques have been applied to ﬁlter spam messages [7]; [8]; [9]. In this paper a Generalized Additive Neural Network (GANN) is applied to a publicly available cor- pus to detect spam messages. The Ling-Spam collection was constructed by [10] and made available as a benchmark corpus. Results obtained by the GANN are then compared to the performances of a Naive Bayesian classiﬁer and a Memory- based learning technique applied by [10]. This comparison will provide insight into the feasibility of using a GANN to detect unwanted messages. The rest of the paper is organized as follows. GANNs are discussed in Section II. Examples of typical supervised prediction models are presented and difﬁculties encountered with neural networks in general are considered. In Section III the publicly available Ling-Spam corpus is discussed. As preprocessing steps, vector representations of the messages are constructed and feature selection is performed. Next, Naive Bayesian classiﬁcation and Memory-based classiﬁcation are considered in Sections IV and V. In addition three practical scenarios where these methods can be applied are discussed. Cost-sensitive measures used to evaluate the learning tech- niques are presented in Section VI. These metrics takes into account the differences in cost of classifying a legitimate message as spam and vice versa. Experimental results are considered in Section VII. Finally, some conclusions on the feasibility of GANNs to detect spam are presented in Section VIII. II. GENERALIZED ADDITIVE NEURAL NETWORKS To arrive at a spam ﬁlter, a decision function f must be obtained that classiﬁes a given e-mail message m as spam (S) or legitimate mail (L) [2]. If the set of all e-mail messages is denoted by M, a search for the function f : M _ {S, L} is performed by supervised learning. With this technique learn- ing methods are trained on a set of pre-classiﬁed messages {(m1, c1), (m2, c2), . . . , (mn, cn)}, mi _ M, ci _ {S, L}. Examples of such learning methods are Generalized Linear Models, Multilayer Perceptrons, Generalized Additive Models and Generalized Additive Neural Networks [11]. Generalized Linear Models [12], g_1 0 (E(y)) = _0 + _1x1 + _2x2 + . . . + _kxk, are often used for predictive modeling. The range of predicted values are restricted by the link function, g_1 0 . For spam detection, the logit link g_1 0 (E(y)) = ln( E(y) 1 _ E(y) ) is appropriate as the expected target (probabilities) is bounded between zero and one. The parameters are usually estimated by maximum likelihood. Multilayer Perceptrons [13]; [14]; [15] are the most widely used type of neural network for supervised prediction. A Multilayer Perceptron (MLP) with a single hidden layer with h hidden neurons has the form g_1 0 (E(y)) = w0 + w1tanh(w01 + k X j=1 wj1xj) + . . . +whtanh(w0h + k X j=1 wjhxj), where the link function is the inverse of the output activation function. Although other sigmoidal functions could be used, the activation function in this case is the hyperbolic tangent. The unknown parameters are estimated by numerically opti- mizing some appropriate measure of ﬁt to the training data such as the negative log likelihood. A Generalized Additive Model (GAM) is deﬁned as g_1 0 (E(y)) = _0 + f1(x1) + f2(x2) + . . . + fk(xk), where the expected target on the link scale is expressed as the sum of unspeciﬁed univariate functions [16]; [17]; [18]. Each univariate function can be regarded as the effect of the corresponding input while holding the other inputs constant. When a GAM is implemented as a neural network it is called a Generalized Additive Neural Network. The main architecture of a GANN is comprised of a separate MLP with a single hidden layer of h units for each input variable: fj(xj ) = w1j tanh(w01j + w11j xj) + . . . +whjtanh(w0hj + w1hjxj). The individual bias terms of the outputs are incorporated into the overall bias _0. Each individual univariate function contains 3h parameters, where h, the number of hidden neurons, could be different across inputs. This architecture can be extended to include an additional parameter for a direct connection (skip layer): fj(xj) = w0j xj + w1jtanh(w01j + w11j xj) + . . . +whjtanh(w0hj + w1hjxj). A backﬁtting algorithm is used by [16] and [17] to esti- mate the individual univariate functions fj. Backﬁtting is not required for GANNs. Any method that is suitable for ﬁtting more general MLPs can be utilized to simultaneously estimate the parameters of GANN models. The usual optimization and model complexity issues also apply to GANN models. In general, to construct a neural network and interpreting results obtained are not trivial tasks. There are a number of decisions that must be made to determine the architecture. These decisions include the number of hidden nodes and the different activation functions. Trial-and-error or experiments is currently the most common way to determine the number of hidden nodes [15]. Additionally, various rules of thumb have been suggested. Examples of these rules are that the number of cases determine the number of hidden nodes and for each weight there should be at least ten records. Some researchers limit the number of hidden nodes with empirical rules. Alas, it has been found that none of these heuristic methods perform well for all problems. Choosing the appropriate number of inputs to the neural network is also not obvious. At best, a relatively small number of essential nodes are required which can identify the unique features found in the data. The learning or prediction capability of the network can be negatively inﬂuenced by too little or too many input nodes. When the number of inputs is too little, the neural network may not achieve the desired level of accuracy. Overtraining may occur when too many input nodes are utilized. To make matters worse, neural networks in general are regarded as black box methods. There is no explicit form to explain and analyze the relationship between inputs and the target which causes difﬁculty in interpreting results from the networks. Fortunately, these concerns are addressed by the automated construction algorithm for GANNs. At present two algorithms exist to estimate GANN models. An interactive construction algorithm was suggested by [19] which utilizes visual diagnostics to specify the complexity of each univariate function. To perform model selection, plots of the ﬁtted univariate functions, ˆfj(xj ) overlaid on the partial residuals prj = g_1 0 (y) _ ˆ_0 _ X ˆfl(xj) l6=j = (g_1 0 (y) _ g_1 0 (ˆy)) + ˆfj(xj ), versus the corresponding jth input are examined [20]; [21]; [22]. For a large number of inputs this evaluation of the partial residual plots can become a daunting and time consuming task. It is also known that human judgement is subjective which may result in models that are suboptimal. Therefore [11] developed an automated construction algorithm based on the search for GANN models using objective model selection criteria or cross-validation. In this algorithm, partial residuals plots are used as a tool to provide insight into the models created and not for model building. With sufﬁcient time to evaluate candidate models, this best-ﬁrst search method is optimal and complete. It was shown that the algorithm is effective, powerful and is comparable to other non-linear model selection techniques found in the literature [11]. The implementation of the automated construction algorithm, called AutoGANN, was applied to the publicly available Ling-Spam corpus to detect spam. Prior to discussing the Naive Bayesian classiﬁer and the Memory-based classiﬁer to which the results of the GANN will be compared, speciﬁc vector notation to represent e-mail messages will be given. This notation together with the Ling- Spam collection and the preprocessing steps performed on the corpus are considered next. III. LING-SPAM CORPUS COLLECTION AND PREPROCESSING The Ling-Spam collection used in this paper was compiled by [10] and is a combination of messages obtained from the Linguist list and spam e-mail messages. The former is a moderated mailing list about the science and occupation of linguistics. The corpus contains 2893 messages which is partitioned into 2412 Linguist messages and 481 spam messages. Cost-sensitive evaluation metrics were introduced by [10] to get an objective picture of the performance of the Naive Bayesian algorithm. These metrics are also utilized in this paper since the cost of misclassiﬁcation differs for the two classes (spam and legitimate). As in [23] a vector representation ~x = hx1, x2, . . . , xni is constructed for every message in Ling-Spam where x1, x2, . . . , xn denote the values of attributes X1, X2, . . . , Xn. These values are binary with Xi = 1 if some characteristic corresponding to Xi is present in the message and Xi = 0 oth- erwise. For the experiments, each attribute indicates whether a particular word (e.g. computer) can be found in the message. Feature selection is performed by ranking the candidate at- tributes by their mutual information (MI) values and choosing the attributes with the m highest MI scores. The MI value of each candidate attribute was computed as follows: M I(X; C) = X P (X = x, C = c)· x_{0,1}, c_{S,L} log P (X = x, C = c) P (X = x) · P (C = c) , where C denotes the category which can be spam (S) or legitimate messages (L) [23]. The probabilities are estimated as frequency ratios from the training corpus. Each word in the Ling-Spam corpus was substituted by its base form with a lemmatizer to prevent handling forms of the same word as different attributes. In the next section the Naive Bayesian ﬁlter utilized by [10] to detect spam is discussed. Moreover, three practical scenarios in which the ﬁlter are employed are considered. IV. NAIVE BAYESIAN CLASSIFICATION According to Bayes’ theorem and the theorem of total probability, the probability that a document d with vector ~x = hx1, x2, . . . , xni is a member of c is: P (C = c| ~X = ~x) = P (C = c) · P ( ~X = ~x|C = c) Pk_{S,L} P (C = k) · P ( ~X = ~x|C = k) , where S denotes a spam message and L a legitimate message. The probabilities P ( ~X|C) are impossible to estimate in practice without simplifying assumptions since the possible values of ~X are too many and data sparseness problems exist. As a result the Naive Bayesian classiﬁer assumes that X1, . . . , Xn are conditionally independent given the category C, which yields P (C = c| ~X = ~x) = P (C = c) · Qn i=1 P (Xi = xi|C = c) Pk_{S,L} P (C = k) · Qn i=1 P (Xi = xi|C = k) . By using the frequencies of the training corpus, P (Xi|C) and P (C) are easy to estimate. P (Xi|C) is the percentage of training corpus messages present (Xi = 1) or absent (Xi = 0) given a certain class (spam or legitimate). P (C) is the percentage of spam or legitimate training corpus messages. A message is ﬁltered as spam if the following condition is satisﬁed: with P (C = S| ~X = ~x) P (C = L| ~X = ~x) > _ P (C = S| ~X = ~x) = 1 _ P (C = L| ~X = ~x). This classiﬁcation condition is equivalent to VI. CLASSIFICATION PERFORMANCE MEASURES P (C = S| ~X = ~x) > t, with t = _ 1 + _ , _ = t 1 _ t . In the experiments performed by [23] t was set to 0.5 (_ = 1), 0.9 (_ = 9) and 0.999 (_ = 999). For the ﬁrst case, it is presumed the spam ﬁlter ﬂags messages considered to be spam to help the user prioritize the reading of the messages. These ﬂagged messages are not removed from the user’s mailbox. Since none of the two types of errors is signiﬁcantly more severe than the other, this setting seems reasonable. More work is needed to recover from a blocked legitimate message than deleting a spam message that passed the ﬁlter. For this scenario, _ is set to 9 to penalize a legitimate message being blocked slightly more than letting a spam message pass the ﬁlter. In the last scenario blocking a legitimate message is re- garded as undesirable as letting 999 spam messages pass the ﬁlter. Setting _ to such a high value can be justiﬁed when blocked messages are discarded without further processing as most users would consider losing a legitimate message as unacceptable. Next, a Memory-based technique applied by [10] on the Ling-Spam corpus is discussed. Results obtained by this ﬁlter on the Ling-Spam corpus is compared to the performance of the AutoGANN system in Section VII. V. MEMORY-BASED CLASSIFICATION Memory-based (instance-based) methods [24] store all train- ing instances in a memory structure and use them directly for classiﬁcation. The multi-dimensional space deﬁned by the attributes in the instance vectors constitutes the simplest form of memory structure. Each training instance vector is represented in this space by a point. A variant of the simple k-nearest-neighbour (k-nn) algorithm is normally employed by the classiﬁcation procedure. This algorithm considers the k training instances (its k-neighbourhood) closest to the unseen instance and assigns the majority class among these instances to the new unseen instance. The memory-based classiﬁcation algorithm implemented in the Tilburg Memory-Based Learner (TiMBL) open source software package was utilized by [10]. This software provides a basic memory-based classiﬁcation algorithm with extensions such as attribute weighting and efﬁcient computation of the k- neighbourhood. TiMBL takes the k closest training instances from the unseen instance into account. When more than one neighbour is found at each distance, the algorithm considers many more instances than the k neighbours. For these cases, a small value of k is chosen to avoid examining instances that are very different from the unseen one. A post-processing stage was added to the basic TiMBL algorithm by [10] to take _ into account. This extension simply multiplies the number of legitimate neighbours by _ before deciding on the majority class in the neighbourhood. In the following section metrics used to evaluate the perfor- mance of the AutoGANN system are examined. Classiﬁcation performance is frequently measured in terms of accuracy (Acc) or error rate (Err = 1 _ Acc) [10]. Suppose NL and NS denote the total number of legitimate and spam messages respectively to be classiﬁed by the ﬁlter and nA_B the number of messages that belongs to category A that the ﬁlter classiﬁes as belonging to category B with (A, B) _ {L, S}. Accuracy and error rate can then be deﬁned as Acc = nL_L + nS_S NL + NS and Err = nL_S + nS_L NL + NS . The measurements for accuracy and error rate assign equal weights to the two types of error (L _ S and S _ L), but L _ S is _ times more costly than S _ L. For evaluation purposes, each legitimate message is handled as if it were _ messages to make the accuracy and error rate sensitive to the difference in cost. Therefore, when a legitimate message goes successfully through the ﬁlter it counts as _ successes and when it is blocked, it counts as _ errors. As a result, the following deﬁnitions of weighted accuracy (W Acc) and weighted error rate (W Err = 1 _ W Acc) can be given: and W Acc = _ · nL_L + nS_S _ · NL + NS W Err = _ · nL_S + nS_L _ · NL + NS . Accuracy and error rate values (or their weighted versions) are often deceivingly high. To counter this effect, it is common to compare the accuracy or error rate to that of a simplistic baseline approach. The case used by [10] is where no ﬁlter is present, thus legitimate messages are never blocked and spam messages always pass. This leads to the weighted accuracy and weighted error rate of the baseline: and W Accb = _ · NL _ · NL + NS W Errb = NS _ · NL + NS . The total cost ratio (TCR) enables the performance of a ﬁlter to be easily compared to that of the baseline: T CR = W Errb W Err = NS _ · nL_S + nS_L . is better not Higher T CR values suggest better performance. When T CR < 1 it to utilize the ﬁlter (baseline approach). An intuitive meaning of T CR can be obtained by assuming cost is proportional to wasted time. Therefore, T CR measures how much time is spend manually deleting spam messages when no ﬁlter is used (NS) compared to the time spend manually deleting spam messages that passed the ﬁlter (nS_L) plus time needed to recover from legitimate messages mistakenly blocked (_ · nL_S). In the next section experimental results obtained by applying the AutoGANN system to the Ling-Spam corpus are discussed. These results are compared to the performances of the Naive Bayesian learning method and the Memory-Based technique harnessed by [10]. VII. EXPERIMENTAL RESULTS As in [10] three experiments were performed by the Au- toGANN system. These experiments correspond to the three scenarios for the _ parameter described in Section IV. The number of selected attributes ranged from 25 to 100 in steps of 25 for each scenario1. The AutoGANN system also performed feature selection in addition to that performed as part of the preprocessing step. In all the experiments, 10-fold cross- validation was performed and W Acc was then averaged over the ten iterations. Finally, T CR was calculated as W Errb divided by the average W Err value. Figures 1 to 6 show the average performance of each learning method in each experiment which includes T CR scores obtained by [10] with the keyword-based Outlook 20002 ﬁlter. The performance of Outlook was included by [10] as an example of a widely used e-mail reader and does not form part of the main comparison with the AutoGANN system. Results attained on the three scenarios are discussed next. A. Scenario 1: Labelling spam messages (_ = 1) In the ﬁrst scenario the misclassiﬁcation cost is the same for both error types. Figures 1 and 2 show the corresponding results.3 All three learning techniques improve signiﬁcantly on the baseline (T CR = 1.0) and produce very accurate results. AutoGANN outperforms the other two techniques by a large margin over the interval [50, 100] (Figure 2). On an individual basis, the techniques performed as follows. The Naive Bayesian classiﬁer achieves the best results for 100 attributes, while TiMBL performs best with a smaller attribute set size of 50 (Figure 1). AutoGANN does best for 100 attributes. Three different values of k (1, 2, and 10) were chosen to evaluate TiMBL’s performance. From Figure 1 it seems the method performs best for small values of k. For k = 10 the method improves only slightly on the base case. This can be ascribed to the large number of ties for each of the k = 10 distances which leads to a very large neighbourhood taken into consideration (> 500 neighbours). For these cases the ﬁlter approximates the default rule which classiﬁes all messages according to the majority class (legitimate in the case of the Ling-Spam corpus). This also explains the insensitivity of the method to the number of attributes for k = 10. Compared to the other three methods, Outlook’s keyword patterns perform very poorly. 1It was found that the AutoGANN system is most effective with 100 or less attributes [11]. 2Outlook 2000 is a trademark of Microsoft Corporation. 3Note than the horizontal and vertical axes of the two ﬁgures have different scales. Figure 1: TCR scores for the comparative techniques when _ = 1 [10]. A u t o G A N N T C R 3 0 2 9 2 8 2 7 2 6 2 5 2 4 2 3 2 2 2 1 2 0 1 9 1 8 1 7 1 6 1 5 1 4 1 3 1 2 1 1 1 0 9 8 7 6 5 4 3 2 1 0 2 5 5 0 7 5 1 0 0 N u m b e r o f r e t a i n e d a t t r i b u t e s Figure 2: TCR scores for the AutoGANN technique when _ = 1. B. Scenario 2: Notifying senders about blocked messages (_ = 9) For the second scenario the cost of misclassifying legit- imate messages is increased by setting _ = 9. Figures 3 and 4 show the corresponding results. The most important difference compared to the ﬁrst scenario is less improvement of the learning techniques over the baseline. As _ increases the performance of the learning techniques relative to the baseline decreases. This is due to the fact that without a ﬁlter all legitimate messages are retained which becomes more beneﬁcial as _ increases. Consequently, it becomes harder to “beat” the baseline. As in the ﬁrst scenario, AutoGANN is clearly superior compared to the other two classiﬁers (Figure 4). From Figure 3 it can be observed that Outlook’s patterns perform below the base case (T CR < 1.0). This suggests one is better off not utilizing the Outlook ﬁlter. attributes. AutoGANN performs signiﬁcantly worse than the baseline (In Figure 6, T CR < 1.0). Table 1 summarizes the number of attributes (# attr.) for which each learning technique performs best. In the next, ﬁnal section, some conclusions are presented. Figure 3: TCR scores for the comparative techniques when _ = 9 [10]. T C R 7 6 5 4 3 2 1 0 A u t o G A N N Figure 5: TCR scores for the comparative techniques when _ = 999 [10]. 2 5 5 0 7 5 1 0 0 N u m b e r o f r e t a i n e d a t t r i b u t e s Figure 4: TCR scores for the AutoGANN technique when _ = 9. C. Scenario 3: Removing blocked messages (_ = 999) For the last scenario a large _ value is used. From Figures 5 and 6 it can be seen that the performance of the learning techniques decreases to such a level that any improvement on the baseline is very hard to achieve. Accordingly, the choice to use any ﬁlter at all becomes doubtful. Note that this high _ value was proposed by [23]. TiMBL with k = 10 is one exception by performing consistently higher than the base case by a small margin. This is again inﬂuenced by the very large neighbourhood, which classiﬁes most messages as legitimate, due to the large value of _. The Naive Bayesian classiﬁer delivers better performance with 300 attributes, but this is the only point where it outperforms the baseline. Locating the optimal attribute size exactly is infeasible in practical applications and therefore TiMBL for k = 2 is the preferred choice. Abrupt ﬂuctuations in the performance of the learning methods can be ascribed to the misclassiﬁcation of a legitimate message which causes a very large slump in TCR. This effect can be observed, for example, with TiMBL for k = 2 and 550 T C R 0 . 0 7 0 . 0 6 0 . 0 5 0 . 0 4 0 . 0 3 0 . 0 2 0 . 0 1 0 A u t o G A N N 2 5 5 0 7 5 1 0 0 N u m b e r o f r e t a i n e d a t t r i b u t e s Figure 6: TCR scores for the AutoGANN technique when _ = 999. _ 1 9 Filter used AutoGANN Naive Bayesian TiMBL(1) TiMBL(2) Outlook patterns TiMBL(10) Baseline (no ﬁlter) AutoGANN Naive Bayesian TiMBL(1) TiMBL(2) TiMBL(10) Baseline (no ﬁlter) Outlook patterns 100 - 100 100 200 50 150 - - # attr. W Acc (%) 100 100 50 50 - 99.378 96.926 96.890 96.753 90.978 89.079 83.374 99.666 99.432 99.274 99.090 98.364 97.832 97.670 TCR 26.71 5.41 5.35 5.12 1.84 1.52 1 6.50 3.82 2.99 2.38 1.33 1 0.93 Table 1: Results on the Ling-Spam corpus. _ # attr. W Acc (%) TCR 300 2.86 2.22 250 250 1.18 Filter used Naive Bayesian TiMBL(2) TiMBL(10) Baseline (no ﬁlter) 0.12 TiMBL(1) 0.07 AutoGANN Outlook patterns 0.02 Table 1: Results on the Ling-Spam corpus (continued). 99.993 99.991 99.983 99.980 99.829 99.709 98.952 200 100 999 1 - - VIII. CONCLUSIONS Spam has become one of the major risk security threats in modern organisations and in order to protect enterprises against risks such as computer viruses, phishing, overloading, unnecessary cost etc., efﬁcient security controls, such as spam ﬁlters, are necessary. In this paper a comparison between the Naive Bayesian clas- siﬁer, a Memory-based classiﬁer and the automated construc- tion algorithm for Generalized Additive Neural Networks was performed to determine the feasibility of the latter technique to ﬁlter spam e-mail messages. These techniques were applied to a publicly available corpus using cost-sensitive evaluation measures. When spam messages are simply to be ﬂagged or when additional means are available to inform senders of blocked messages, the AutoGANN system provides a considerable improvement over the performances of the Naive Bayesian classiﬁer and the Memory-based technique. These ﬁndings suggest the AutoGANN system can be used successfully as an anti-spam ﬁlter for these two scenarios. Moreover, the AutoGANN system clearly exceeded the performance of the anti-spam keyword patterns of an extensively used e-mail reader. When no additional means are available and messages are discarded without further processing, a memory-based approach seems to be more viable, but the ﬁlter must be conﬁgured appropriately with great care.",Scientific Journal,0
A Property Based Security Risk Analysis Through Weighted Simulation,"",,
"les to """,Scientific Journal,0,
"With New Congress Poised to Convene, Obama‚Äôs Policies Are in Peril - The New York Times","WASHINGTON  ‚Äî   The most powerful and ambitious   Congress in 20 years will convene Tuesday, with plans to leave its mark on virtually every facet of American life  ‚Äî   refashioning the country‚Äôs social safety net, wiping out scores of labor and environmental regulations and unraveling some of the most significant policy prescriptions put forward by the Obama administration. Even before   Donald J. Trump is sworn in on Jan. 20, giving their party full control of the government, Republicans plan quick action on several of their top priorities  ‚Äî   most notably a measure to clear a path for the Affordable Care Act‚Äôs repeal. Perhaps the first thing that will happen in the new Congress is the push for deregulation. Also up early: filling a   Supreme Court seat, which is sure to set off a pitched showdown, and starting confirmation hearings for Mr. Trump‚Äôs cabinet nominees. ‚ÄúIt‚Äôs a big job to actually have responsibility and produce results,‚Äù said Senator Mitch McConnell of Kentucky, the majority leader. ‚ÄúAnd we intend to do it. ‚Äù But as Republicans plan to reserve the first 100 days of Congress for their more partisan goals, Democrats are preparing roadblocks. The party‚Äôs brutal   wounds have been salted by evidence of Russian election interference, Mr. Trump‚Äôs   cabinet picks and his taunting Twitter posts. (On Saturday, he offered New Year‚Äôs wishes ‚Äúto all,‚Äù including ‚Äúthose who have fought me and lost so badly they just don‚Äôt know what to do. ‚Äù) Obstacles will also come from Republicans, who are divided on how to proceed with the health care law and a pledge to rewrite the tax code. Some are also skittish about certain policy proposals, like vast changes to Medicare, that could prove unpopular among the broad electorate. And any burst of legislative action will come only if Congress can break free of its longstanding tendency toward gridlock. For Republicans, the path to this moment has been long and transparently paved  ‚Äî   the House in particular has signaled the Republican policy vision through bills it has been passing for years. But many of those measures have gathered dust in the Senate or been doused in veto ink. The cleft between the two chambers recalls the situation faced by the insurgent House Republican majority in the  . Speaker Newt Gingrich took control with a determined agenda, only to be stymied by the Senate majority leader, Bob Dole, who stacked conservative House bills like so many fire logs in the back of the Senate chamber. ‚ÄúThey‚Äôve been given a golden opportunity here,‚Äù said Trent Lott, the former Republican Senate majority leader. ‚ÄúBut I have watched over the years when one party has had control of the White House and the Senate and the House, and the danger is overplaying your hand. ‚ÄúIf you go too far, like what happened with Obamacare, and you get no support at all from the other side, you have a problem,‚Äù Mr. Lott continued. ‚ÄúYou have to find a way to work with people across the aisle who will work with you. ‚Äù The tax overhaul and an infrastructure bill may be two opportunities for bipartisan cooperation the Senate Finance Committee is already moving in that direction. Still, both of those issues are expected to remain on the back burner, despite promises to the contrary from Mr. Trump‚Äôs chief of staff, Reince Priebus. The Senate may be narrowly divided, but among the 48 senators in the Democratic caucus are 10 who will stand for   in two years in states that voted for Mr. Trump. Republicans are counting on their support, at least some of the time. But on many issues, Senate Democrats  ‚Äî   including their new leader, Chuck Schumer of New York  ‚Äî   are expected to pivot from postelection carping to active thwarting, using complex Senate procedures and political messaging to slow or perhaps block elements of Mr. Trump‚Äôs agenda. ‚ÄúAfter campaigning on a promise to help the middle class,   Trump‚Äôs postelection actions suggest he intends to do the exact opposite after he‚Äôs sworn in,‚Äù said Senator Patty Murray, Democrat of Washington. ‚ÄúDemocrats will do everything we can to fight back if he continues to pursue an agenda prioritizing billionaires and big corporations while devastating   families and the economy. ‚Äù Republicans have chafed for years at a host of rules, many   that President Obama has issued through the regulatory process, and they have been advising the Trump team on which ones should be undone. ‚ÄúI hear probably more about the strangulation of regulations on business and their growth and their development than probably anything else,‚Äù the House speaker, Paul D. Ryan of Wisconsin, said at a recent forum. ‚ÄúI think if we can provide regulatory relief right away, that can breathe a sigh of relief into the economy. ‚Äù In late December, the Obama administration rolled out a major new environmental regulation intended to rein in   mining. That regulation, one of dozens that Mr. Trump is expected to reverse, is meant to go into effect one day before his inauguration. But Congress is likely to block it, using the obscure Congressional Review Act, which permits lawmakers to undo new regulations with only 51 Senate votes within the first 60 legislative days of the rules‚Äô completion. Given time constraints on the Senate floor, members will have to pick some priorities. They are expected to train their sights on a rule that requires oil and gas producers to reduce methane gases, another that requires mining and fossil fuel companies to disclose payments they have made to foreign governments to extract natural resources, and still others that restrict pesticide use. Republicans will also move quickly to repeal the Affordable Care Act. They plan to pass a truncated budget resolution for the remainder of the fiscal year  ‚Äî   already a quarter over  ‚Äî   that includes special instructions ensuring that the final repeal legislation could circumvent any Democratic filibuster. But Republican leaders have not settled on a health care plan to replace Mr. Obama‚Äôs, and they may delay the repeal measure‚Äôs effective date for years. The Senate must also consider Mr. Trump‚Äôs cabinet picks, and Senate Democrats are already trying to slow the process. However, they cannot do much more than that, because when they were in charge, they changed the rules so that presidential nominees other than Supreme Court picks need only 51 votes to be confirmed. Previously, such nominations could face a filibuster, which required 60 votes to overcome. Democratic leaders have encouraged members to avoid meeting with Mr. Trump‚Äôs nominees until they have turned over their tax returns and made other disclosures. Republicans have been particularly upset that Senator Jeff Sessions of Alabama, whom Mr. Trump picked quickly to be attorney general, has either not gotten meetings with Democrats on the Senate Judiciary Committee or had meetings canceled. Senator Dianne Feinstein of California postponed her meeting with Mr. Sessions until January because, she said, her schedule got too busy. ‚ÄúThe senator doesn‚Äôt want to rush,‚Äù said her spokesman, Tom Mentzer. One reason that Democrats are in no hurry is their bitterness over Mr. McConnell‚Äôs refusal last year to hold a hearing on the Supreme Court nomination of Judge Merrick B. Garland. Lingering in the background is the specter of Russia. Democrats  ‚Äî   and some Republicans, who are at odds with Mr. Trump on the issue and may at times be a brake on him  ‚Äî   want a vigorous investigation of its efforts to disrupt the election. The Obama administration, which took sweeping steps last week to punish the Russians over election hacking, will release a report this month that is likely to serve as a turning point in those discussions. While Republicans may have a rare chance to open the flow of legislation, the party‚Äôs leaders are acutely aware of the punishment that Americans have historically delivered in midterm elections when things have not gone well. ‚ÄúThis is no time for hubris,‚Äù Mr. McConnell said. ‚ÄúYou have to perform. ‚Äù",New York Times,0
Donald Trump May Have Broken The Law To Avoid Paying Federal Income Taxes,"By Sean Colarossi on Mon, Oct 31st, 2016 at 9:52 pm Trump used a ‚Äútax avoidance maneuver so legally dubious his own lawyers advised him that the Internal Revenue Service would likely declare it improper if he were audited.‚Äù Share on Twitter Print This Post 
While Donald Trump has repeatedly said he took advantage of loopholes that allowed him to legally pay nothing in federal income taxes for decades, the New York Times reported on Monday that Trump may have actually crossed a legal boundary to avoid paying his fair share. 
According to the Times, the Republican nominee used a ‚Äútax avoidance maneuver so legally dubious his own lawyers advised him that the Internal Revenue Service would likely declare it improper if he were audited.‚Äù 
The report: 
Tax experts who reviewed the newly obtained documents for The New York Times said Mr. Trump‚Äôs tax avoidance maneuver, conjured from ambiguous provisions of highly technical tax court rulings, clearly pushed the edge of the envelope of what tax laws permitted at the time. ‚ÄúWhatever loophole existed was not ‚Äòexploited‚Äô here, but stretched beyond any recognition,‚Äù said Steven M. Rosenthal, a senior fellow at the nonpartisan Tax Policy Center who helped draft tax legislation in the early 1990s. 
Moreover, the tax experts said the maneuver trampled a core tenet of American tax policy by conferring enormous tax benefits to Mr. Trump for losing vast amounts of other people‚Äôs money ‚Äî in this case, money investors and banks had entrusted to him to build a casino empire in Atlantic City. 
The reason that Trump bent over backward ‚Äì and potentially broke laws ‚Äì to avoid paying these taxes is because he was scrambling to ‚Äústave off financial ruin.‚Äù 
This is the same man who claims he would be a great president because of his ‚Äúsuccessful‚Äù business background. 
Yet, he calls himself the ‚Äúking of debt.‚Äù His businesses have declared bankruptcy six times. He has spent decades paying zero dollars in federal income taxes. He lost a billion dollars in a single year running a casino business. Reporting out tonight even suggests that he has direct financial ties to Russia. 
All of this is just the tip of the iceberg when it comes to his business background. 
Even as all of this information is out there, American voters still know very little about Donald Trump‚Äôs finances because he has refused to be transparent about them. 
And consider this: What Trump is hiding in his tax returns must be much worse than the already damning information that keeps coming out through good reporting. If it weren‚Äôt, he would have already released them. 
The latest New York Times story is just the latest installment of Trump‚Äôs shady business background. The media should spend the remaining week of this campaign demanding more answers from a man who wants to be in charge of the country‚Äôs pocketbook.",politicususa.com,1
A Sample of Digital Forensic Quality Assurance in the South African Criminal Justice System ,"",,
"some qu""",Scientific Journal,0,
"After ‚ÄòThe Biggest Loser,‚Äô Their Bodies Fought to Regain Weight ","Danny Cahill stood, slightly dazed, in a blizzard of confetti as the audience screamed and his family ran on stage. He had won Season 8 of NBC‚Äôs reality television show ‚ÄúThe Biggest Loser,‚Äù shedding more weight than anyone ever had on the program  ‚Äî   an astonishing 239 pounds in seven months. When he got on the scale for all to see that evening, Dec. 8, 2009, he weighed just 191 pounds, down from 430. Dressed in a   and   shorts, he was lean, athletic and as handsome as a model. ‚ÄúI‚Äôve got my life back,‚Äù he declared. ‚ÄúI mean, I feel like a million bucks. ‚Äù Mr. Cahill left the show‚Äôs stage in Hollywood and flew directly to New York to start a triumphal tour of the talk shows, chatting with Jay Leno, Regis Philbin and Joy Behar. As he heard from fans all over the world, his elation knew no bounds. But in the years since, more than 100 pounds have crept back onto his    frame despite his best efforts. In fact, most of that season‚Äôs 16 contestants have regained much if not all the weight they lost so arduously. Some are even heavier now. Yet their experiences, while a bitter personal disappointment, have been a gift to science. A study of Season 8‚Äôs contestants has yielded surprising new discoveries about the physiology of obesity that help explain why so many people struggle unsuccessfully to keep off the weight they lose. Kevin Hall, a scientist at a federal research center who admits to a weakness for reality TV, had the idea to follow the ‚ÄúBiggest Loser‚Äù contestants for six years after that victorious night. The project was the first to measure what happened to people over as long as six years after they had lost large amounts of weight with intensive dieting and exercise. The results, the researchers said, were stunning. They showed just how hard the body fights back against weight loss. ‚ÄúIt is frightening and amazing,‚Äù said Dr. Hall, an expert on metabolism at the National Institute of Diabetes and Digestive and Kidney Diseases, which is part of the National Institutes of Health. ‚ÄúI am just blown away. ‚Äù It has to do with resting metabolism, which determines how many calories a person burns when at rest. When the show began, the contestants, though hugely overweight, had normal metabolisms for their size, meaning they were burning a normal number of calories for people of their weight. When it ended, their metabolisms had slowed radically and their bodies were not burning enough calories to maintain their thinner sizes. Researchers knew that just about anyone who deliberately loses weight  ‚Äî   even if they start at a normal weight or even underweight  ‚Äî   will have a slower metabolism when the diet ends. So they were not surprised to see that ‚ÄúThe Biggest Loser‚Äù contestants had slow metabolisms when the show ended. What shocked the researchers was what happened next: As the years went by and the numbers on the scale climbed, the contestants‚Äô metabolisms did not recover. They became even slower, and the pounds kept piling on. It was as if their bodies were intensifying their effort to pull the contestants back to their original weight. Mr. Cahill was one of the worst off. As he regained more than 100 pounds, his metabolism slowed so much that, just to maintain his current weight of 295 pounds, he now has to eat 800 calories a day less than a typical man his size. Anything more turns to fat. The struggles the contestants went through help explain why it has been so hard to make headway against the nation‚Äôs obesity problem, which afflicts more than a third of American adults. Despite spending billions of dollars on   drugs and dieting programs, even the most motivated are working against their own biology. Their experience shows that the body will fight back for years. And that, said Dr. Michael Schwartz, an obesity and diabetes researcher who is a professor of medicine at the University of Washington, is ‚Äúnew and important. ‚Äù ‚ÄúThe key point is that you can be on TV, you can lose enormous amounts of weight, you can go on for six years, but you can‚Äôt get away from a basic biological reality,‚Äù said Dr. Schwartz, who was not involved in the study. ‚ÄúAs long as you are below your initial weight, your body is going to try to get you back. ‚Äù The show‚Äôs doctor, Robert Huizenga, says he expected the contestants‚Äô metabolic rates to fall just after the show, but was hoping for a smaller drop. He questioned, though, whether the measurements six years later were accurate. But maintaining weight loss is difficult, he said, which is why he tells contestants that they should exercise at least nine hours a week and monitor their diets to keep the weight off. ‚ÄúUnfortunately, many contestants are unable to find or afford adequate ongoing support with exercise doctors, psychologists, sleep specialists, and trainers  ‚Äî   and that‚Äôs something we all need to work hard to change,‚Äù he said in an email. The study‚Äôs findings, to be published on Monday in the journal Obesity, are part of a scientific push to answer some of the most fundamental questions about obesity. Researchers are figuring out why being fat makes so many people develop diabetes and other medical conditions, and they are searching for new ways to block the poison in fat. They are starting to unravel the reasons bariatric surgery allows most people to lose significant amounts of weight when dieting so often fails. And they are looking afresh at medical care for obese people. The hope is that this work will eventually lead to new therapies that treat obesity as a chronic disease and can help keep weight under control for life. Most people who have tried to lose weight know how hard it is to keep the weight off, but many blame themselves when the pounds come back. But what obesity research has consistently shown is that dieters are at the mercy of their own bodies, which muster hormones and an altered metabolic rate to pull them back to their old weights, whether that is hundreds of pounds more or that extra 10 or 15 that many people are trying to keep off. There is always a weight a person‚Äôs body maintains without any effort. And while it is not known why that weight can change over the years  ‚Äî   it may be an effect of aging  ‚Äî   at any point, there is a weight that is easy to maintain, and that is the weight the body fights to defend. Finding a way to thwart these mechanisms is the goal scientists are striving for. First, though, they are trying to understand them in greater detail. Dr. David Ludwig, the director of the New Balance Foundation Obesity Prevention Center at Boston Children‚Äôs Hospital, who was not involved in the research, said the findings showed the need for new approaches to weight control. He cautioned that the study was limited by its small size and the lack of a control group of obese people who did not lose weight. But, he added, the findings made sense. ‚ÄúThis is a subset of the most successful‚Äù dieters, he said. ‚ÄúIf they don‚Äôt show a return to normal in metabolism, what hope is there for the rest of us?‚Äù Still, he added, ‚Äúthat shouldn‚Äôt be interpreted to mean we are doomed to battle our biology or remain fat. It means we need to explore other approaches. ‚Äù Some scientists say weight maintenance has to be treated as an issue separate from weight loss. Only when that challenge is solved, they say, can progress truly be made against obesity. ‚ÄúThere is a lot of basic research we still need to do,‚Äù said Dr. Margaret Jackson, who is directing a project at Pfizer. Her group is testing a drug that, in animals at least, acts like leptin, a hormone that controls hunger. With weight loss, leptin levels fall and people become hungry. The idea is to trick the brains of people who have lost weight so they do not become ravenous for lack of leptin. While many of the contestants kept enough weight off to improve their health and became more physically active, the low weights they strived to keep eluded all but one of them: Erinn Egbert, a   caregiver for her mother in Versailles, Ky. And she struggles mightily to keep the pounds off because her metabolism burns 552 fewer calories a day than would be expected for someone her size. ‚ÄúWhat people don‚Äôt understand is that a treat is like a drug,‚Äù said Ms. Egbert, who went from 263 pounds to just under 176 on the show, and now weighs between 152 and 157. ‚ÄúTwo treats can turn into a binge over a   period. That is what I struggle with. ‚Äù Six years after Season 8 ended, 14 of the 16 contestants went to the N. I. H. last fall for three days of testing. The researchers were concerned that the contestants might try to frantically lose weight before coming in, so they shipped equipment to them that would measure their physical activity and weight before their visit, and had the information sent remotely to the N. I. H. The contestants received their metabolic results last week. They were shocked, but on further reflection, decided the numbers explained a lot. ‚ÄúAll my friends were drinking beer and not gaining massive amounts of weight,‚Äù Mr. Cahill said. ‚ÄúThe moment I started drinking beer, there goes another 20 pounds. I said, ‚ÄòThis is not right. Something is wrong with my body. ‚Äô‚Äù Sean Algaier, 36, a pastor from Charlotte, N. C. feels cheated. He went from 444 pounds to 289 as a contestant on the show. Now his weight is up to 450 again, and he is burning 458 fewer calories a day than would be expected for a man his size. ‚ÄúIt‚Äôs kind of like hearing you have a life sentence,‚Äù he said. Slower metabolisms were not the only reason the contestants regained weight, though. They constantly battled hunger, cravings and binges. The investigators found at least one reason: plummeting levels of leptin. The contestants started out with normal levels of leptin. By the season‚Äôs finale, they had almost no leptin at all, which would have made them ravenous all the time. As their weight returned, their leptin levels drifted up again, but only to about half of what they had been when the season began, the researchers found, thus helping to explain their urges to eat. Leptin is just one of a cluster of hormones that control hunger, and although Dr. Hall and his colleagues did not measure the rest of them, another group of researchers, in a different project, did. In a   study funded by Australia‚Äôs National Health and Medical Research Council, Dr. Joseph Proietto of the University of Melbourne and his colleagues recruited 50 overweight people who agreed to consume just 550 calories a day for eight or nine weeks. They lost an average of nearly 30 pounds, but over the next year, the pounds started coming back. Dr. Proietto and his colleagues looked at leptin and four other hormones that satiate people. Levels of most of them fell in their study subjects. They also looked at a hormone that makes people want to eat. Its level rose. ‚ÄúWhat was surprising was what a coordinated effect it is,‚Äù Dr. Proietto said. ‚ÄúThe body puts multiple mechanisms in place to get you back to your weight. The only way to maintain weight loss is to be hungry all the time. We desperately need agents that will suppress hunger and that are safe with   use. ‚Äù Mr. Cahill, 46, said his weight problem began when he was in the third grade. He got fat, then fatter. He would starve himself, and then eat a whole can of cake frosting with a spoon. Afterward, he would cower in the pantry off the kitchen, feeling overwhelmed with shame. Over the years, his insatiable urge to eat kept overcoming him, and his weight climbed: 370 pounds, 400, 460, 485. ‚ÄúI used to look at myself and think, ‚ÄòI am horrible, I am a monster, subhuman,‚Äô‚Äù he said. He began sleeping in a recliner because he was too heavy to sleep lying down. Walking hurt stairs were agony. Buying clothes with a 68 waist was humiliating. ‚ÄúI remember sitting in a dressing room one day, and nothing would fit. I looked at the traffic outside on the street and thought, ‚ÄòI should just run out in front of a car. ‚Äô‚Äù He eventually seized on ‚ÄúThe Biggest Loser‚Äù as his best chance to lose enough weight to live a normal life. He tried three times and was finally selected. Before the show began, the contestants underwent medical tests to be sure they could endure the rigorous schedule that lay ahead. And rigorous it was. Sequestered on the ‚ÄúBiggest Loser‚Äù ranch with the other contestants, Mr. Cahill exercised seven hours a day, burning 8, 000 to 9, 000 calories according to a calorie tracker the show gave him. He took electrolyte tablets to help replace the salts he lost through sweating, consuming many fewer calories than before. Eventually, he and the others were sent home for four months to try to keep losing weight on their own. Mr. Cahill set a goal of a   deficit per day. The idea was to lose a pound a day. He quit his job as a land surveyor to do it. His routine went like this: Wake up at 5 a. m. and run on a treadmill for 45 minutes. Have breakfast  ‚Äî   typically one egg and two egg whites, half a grapefruit and a piece of sprouted grain toast. Run on the treadmill for another 45 minutes. Rest for 40 minutes bike ride nine miles to a gym. Work out for two and a half hours. Shower, ride home, eat lunch  ‚Äî   typically a grilled skinless chicken breast, a cup of broccoli and 10 spears of asparagus. Rest for an hour. Drive to the gym for another round of exercise. If he had not burned enough calories to hit his goal, he went back to the gym after dinner to work out some more. At times, he found himself running around his neighborhood in the dark until his   indicator reset to zero at midnight. On the day of the   on the show‚Äôs finale, Mr. Cahill and the others dressed carefully to hide the rolls of loose skin that remained, to their surprise and horror, after they had lost weight. They wore compression undergarments to hold it in. Mr. Cahill knew he could not maintain his finale weight of 191 pounds. He was so mentally and physically exhausted he barely moved for two weeks after his publicity tour ended. But he had started a new career giving motivational speeches as the biggest loser ever, and for the next four years, he managed to keep his weight below 255 pounds by exercising two to three hours a day. But two years ago, he went back to his job as a surveyor, and the pounds started coming back. Soon the scale hit 265. Mr. Cahill started weighing and measuring his food again and stepped up his exercise. He got back down to 235 to 240 pounds. But his weight edged up again, to 275, then 295. His slow metabolism is part of the problem, and so are his food cravings. He opens a bag of chips, thinking he will have just a few. ‚ÄúI‚Äôd eat five bites. Then I‚Äôd black out and eat the whole bag of chips and say, ‚ÄòWhat did I do? ‚Äô‚Äù Dr. Lee Kaplan, an obesity researcher at Harvard, says the brain sets the number of calories we consume, and it can be easy for people to miss that how much they eat matters less than the fact that their bodies want to hold on to more of those calories. Dr. Michael Rosenbaum, an obesity researcher at Columbia University who has collaborated with Dr. Hall in previous studies, said the body‚Äôs systems for regulating how many calories are consumed and how many are burned are tightly coupled when people are not strenuously trying to lose weight or to maintain a significant weight loss. Still, pounds can insidiously creep on. ‚ÄúWe eat about 900, 000 to a million calories a year, and burn them all except those annoying 3, 000 to 5, 000 calories that result in an average annual weight gain of about one to two pounds,‚Äù he said. ‚ÄúThese very small differences between intake and output average out to only about 10 to 20 calories per day  ‚Äî   less than one Starburst candy  ‚Äî   but the cumulative consequences over time can be devastating. ‚Äù ‚ÄúIt is not clear whether this small imbalance and the resultant weight gain that most of us experience as we age are the consequences of changes in lifestyle, the environment or just the biology of aging,‚Äù Dr. Rosenbaum added. The effects of small imbalances between calories eaten and calories burned are more pronounced when people deliberately lose weight, Dr. Hall said. Yes, there are signals to regain weight, but he wondered how many extra calories people were driven to eat. He found a way to figure that out. He analyzed data from a clinical trial in which people took a diabetes drug, canagliflozin, that makes them spill 360 calories a day into their urine, or took a placebo. The drug has no known effect on the brain, and the person does not realize those calories are being spilled. Those taking the drug gradually lost weight. But for every five pounds they lost, they were, without realizing it, eating an additional 200 calories a day. Those extra calories, Dr. Hall said, were a bigger driver of weight regained than the slowing of the metabolism. And, he added, if people fought the urge to eat those calories, they would be hungry. ‚ÄúUnless they continue to fight it constantly, they will regain the weight,‚Äù he said. All this does not mean that modest weight loss is hopeless, experts say. Individuals respond differently to diet manipulations  ‚Äî     or   diets, for example  ‚Äî   and to exercise and   drugs, among other interventions. But Dr. Ludwig said that simply cutting calories was not the answer. ‚ÄúThere are no doubt exceptional individuals who can ignore primal biological signals and maintain weight loss for the long term by restricting calories,‚Äù he said, but he added that ‚Äúfor most people, the combination of incessant hunger and slowing metabolism is a recipe for weight regain  ‚Äî   explaining why so few individuals can maintain weight loss for more than a few months. ‚Äù Dr. Rosenbaum agreed. ‚ÄúThe difficulty in keeping weight off reflects biology, not a pathological lack of willpower affecting   of the U. S. A. ,‚Äù he said. Mr. Cahill knows that now. And with his report from Dr. Hall‚Äôs group showing just how much his metabolism had slowed, he stopped blaming himself for his weight gain. ‚ÄúThat shame that was on my shoulders went off,‚Äù he said.",New York Times,0
"Damaged by War, Syria‚Äôs Cultural Sites Rise Anew in France","PARIS  ‚Äî   When the Islamic State was about to be driven out of the ancient city of Palmyra in March, Yves Ubelmann got a call from Syria‚Äôs director of antiquities to come over in a hurry. An architect by training, Mr. Ubelmann, 36, had worked in Syria before the country was engulfed by war. But now there was special urgency for the kind of work his youthful team of architects, mathematicians and designers did from their cramped offices in Paris: producing digital copies of threatened historical sites. Palmyra, parts of it already destroyed by the Islamists who deemed these monuments idolatrous, was still rigged with explosives. So he and Houmam Saad, his Syrian colleague, spent four days flying a drone with a robot camera over the crumbled arches and temples. ‚ÄúDrones with four or six rotors can hover really close and register structural details, every crack and hole, and we can take very precise measurements,‚Äù said Mr. Ubelmann, who founded the company Iconem. ‚ÄúThis is the stuff architects and archaeologists need. ‚Äù They need it in a new push for virtual preservation that scientists, archaeologists and others, like Mr. Ubelmann, are compiling on a large scale. The records could be used to create computer models that would show how monuments and endangered historical sites might one day be restored, repaired or reconstructed. Of special interest today are ancient sites in Syria, and also Iraq, that have suffered from war, looting and the Islamic State. ‚ÄúPalmyra was very difficult,‚Äù Mr. Ubelmann said. ‚ÄúThe terrorists were uploading videos with them blowing up monuments and smashing statues to manipulate public opinion,‚Äù he said. ‚ÄúWe felt the best response was to magnify the pictures of these places and show their splendor and their importance to the culture. It became a war of images. ‚Äù The latest front in that war is in the exhibition halls of the Grand Palais in Paris, where, through Jan. 9, many of the 40, 000 images he and his team took at Palmyra have become the basis for displays. Called ‚ÄúEternal Sites: From Bamiyan to Palmyra,‚Äù the show aims to draw attention to the rising threats to global heritage. To underscore the exhibition‚Äôs political importance, it was opened several weeks ago by President Fran√ßois Hollande of France, who described it as ‚Äúan act of resistance‚Äù against terror and intolerance. Showing the beauty of the Middle Eastern heritage, he said, ‚Äúis the best answer to the Islamist propaganda of hate, destruction and death. ‚Äù   Martinez, the director of the Louvre and the lead curator of the show, said the sites had been chosen because ‚Äúall are under threat from pillaging, neglect or destruction and are not accessible to the public. ‚Äù He said it aimed to mobilize public opinion ‚Äúin the face of the devastation of unique heritage. ‚Äù Besides images from Palmyra, the multimedia show projects enormous   photographs and videos, immersing visitors in different eras, including the ancient Iraqi city of Khorsabad around 700 B. C. an   mosque in Damascus and a medieval Christian citadel. Mr. Ubelmann dismissed any criticism of collaboration with the government of the Syrian president, Bashar  . ‚ÄúWe were working pro bono, not for any government, but to help the archaeologists,‚Äù he said. They shared their work with the Syrian archaeologists, he said, adding, ‚ÄúWe also train our colleagues so they can later do this on their own. ‚Äù What is paramount is memory and potential restoration. In the last year, his team has flown drones over some 20 historic sites in Syria. Recently, it moved into   zones in Iraq, close to the front line in the fight against the Islamic State. The team is now analyzing the war‚Äôs effects on the remains of once thriving cities dating back some 3, 000 years, including Nineveh, Khorsabad and the thrashed temple and palace of Nimrud, where the government drove out the jihadists in November. In 2015, Islamists sent out videos showing militants using sledgehammers to break reliefs of human figures and mythical winged bulls as part of their   campaign. ‚ÄúNimrud was probably the most splendid of the Assyrian cities,‚Äù Layla Abdulkarim, a Syrian architect, said as she analyzed aerial photographs. Using drones in archaeological work is not entirely new, specialists say, but at a recent gathering in Paris researchers from Europe and the Middle East said they were now having to practice ‚Äúwar archaeology,‚Äù that is, collecting reliable data from   areas. The images from the drones in war zones had proved immensely valuable. But these were barely scratching the surface. Before the war, close to 150 archaeological projects were underway, just in Syria, researchers said. Experts from many countries are trying to assess the damage in Syria‚Äôs old cities but also in the area where the Islamic State held sway that is straddling Iraq and Syria, the region that is seen as central to human history and often called the birthplace of modern economics and writing. There is an outcry for data about the havoc wreaked in Yemen by Saudi bombing. ‚ÄúPeople are exchanging satellite images and data on blogs and other research platforms, but we have no real assessment yet because so many ancient sites are not accessible,‚Äù said Pascal Butterlin, a professor of archaeology at the Sorbonne in Paris. Time is of the essence, even in the case of    ruins, Mr. Butterlin said. He has led expeditions for more than 20 years to Mari, near Syria‚Äôs border with Iraq. Before fleeing, the guards at Mari reported that looters had come from Iraq, he said. ‚ÄúWe need to know what places need to be stabilized and how looters have altered the sites,‚Äù he said. ‚ÄúImportant evidence, like clandestine pits, can disappear very quickly through sandstorms and erosion. ‚Äù Cheikhmous Ali, a Syrian archaeologist based in France, who founded the international group the Association for the Protection of Syrian Archaeology, said reports of organized pillaging continued. A first wave of looting began in 2012, Mr. Ali said, and looting has accelerated since 2014 with the arrival of the Islamic State. While jihadists were more motivated to destroy the artifacts, they had also allowed looters to operate in exchange for money. Mr. Ali said he kept an ever changing tally of museums bombed, objects carted off, safes stolen. The exhibition in Paris, which is drawing large crowds, coincides with ‚ÄúHistory Begins in Mesopotamia,‚Äù a show at the Louvre‚Äôs regional museum in Lens. Both exhibitions highlight the French government‚Äôs active concern about cultural damage in Syria, which was briefly controlled by France in the first half of the 20th century. Mr. Hollande has taken a strong interest, condemning the deliberate destruction of patrimony by all sides as ‚Äúwar crimes. ‚Äù This past month, France offered $30 million toward a proposed $100 million fund to protect sites as fighting abates, provide emergency storage for artifacts and eventually rehabilitate monuments. At the ‚ÄúEternal Sites‚Äù opening at the Grand Palais, Mr. Hollande stressed that France was taking in more Syrian refugees trying to protect monuments of great historical and cultural importance did not mean ignoring the suffering of the population. ‚ÄúShould we be concerned about the patrimony?‚Äù he asked. ‚ÄúWhat is more important, saving lives or saving stones? In reality, these two are inseparable. ‚Äù",New York Times,0
House Republicans Fret About Winning Their Health Care Suit,"WASHINGTON  ‚Äî   Congressional Republicans have a new fear when it comes to their    health care lawsuit against the Obama administration: They might win. The incoming Trump administration could choose to no longer defend the executive branch against the suit, which challenges the administration‚Äôs authority to spend billions of dollars on health insurance subsidies for   and   Americans, handing House Republicans a big victory on    issues. But a sudden loss of the disputed subsidies could conceivably cause the health care program to implode, leaving millions of people without access to health insurance before Republicans have prepared a replacement. That could lead to chaos in the insurance market and spur a political backlash just as Republicans gain full control of the government. To stave off that outcome, Republicans could find themselves in the awkward position of appropriating huge sums to temporarily prop up the Obama health care law, angering conservative voters who have been demanding an end to the law for years. In another twist, Donald J. Trump‚Äôs administration, worried about preserving executive branch prerogatives, could choose to fight its Republican allies in the House on some central questions in the dispute. Eager to avoid an ugly political pileup, Republicans on Capitol Hill and the Trump transition team are gaming out how to handle the lawsuit, which, after the election, has been put in limbo until at least late February by the United States Court of Appeals for the District of Columbia Circuit. They are not yet ready to divulge their strategy. ‚ÄúGiven that this pending litigation involves the Obama administration and Congress, it would be inappropriate to comment,‚Äù said Phillip J. Blando, a spokesman for the Trump transition effort. ‚ÄúUpon taking office, the Trump administration will evaluate this case and all related aspects of the Affordable Care Act. ‚Äù In a potentially   decision in 2015, Judge Rosemary M. Collyer ruled that House Republicans had the standing to sue the executive branch over a spending dispute and that the Obama administration had been distributing the health insurance subsidies, in violation of the Constitution, without approval from Congress. The Justice Department, confident that Judge Collyer‚Äôs decision would be reversed, quickly appealed, and the subsidies have remained in place during the appeal. In successfully seeking a temporary halt in the proceedings after Mr. Trump won, House Republicans last month told the court that they ‚Äúand the  ‚Äôs transition team currently are discussing potential options for resolution of this matter, to take effect after the  ‚Äôs inauguration on Jan. 20, 2017. ‚Äù The suspension of the case, House lawyers said, will ‚Äúprovide the   and his future administration time to consider whether to continue prosecuting or to otherwise resolve this appeal. ‚Äù Republican leadership officials in the House acknowledge the possibility of ‚Äúcascading effects‚Äù if the   payments, which have totaled an estimated $13 billion, are suddenly stopped. Insurers that receive the subsidies in exchange for paying    costs such as deductibles and   for eligible consumers could race to drop coverage since they would be losing money. Over all, the loss of the subsidies could destabilize the entire program and cause a lack of confidence that leads other insurers to seek a quick exit as well. Anticipating that the Trump administration might not be inclined to mount a vigorous fight against the House Republicans given the  ‚Äôs dim view of the health care law, a team of lawyers this month sought to intervene in the case on behalf of two participants in the health care program. In their request, the lawyers predicted that a deal between House Republicans and the new administration to dismiss or settle the case ‚Äúwill produce devastating consequences for the individuals who receive these reductions, as well as for the nation‚Äôs health insurance and health care systems generally. ‚Äù No matter what happens, House Republicans say, they want to prevail on two overarching concepts: the congressional power of the purse, and the right of Congress to sue the executive branch if it violates the Constitution regarding that spending power. House Republicans contend that Congress never appropriated the money for the subsidies, as required by the Constitution. In the suit, which was initially championed by John A. Boehner, the House speaker at the time, and later in House committee reports, Republicans asserted that the administration, desperate for the funding, had required the Treasury Department to provide it despite widespread internal skepticism that the spending was proper. The White House said that the spending was a permanent part of the law passed in 2010, and that no annual appropriation was required  ‚Äî   even though the administration initially sought one. Just as important to House Republicans, Judge Collyer found that Congress had the standing to sue the White House on this issue  ‚Äî   a ruling that many legal experts said was flawed  ‚Äî   and they want that precedent to be set to restore congressional leverage over the executive branch. But on spending power and standing, the Trump administration may come under pressure from advocates of presidential authority to fight the House no matter their shared views on health care, since those precedents could have broad repercussions. It is a complicated set of dynamics illustrating how a quick legal victory for the House in the Trump era might come with costs that Republicans never anticipated when they took on the Obama White House.",New York Times,0
An Evaluation of Lightweight Classiﬁcation Methods for Identifying Malicious URLs,"I. INTRODUCTION In recent trends the motivation behind malicious websites has moved towards ﬁnancial gain [1]. This is primarily done through the use of phishing and spam sites that attempt to sell fake goods such as pharmaceuticals or through the use of ”drive-by-downloads”, causing the user to unknowingly install malicious applications [2]. The main outcomes of malicious content can be broadly grouped into the following three categories: • Phishing • Fraudelent advertising • Computer infection for unauthorized use A. Phishing Phishing is an attack whereby an attacker tries to obtain user’s personal information by trying to trick the user into entering identifying and account information for a legitimate service. This method predominantly targets ﬁnancial and pay- ment service sectors as indicated by [3]. Phishing attacks focusing on the ﬁnancial and payment service sectors account for about 71% of the phishing attacks during that time, as indicated by the statistics. B. Fraudulent advertising This method of attack tries to prompt users to buy coun- terfeit goods at low cost. The main vector for this attack is through email spam advertising which will often show a price list as well as a URL to the web page where the goods may be purchased. C. Computer infection for unauthorized use Botnets are primarily propagated in a manner whereby a user installs software which exploits their machine. This can be done via many attack vectors, including that of drive-by- downloads. These are executed when a user follows a link to a web page which then exploits the user’s browser, installing the software in the background without the user’s knowledge or permission. II. STRUCTURE The Obfuscation section of this paper describes the methods whereby the owners of malicious websites try to obfuscate the URLs with the intention of making them more difﬁcult to detect and the different types of obfuscation employed. The next section entitled Counter Measures describes methods employed by the security industry to try to detect these malicious URLs and attempt to warn the user as to their content (or even forcibly stop the user from visiting them). The two broad categories discussed are blacklisting and lexical analysis of the URL itself. The section titled Lightweight Classiﬁcation Algorithms is a discussion around three modern algorithms designed to train perceptrons (using lexical anal- ysis) to identify malicious URLs without the use of external data sources while remaining highly accurate. The algorithms discussed are the Online Perceptron, Conﬁdence Weighted and Adaptive Regularization of Weights methods. The applications of these lightweight classiﬁcation applications are discussed in the section Applications, as well as the work already achieved in our research and work currently underway. The ﬁnal section Conclusion summarises the topics covered in this paper. III. OBFUSCATION One of the challenges faced when identifying malicious URLs is that they are often obfuscated using a variety of methods. Doshi et al. [4] list the following four types of obfuscation which are intended to hide the malicious nature of the web site. • Type I - This type of obfuscation refers to cases where the hostname is replaced with an IP address and in some cases where a port number is used. This can be particularly effective when the site that is being impersonated is somewhere in the URL path and the IP address is represented as hexadecimal value [4]. • Type II - The hostname in the URL has a domain name that appears to be legitimate but usually contains a redirect to another host [4]. • Type III - Again, the host name is obfuscated, but in this form a large string of other valid domains is appended to it. This gives the appearance of a valid hostname [4]. • Type IV - Misspelled or no domain name is given in the URL [4]. The purpose of this obfuscation is to hide the true nature of the URL and to make it appear to be a legitimate website, tricking users into believing that the URL is safe. Obfuscation also intends to make the URL more difﬁcult to detect through automated testing, but that makes these URLs look suspicious to advanced users and make them identiﬁable by machine learning. is this fact however, it IV. COUNTER MEASURES As mentioned in [2], the user is required to follow a URL to become a target. This is where a large amount of research, generated by the security industry, is focused and tries to ﬁnd methods of preventing users from following URLs which may be potentially malicious or contain fraudulent content. A. Blacklists There are several blacklists available which are a collection of malicious URLs and can be queried before visiting a page. Phishtank is one such site and provides a blacklist of malicious URLs which is supplied by the general public and then veriﬁed as having malicious content [5], [6]. This is one method of generating blacklists and can be very accurate as it uses human veriﬁcation. A disadvantage with this method of blacklisting is that it can be slow as a direct result of the veriﬁcation process. Another difﬁculty is that it requires users to discover malicious pages and report them. As a result, new malicious pages simply may not be on the blacklist or may still be waiting for veriﬁcation. Other methods for creating and maintaining blacklists are outlined in [7] and [2] and include honeypot data, web crawlers and heuristic content analysis of the particular webpage. How- ever, these are not always accurate, as indicated by [2], because malicious pages have been known to ’cloak’ themselves and display speciﬁc content depending on who is requesting it. For example, some pages will show completely benign content to IP addresses originating from known security companies and web crawlers, making them almost impossible to detect by these methods. These blacklists can be queried in several ways, examples of this include browser plugins such as those offered by the Microsoft Smart Screen service [8] and Google Safe Browsing service [9], ﬁrewalls, proxies and search engines. Another form of blacklisting occurs in spam ﬁlters which ﬁlter out mail according to a blacklist of addresses. The Google Smart Screen API may be used for the purpose of creating URL blacklist ﬁlters and is available from [9]. An- other method of implementing blacklisting is offered through websites provided by security companies such as Norton [10] where users may enter the URL of a suspect website before visiting it to determine whether it is in the blacklist. This is not an ideal situation as the user has to enter the URL into the site which costs time and will almost deﬁnitely not be done for every link that the user wishes to visit. A more effective solution is to have each link checked as the user is browsing through the use of browser extensions that use blacklists or classiﬁers. B. Classiﬁcation A new method of identifying potentially malicious web sites is through the use of machine learning using Artiﬁcial Neural Networks (ANN) as classiﬁers, such as those presented in [2], [7], [6]. There are different approaches to this and vary in terms of features that are analysed an learning algorithms used to update the classiﬁer. The features analysed by a classiﬁer can be identiﬁed as host-based and lexical, the combination of the two is known as full-featured analysis [6]. 1) Host-based features: Host based features are those that require the use of external sources. The two sources of information used in [6] are WHOIS and Team Cymru which is available from [11]. The following list describes important information outlined in [2] and [7]. • WHOIS data - Details included are registration dates of the domain, registrars and registrants. This allows the classiﬁer to determine how new the domain is and whether or not the domain belongs to an individual already associated with other malicious URLs. Also identiﬁed as important information in [2] is the expiration date and whether or not the WHOIS entry is locked. the longest directory name. • IP address information - [2] uses this information to check whether or not an IP address is in a blacklist and checks to see if the IPs of the A, MX and NS records in within the same AS as each other. This feature is described in [7] as including all identifying information regarding the hosting of the website and includes the IP address preﬁx and the AS number. This allows a speciﬁc ISP’s IP preﬁx to be ﬂagged as malicious by the classiﬁer. Also associated with this data is geolocation of the IP address. • Connection speed - This is cited as being an important is often hosted on private machines factor compromised machines, with low connection speeds such as DSL [7]. usually as malicious content • Domain Name - Included in these properties are the TTL, whether or not the certain keywords exist in the hostname and if it contains an IP address [2]. These properties are common to almost all research regarding classiﬁcation using host-based features. The features represent a valuable set of data and are fairly easy to obtain through automated software. The negative aspect of using these external features for classiﬁcation is that they may incur signiﬁcant additional latency in the case of using the classiﬁer as a browser extension and as such, may not be appropriate on connections where bandwidth is limited [6]. 2) Lexical features: Lexical features of a URL refer to the actual text of a URL and include no external information. These features are useful as malicious URLs often ”look” different than benign ones to experts [2]. Features that belong to this group include numerical information regarding lengths of features, numbers of delimiters and directory structure. This information is useful as it is obfuscation resistant [6]. The model outlined by [6] will be used for the purposes of this research and will be described below. • Automatic features - These features describe the URL in the form of tokens and include items such as the domain name, the ﬁle being accessed, it’s ﬁle type and the arguments passed. top-level domain, directory structure, • Domain name - Length, number of tokens and hyphens are collected. Also, whether the domain name uses and IP address or port number used as binary features that are extracted from the domain name. • Directory - Among the directory structure considerations are how many directory traversals are used, the largest number of delimiters used in a particular directory, and • Filename - Extracted ﬁlename features include the length of the ﬁlename and the number of delimiters • Arguments - A very important factor in the feature extraction is that of the arguments passed to the ﬁle. These features include the number of variables passed, the longest variable value passed and the highest number of delimiters used. C. Conclusion These features, be it host-based, lexical or a combination of both, are then run through a classiﬁer which will then present a prediction as to whether the URL is malicious or benign. This is usually represented as a 1 for malicious or a 0 as benign. V. LIGHTWEIGHT CLASSIFICATION ALGORITHMS Classiﬁers range from state of the art machine learning techniques to simple perceptrons. All classifying algorithms share a common structure based on the structure of the perceptron, but differ in feed forward propagation and back- propagation learning algorithms. While the structure of these simple artiﬁcial neural networks is shared, the learning mecha- nism represents a signiﬁcant difference and greatly affects the effectiveness of the algorithm. Classiﬁcation algorithms which only use the lexical features are known as lightweight classiﬁcation algorithms and have a major advantage over fully featured classiﬁcation algorithms as they do not require the use of external information sources to make a prediction about the safety of a URL. This is a noteworthy factor, especially when due consideration is given to performance on the client’s side of the process, as no additional latency is introduced through the execution of lookups. Another positive factor in this regard is the use of a feed forward mechanism of a neural network, which may be executed efﬁciently, thereby speeding up the process substantially. A. Online Perceptron The structure of a perceptron is known as a single neuron artiﬁcial neural network. It consists of a number of input neurons in a layer which take on the values of the input ﬁelds that they represent. They also have a single neuron in a second layer which is connected to the ﬁrst via a series of weighted connections and acts as a single output layer. The input layer is not considered a layer in neural networks, so the single neuron is considered the only layer and is thus known as a single layer neural network and works as a binary linear classiﬁer. The neural network mechanism can be described as two steps. The ﬁrst step is used by the network to create a prediction given the input supplied and is known as the ”Feed forward” mechanism Firstly, the inputs are multiplied by their respective weights and summed. This process is known as a Linear Combiner and is given by: Linear Combination = n(cid:88) i=1 xiwi (1) The result of this is then passed through a hard limiter which simply adds a bias (or threshold) to the value. This is then compared to 0, and if it is greater than 0, the output is set to 1, else it is set to false. This is known as a step activation function. The entrire feed forward mechanism is given by: (cid:26) 1 0 f (x) = (cid:27) if x · w + b > 0 if x · w + b ≤ 0 (2) Where f (x) represents the prediction, x represents the input vector, w represents the weight vector assigned to the inputs and b represents the bias (or threshold). This activation function is known as a step activation function. In multilayer neural networks, this step is repeated for each neuron on each layer until the all neurons are activated. The ﬁnal layer represents the output and gives the entire network’s prediction based on the supplied input. The second step is known as the learning mechanism and is used in conjunction with the feed forward mechanism to facilitate the learning process. Learning requires that the input data is ”labeled”. This means that a correct answer is supplied with the input so that when the neural network generates a prediction, the correct label can be used to create an error value (_). This value is then used by the back-propagation (or learning) mechanism to update weights in the neural network. This mechanism can be said to move through the neurons in the same way that the feed forward mechanism does, only in reverse. The weight updates for the perceptron are calculated via the following method: _ = Yd _ Y (3) Where _ represents the error, Yd represents the desired output, or the labeled output, and Y represents the actual output of the neuron. The following formula shows how the change in any particular weight may be calculated given the input associated with that weight: ∆w = _ _ xi _ _ (4) Where ∆w represents the change to be made to the weight w and _ is the learning rate for the network. This learning rate is a customizable value that must be larger than 0. Finally the weights are adjusted via the following formula: wt+1 = wt + ∆wt (5) network as it has the ability to over compensate when an error is made. It is shown in [6] that the online perceptron achieves an accuracy of 93% to 96% when trained on data from phishtank [5] and malwarepatrol [12]. It that different classiﬁers should be used for the two different sources as they differ in the nature of the malicious URLs [6]. is indicated, however, The basic model structure of the perceptron is shared by all of the following lightweight classiﬁers that are described here. As already mentioned, they only differ in their method of training, which greatly affects the model’s ability to classify URLs correctly. B. Conﬁdence Weighted Algorithm The problem highlighted with the online perceptron is that it has the tendency to over or under compensate for an error on any particular feature. An example of such an error would oc- cur if a domain is registered with URL features that resembles a legitimate web site. When the classiﬁer predicts (incorrectly) that the URL is benign, it tries to correct itself with the same linear update method for other, more identiﬁable, errors. The Conﬁdence Weighted (CW) algorithm tries to overcome this by maintaining a conﬁdence level in each feature (or input) of the classiﬁer [13]. This is done by keeping a record of the mean µ of the input weights and a covariance matrix _. The mean value for the input weight i is represented by µi while _i represents the algorithm’s conﬁdence in the feature i. These two features alow the algorithm to update itself in proportion to the conﬁdence that it has in any particular feature (smaller changes for features with high conﬁdence and larger changes for ones with low conﬁdence). As indicated by [6], the weight of a feature i at a timestep may be taken as µ for that feature. Classiﬁcation of that URL is done in the same way as that of the perceptron: through the step activation function over the inputs and their respective weights. The learning method of the CW is shown below: (µt+1, _t+1) = arg min µ_ DKL(N (µ, _)(cid:107)N (µt, _t)), s.t.Prw_N (µ,_)[yt(w · xt] ≥ _ (6) (7) This equation is used to calculate the mean vector of the weights and the covariance matrix for the next time step. DKL represents the KL divergence between the normal distribu- tions N (µ, _) and N (µt, _t) and is a standard measure of difference between distributions. It is shown in [6] that the CW algoritm is accurate in about 98% of cases, only 1% less accurate than a fully featured classiﬁcation. This improvement over the online perceptron is due to method in which CW learns by updating through conﬁdence levels in features. This process of using labeled training data is repeated until the neural network’s error reaches an acceptable level. It is important to note at this point that a perceptron using these rules for weight training will only update values if an incorrect prediction is made. Also, the perceptron uses a linear update method which can be detrimental to the performance of the C. Adaptive Regularization of Weights One problem with the CW algorithm, highlighted in [6], is that of noisy training data in terms of labels. If a URL is labeled incorrectly, the CW may label that kind of site as malicious in future. While it does not suffer highly from this problem due to its conﬁdence in features, it can still cause enough of an error to start missing malicious sites. The idea behind Adaptive Regularization of Weights (AROW) is to use the CW’s method of conﬁdence in features, but to modify it in such a way as to be more tolerant of miss-labeled data [14]. The adapted learning method follows: (µt+1, _t+1) = arg min µ_ DKL(N (µ, _)(cid:107)N (µt, _t)) +_1lh2(yt, µ · xt) + _xT t _xt, s.t.Prw_N (µ,_)[yt(w · xt] ≥ _ (8) (9) (10) The extra ﬁelds include _n which are an adjustable parame- ters according to [6] and yt which is the desired prediction of the URL. This update to the CW makes AROW an accurate and more robust classiﬁer, able to handle noisy (incorrectly labeled) training data. The authors of [6] achieved an accuracy of 96% to 97%. VI. APPLICATIONS One of the largest advantages of the lightweight classiﬁca- tion of URLs is that it introduces little overhead in terms of latency and processing. While this fact has been mentioned several times in much of the work presented on the topic and the example use of client-side browser plugins has been mentioned, there are other uses to which this work can be put. Firstly, a web proxy within an organisation may use the classiﬁer to ﬁlter malicious URLs from being visited. This may be used on a standalone basis or it can be used to augment the proxy’s existing blacklist of URLs. If the proxy receives a request which the classiﬁer predicts to be malicious, it may deny the request and add the URL to the blacklist or another database system for further analysis. Another use for these lightweight classiﬁers is that the may be used as an initial ﬁrst pass of large URL logs when trying to analyse a network incident. The URLs may then be ﬂagged for further analysis such as information gathering from external sources, such as WHOIS data. A. Implementation The primary goal of this research is to implement all three of these lightweight classiﬁers and to test them on data collected from real world trafﬁc. Finally, once they have been tested, the classiﬁers will be put to work within a framework designed for use in incident analysis, a browser plugin for use within organisations and a tool designed to work with a proxy to ﬁlter requests for malicious sites. Another possible outcome of this implementation would be to create a plugin for use within email clients that could scan emails for URLs that link to malicious pages. This would be especially useful in trying to combat fraudulent advertising and would not be any different in terms of classiﬁer than any of the other tools mentioned here. B. Training data Le et al [6] trained classiﬁers on grouped data such as the pairing of Phishtank and Yahoo random benign URLs. They showed, through their results, that seperate classiﬁers should be maintained for these different pairs as the data from Phishtank and MalwarePatrol indicate different types of malicious sites (phishing vs. malware) and that these different classiﬁers results in a higher accuracy than the grouping of all malicious URLs and all benign URLs. VII. FUTURE WORK Training data has already been collected from Phishtank, MalwarePatrol and a live proxy, serving thousands of requests a day. In excess of 4000 URLs have been gathered from Phishtank as well as another 4000 from malware patrol. It is important to note that the proxy data is already largely ﬁltered through standard means such as blacklisting. Also, benign URLs have been gathered from Yahoo’s random URL generator (available from [15]). Software has already been written to handle formatting and output of standardized labeled URLs from different sources. A prototype implementation of the Online Perceptron has been written in Matlab. Implementations of the CW and AROW algorithms are nearing completion at the time of writing, and will also be implemented in Matblab. The ﬁnal implementation for testing of these three algorithms will be written in python, using the numpy package for integration with Matlab. Training the classiﬁers will be done in the method suggested by [6], where the data sources will be grouped by the type of malicious content that they list. For this reason, two versions of each classiﬁer will be maintained, allowing these tools to classify a URL in terms of malicious content in addition to phishing content. Once the these alogrithms have been tested, the tools men- tioned in Applications will be implemented. The ﬁrst will be a browser plugin for the Firefox web browser. The second will be a proxy plugin followed by a mail server addon. All of these implementations will be updated through a central server that will query external data sources periodically for new training data with which to update its classiﬁcation models. Finally, a tool will be developed that will enable these classiﬁers to be deployed as plugins for a forensic incident analysis framework. VIII. CONCLUSION The work presented by [2], [7], [5] has shown that the effectiveness of lightweight classiﬁer is only slightly lower than that of fully-featured classiﬁcation. Also, the ability to classify a URL with extremely high accuracy and low overhead means that it is ideal for use in browsers and as a ﬁrst pass analysis of large trafﬁc logs. It is the intention of this research to build these tools and to test their effectiveness in real world situations using real world trafﬁc data, primarily using a proxy server that serves several thousand requests a day and as modular tool built for use in a forensic data gathering framework. These implementations will provide a good understanding of how well algorithms work in real time on real world networks. lightweight classiﬁcation ",Scientific Journal,0
A Software Gateway to Affordable and Effective Information Security Governance in SMMEs,"",,
"tial bu""",Scientific Journal,0,
America's shrimp imports being injected with dangerous chemicals - just to add weight,"America's shrimp imports being injected with dangerous chemicals - just to add weight 
Sunday, October 30, 2016 by: J. D. Heyes Tags: shrimp imports , chemical injections , food fraud (NaturalNews) It seems that the more advanced human civilization becomes, the less we learn about what is and isn't good for our bodies. That's the only way to explain why many commercial food operations are so toxic and hazardous to our health.The UK's Daily Mail is reporting on a commercial shrimp operation in Vietnam where workers have been videotaped injecting them with disgusting gel-like substances in order to make them appear bigger and fresher before they are sold and exported.The video shows factory workers injecting tiger prawns (shrimp) in the head, tail and midsection with a gel substance to make them heavier before they are sold.Vietnam is the largest source of imported shrimp for many countries.The video footage was shot by a Vietnamese television station earlier this year. Since surfacing, it has gone viral online on social media sites like Facebook, with many people expressing disgust and outrage.The substance is often used in food as a thickening agent, in particular for icing. But many people who have watched the video are labeling the Vietnamese factory and its workers as ""shameless"" and dishonest in their attempt to make the shrimp appear bigger and more meaty. In addition, the practice makes one wonder what else is being done to shrimp or other food products, not just in Vietnam, but throughout the region. 'All local shrimp suppliers do this' Environmental protection organization Greenpeace recently released a report titled, ""Dodgy Prawns,"" as an aid for importers of shrimp to find sources that are harvested without using slave labor, methods that are destructive or harmful to ocean ecosystems, or that utilize toxic chemicals.The guide recommended shrimp harvested in and around Australia, but also ""ecologically certified"" black tiger prawns from Vietnam. The organization also warned against all imported vannamei, which very often is sourced from Vietnam, because of concern about mangrove destruction, pollution and the use of invasive species. Natural Blaze , in reporting on the story , noted that workers at the Vietnam shrimp preparation factory in question said they injected the shrimp there because ""all local shrimp suppliers do this.""The site noted further that the largest suppliers of shrimp products in Vietnam import more than half of their raw materials for processing from other countries, including China, which has a well-deserved reputation for unclean, unsanitary food production.In the U.S., about 100 million pounds of shrimp ‚Äì or about 8 percent of the amount of shrimp Americans consume annually ‚Äì comes from Vietnam, which in turn gets most of its shrimp from China, where food regulation is a cruel joke. Added weight means additional profits Natural Blaze reported that after about $150 million worth of shrimp was imported into the U.S. from China between January and October 2015, the U.S. Food and Drug Administration issued an import alert in December of that year. The alert warned importers of the ""presence of new animals' drugs and/or unsafe food additives"" in seafood originating in China, and that included shrimp.The Daily Mail said that workers were injecting the shrimp with a substance known as carboxymethyl cellulose, or CMC. Many say it is not considered harmful because it dissolves in water and becomes a smooth liquid before it is then pumped into the shrimp.But obviously CMC is not a naturally-occurring element either, which means shrimp that it is injected into are no longer organic and natural . And the substance has been linked to diarrhea, abdominal cramps, eye pain and irritation, and lingering changes in vision, Natural Blaze noted.And yet its use in food continues. That's because wholesalers and retailers see a bigger profit margin in artificially inflating the size of shrimp that is then sold by weight. And even if retailers aren't doing the injecting, they are knowingly selling a product that has been injected at the source. Sources:",naturalnews.com,1
Poll Finds Americans' Support For Police Highest In Nearly 50 Years," 
In the past year, Americans have seen police officers ambushed and assassinated, and they have watched as the thin line protecting citizens from criminals has been reviled by groups such as Black Lives Matter.
And now America is taking its stand.
A new Gallup poll shows that respect for the police has hit its highest point in almost 50 years.
 
Related Stories Video: Suspect Beats Female Officer Who Didn‚Äôt Use Weapon Out Of Fear Of Backlash Police Officer Issues A Moving Response To Kaepernick‚Äôs Protest NBA Anthem Singer Drops To Knee During Final Line To Protest Racial Inequality The poll, taken in early October, found that 76 percent of Americans have ‚Äúa great deal‚Äù of respect for police and another 17 percent say they have ‚Äúsome‚Äù respect.
‚ÄúObviously, this violent rhetoric we have seen from BLM has backfired ,‚Äù wrote Andrew Mark Miller on Young Conservatives.
‚ÄúPeople respect the cops more than ever and they clearly see what a difficult job they have,‚Äù he added.
 
Miller said the Black Lives Matter movement denies its true purpose.
‚ÄúAnd don‚Äôt give me this nonsense about how BLM isn‚Äôt anti-police,‚Äù he wrote. ‚ÄúIn fact, many BLM supporters have been out there saying we should eliminate the police force entirely .‚Äù
Miller said the bottom line is clear.
‚ÄúThis poll is a repudiation of the tactics the left has used to demonize law enforcement. No getting around it,‚Äù he wrote.
 
Trending Stories Frustrated With Media Bias, Trump Campaign Takes Its Case Directly To Voters With Nightly Show On Facebook RNC Official Takes CNN Host To Task For Claiming There Is No Media Bias Independent Voters Push Trump To The Front In Florida And Ohio The level of Americans supporting the police that was reflected in the 2016 poll had not been reached since 1967, when urban riots and protest-driven long, hot summers were making cities dangerous and 77 percent of respondents said they gave police ‚Äúa great deal‚Äù of respect. Gallup began surveying Americans about the police in 1965.
The 2016 poll also shows a major change in attitudes from just a year earlier. The 2015 poll showed the highest level to date of Americans lacking confidence in their police officers, at 18 percent.
‚ÄúThe increase in shootings of police coincided with high-profile incidents of law enforcement officials shooting and killing unarmed black men. Despite the flaring of racial tensions after these incidents, respect for local police has increased among both whites and nonwhites,‚Äù Gallup reported in analyzing its results.
‚ÄúThe sharp increase over the past year in professed respect for local law enforcement comes as many police say they feel they are on the defensive ‚Äî both politically and for their lives while they are on duty ‚Äî amid heated national discussions on police brutality and shootings,‚Äù it added.
Gallup said future events could change the level of support enjoyed by police.
‚ÄúIt‚Äôs unclear whether the spike in respect for police will have staying power or if it reflects mostly a reaction to the retaliatory killings against police officers last summer,‚Äù it wrote.
What do you think?",westernjournalism.com,1
Clinton campaign urges FBI to detail new developments in email case,"By Reuters   6:34 pm Democrat Hillary Clinton's top aides on Saturday lashed out at FBI Director James Comey for igniting a new controversy over a long-running private email investigation less than two weeks before the presidential election, saying there was no evidence of wrongdoing and accusing him of spreading ""innuendo.""   
By Roberta Rampton and Steve Holland 
DAYTONA BEACH, Fla./GOLDEN, Colo. (Reuters) ‚Äì Democrat Hillary Clinton‚Äôs top aides on Saturday lashed out at FBI Director James Comey for igniting a new controversy over a long-running private email investigation less than two weeks before the presidential election, saying there was no evidence of wrongdoing and accusing him of spreading ‚Äúinnuendo.‚Äù 
Comey, who has headed the Federal Bureau of Investigation since 2013, notified the U.S. Congress on Friday that the agency is again reviewing emails, even though in July it decided not to seek prosecution of Clinton for her handling of classified materials on a private email server while she was secretary of state. 
John Podesta, who heads Clinton‚Äôs presidential campaign, told reporters that Comey‚Äôs letter to Congress was ‚Äúlight on facts, heavy on innuendo,‚Äù and he urged Comey to ‚Äúcome forward and give those answers to the American public‚Äù about the exact nature of the FBI‚Äôs latest review of emails. 
Sources close to the investigation on Friday said the latest emails were discovered as part of a separate probe into Anthony Weiner, the estranged husband of top Clinton aide Huma Abedin. 
Weiner, a former U.S. congressman from New York, is the target of an FBI investigation into illicit text messages he is alleged to have sent to a 15-year-old girl in North Carolina. 
With the Nov. 8 elections quickly approaching, Republican presidential candidate Donald Trump pounded away at the new FBI development, devoting a large part of a campaign speech in Golden, Colorado, to attacking Clinton and arguing that she is not to be trusted with the presidency. 
‚ÄúHer criminal action was willful, deliberate, intentional and purposeful,‚Äù Trump said, standing in front of hay bales stacked in a horse barn. ‚ÄúHillary set up an illegal server for the obvious purpose of shielding her illegal actions from public disclosure and exposure.‚Äù 
Comey, however, has not provided any details on whether the emails now under review are being seen for the first time by the FBI or the nature of their contents. 
Clinton‚Äôs campaign tried to play down the new review. ‚ÄúThere‚Äôs no evidence of wrongdoing, no charge of wrongdoing,‚Äù Podesta said, in the FBI‚Äôs latest announcement that it was taking ‚Äúappropriate investigative steps‚Äù after learning of emails ‚Äúthat appear to be pertinent‚Äù to the earlier probe. 
Clinton campaign manager Robby Mook, trying to tamp down speculation of a voter backlash this late in the campaign, said Americans had already ‚Äúfactored‚Äù what they knew about the email investigation into how they would cast their ballots. 
‚ÄúWe don‚Äôt see it changing the landscape‚Äù for undecided voters, Mook said. 
Clinton aides also said this latest controversy has further energized her supporters. 
Clinton was campaigning in Florida on Saturday, a battleground state that many analysts say Trump must win in order to have any chance of being elected. 
In recent weeks, Trump has been running behind Clinton in most public opinion polls. 
Justice Department officials, according to a source who asked not to be identified, were opposed to the FBI director‚Äôs letter being sent to Congress and believe his actions conflict with a Justice Department memo outlining instructions that agencies should not to act in ways that could influence elections. 
While Attorney General Loretta Lynch did not discuss the matter directly with Comey, the source said aides were in touch with each other. 
Comey let it be known he felt he had to send the letter as a follow-up to his congressional testimony earlier this year regarding the FBI‚Äôs probe of Clinton‚Äôs emails, the source said. 
Meanwhile, Clinton supporters rallied to her defense following Friday‚Äôs FBI disclosure. 
The Congressional Black Caucus, comprised of about 45 members of the House of Representatives, nearly all Democrats, held a news conference in Columbus, Ohio, and urged Comey to release more details. 
(Reporting by Steve Holland in Golden, Colorado, ans Roberta Rampton in Daytona Beach, Florida; Additional reporting by Mark Hosenball in Washington; Writing by Richard Cowan; Editing by James Dalgleish and Leslie Adlerand Leslie Adler) 
Clinton campaign urges FBI to detail new developments in email case added by Reuters on Sat, Oct 29th, 2016",politicususa.com,1
"Life: If You Love Enamel Pins, You‚Äôll Love This Etsy Shop, And If You Don‚Äôt Care About Enamel Pins, You‚Äôll Explode Over This Picture Of A Bulldog Puppy, And If That Doesn‚Äôt Do It For You, You‚Äôll Lose Your Shit Over This Amazing-Looking Pizza, And If You Don‚Äôt, We‚Äôll Find Something For You","Email If you can‚Äôt get enough enamel pins, then it‚Äôs time to start freaking out, because this Etsy shop is all about them. With hundreds of unique designs for you to mix and match, it‚Äôs easy to get lost in this enamel fan‚Äôs wonderland. Check it out: Or maybe enamel isn‚Äôt for you. Not a problem. Forget about the pins, in that case, and get ready to explode the moment you see this little guy: A ! He‚Äôs a bulldog puppy, he‚Äôs 2 months old‚Äîoh, and did we mention he has the most adorable face anyone has ever seen? Bulldog isn‚Äôt doing the trick? Hey, that‚Äôs totally okay! Let‚Äôs move on. Here‚Äôs an incredible pizza. And here‚Äôs a grilled cheese you need in your belly ASAP. We have collected it for you. It is here to annihilate your mind. How‚Äôs that? Are you now slobbering uncontrollably from the food? Has the food melted you to a quivering puddle of yes? If not, keep scrolling, because we‚Äôre just getting started. We will find just the thing. What about this diabolical optical illusion that is going to destroy your mind. Total brain collapse in three‚Ä¶two‚Ä¶one‚Ä¶ Did that work? Surely you have lost it and you can‚Äôt even handle it and you love it so much it is so you. No? Fine‚Ä¶What else‚Ä¶Perhaps this is the thing that will wreck you at last: a badass vintage car. Go ahead and flip out: Still nothing? You are not yet blowing up with joy? Okay. Don‚Äôt worry. The GIF of the dog who is trapped in the toilet will now transform your body into rubble: Jesus. Let‚Äôs go back to the enamel store for a sec, maybe we scooted off it too quickly. It really is a great store. Hard enamel. Soft enamel. Lapel pins. Regular old pin-pins. If you like enamel AT ALL then it should only take one look at this set of OMG-worthy pins and you‚Äôll be out of control with joy: What?? What do you want from us? What do you need? We will collect it here. You crave Bill Murray in public? Here is Bill Murray in public: Beautiful pic of biggest waterfall? There. It is done. Look. Look at these things. There must be",clickhole.com,1
Amber: A Zero-Interaction Honeypot and Network Enforcer with Modular Intelligence ,"INTRODUCTION Antivirus, firewalls, intrusion detection/prevention systems and email filters, all share the trait that they implement their information the information that they are presented with. enhancements by analyzing security Antivirus software uses signature repositories to compare executed code to known bad snippets of code, labelling them as viruses [1]. Firewalls traditionally define what ports and services are allowed to access a network node and block everything else [2]. Intrusion protection systems (IPS) use the same principal as antivirus, inspecting network traffic and attempts to match known bad sequences to incoming traffic [3]. Email filters make a decision on whether an email is spam based on numerous factors such as destination, source and content [4]. Each one of those basic information security controls has a list of predefined patterns or actions that have been classified as malicious, and when they detect them they take action based on the severity of the identified threat [5]. This process of decision-making can be referred to as Decision through Detection (DtD), because it requires the information security control to base its decision on the behaviour of the information flow that it is presented. If the behaviour matches a schematic that is seen as being malicious, the security control is triggered. The process of building a behavioural database is intensive and prone to both false positives and negatives, as detailed in Cohen’s Models of Practical Defenses Against Computer Viruses [1]. A honeypot, however, is an information security system that has no productive business use, because it occupies otherwise empty information space. It is normally classified as either low or high interactive, referring to the amount of maintenance that the honeypot needs to add value. Therefore any interaction with the honeypot is automatically suspicious, regardless of the behavioural traits of the interaction [6][7]. By not needing to take into consideration the behaviour of the analysed data flows, honeypots use a different decision system based on the presence of information and not the behaviour. Decision through Presence (DtP) is unique to honeypots by virtue of its links to the surrounding information resources. Network Associate’s Cybercop Sting, released 14 July 1998, is the earliest version of a commercial honeypot, originally referred to as a decoy system [8]. It offered consumers a way to audit and monitor intruders before legitimate data and data systems were at risk. Prior to that, Fred Cohen & Associates released a package called The Deception Toolkit. It provided an open source alternative to Cybercop Sting, and enabled the open source community to experiment with the idea of allowing intruders to compromise assets, using the interactions as a way of learning how to defend against them [9]. Cohens’s toolkit formed a basis from which the community could actively start researching the field of honeypots as his system was a collection of Perl scripts. One such researcher was Lance Spitzner, founder of the Honeynet Project, and author of Honeypots: Tracking Hackers. Spitzner’s book served as a formal foundation for honeypots. It contains real world examples of why honeypots are useful, as well as the core concepts of honeypots: occupying unused and unproductive Internet space, with the intent of luring and learning about the actions of malicious attackers [6]. In his paper, To Build a Honeypot, Spitzner details the basics of building a server that can be used to record the steps and actions taken by a malicious intruder [10]. The server (or honeypot) would be separated from the valued network so that it could safely sustain damage and report back on how that damage was inflicted. To Build a Honeypot led Spitzner and co-author Niels Provos to create a novel and simple way of deploying a honeypot: through Honeyd, a virtual honeypot daemon [7]. Honeyd can simulate a TCP stack as to further fool would be attackers, and can be configured with a number of templates, which mirror certain grades of common production servers. This paper will attempt to bring the concept of honeypots into a working, security model that is both efficient and useful. The paper will introduce and dissect current security models, and then bridge them into a new model. Certain tests were then used to test the validity of the new security model. Section II will introduce and give an in depth explanation of the two security models, as well as the two distinct phases within each of the models. Section III explains how the empirical test bed was designed to address the critical success factors, the technical design and implementation and the results of the test suite. After discussing the findings, the possibility of expanding the capabilities of the security control is evaluated and tested. Section IV acts as a summation of this research. II. DECISION THROUGHT PRESENCE AND DECISION THROUGH DETECTION A. Discovery Phase The basic principle of this research is to construct a system that is able to combine the most useful pieces of the two security models, namely Decision through Detection (DtD), and Decision through Presence (DtP). While the drawbacks of antivirus and traditional firewall’s DtD security model have been mentioned above, their advantages have outweighed their disadvantages, which is testament to their success over the years. While it does require a great deal of human capital and technical infrastructure to build signatures, pattern files and a detection engine to run them [11], the model has the advantage of scaling perfectly as defensive nodes increase. Because any work or investment that is done on one defensive node can also be applied to another similar defensive node, the total cost decreases as nodes are added (1). The only unit cost is that associated with the delivery of the signature to the node (1), which has become relatively small with the cost and speed of data transfer over the Internet [12]. Cost to Detect = (RC / n) + (DC * n)_ ____ RC: research cost n: number of nodes DC: distribution cost It is this scaling that has facilitated the commoditization of DtD products, and is the reason for their commercial success [13]. Distributing the cost of research amongst all the users of the DtD system also means that each individual node controller receives a certain degree of improvement to its security posture, without having to assign research and development resources to it. In contrast to this, systems that make use of the DtP security model have a cost of zero, because the evaluation is simply that of existence. B. Action Phase DtP loses its cost advantage in the second, Action, phase because its Discovery phase is based only on whether something had a presence, and not the nature of that presence. It is unknown at this stage, in a DtP model, if the information that is being seen is benign or malicious. The burden then falls on the Action phase of the security model to determine how to proceed with the data. In the case of honeypots, the system owner would deploy the honeypot to gather information on possible new attacks, and use it as a research basis to develop a counter-attack or to modify existing defences [14]. The value of this kind of action is that the output is more significant to the organisation housing the honeypot because it is an information stream that is actively targeting their systems. But the cost equation for the DtP security model does create barriers. As every context requires analysis to add any value, the cost of maintaining a DtP system as a going concern is extremely high, both in time and skilled labour, as shown in (2). Every context (C) of incoming information above a certain threshold would need to be researched to extract information and value out of it, and the process needs to be repeated for every node (n). Cost of Action = (C * threshold) * RCperC * n (2) C: information context RCperC: research cost per information context N: number of nodes DtD, on the other hand, benefits from the higher cost of the earlier Discovery phase, because the signature that was used to detect the context has already decided on its nature, be it benign or malicious. But using centralized research does have an additional, intrinsic cost associated with it. As certain requirements need to be generalized for the signature to be significant to as many nodes as possible, consumers of centralised research pay an additional fee for the lack of specialization. This fee materializes as an increased rate of false positives, and is levied on each deployed node (3).  Cost of Action = n * FPRate (3) n: number of nodes FPRate: False Positive Rate C. Security Model Synopsis The above equations show that both security models have competitive advantages, defined in terms of cost, in different phases. The DtD security model has the upper hand in the Action phase because most of the work has already been done in the earlier Discovery phase, but DtP is superior in the Discovery phase due to its simplistic criteria. To help minimise the cost in both phases, it would then be preferable for a system to utilise a hybrid security model that uses the Discovery phase from DtP and the Action phase from DtD where possible. III. AMBER – TECHNICAL IMPLEMENTATION A. Technical Design A technical solution was prototyped that would aim to build a fully functional hybrid system to test how a system would react in a real world environment and with harsh constraints placed on it. The system was named Amber. Following the security models previously discussed, Amber would need to employ a Discovery phase that is similar to the DtP security model approach (because of the low cost associated with it) but cannot use the DtP Action phase because of the prohibitive costs in that phase. The DtD model is preferable in the Action phase, but because it relies so heavily on its own Discovery phase, it is not possible to implement it on the back of a DtP based Discovery phase. As superior as the DtD security model’s Action phase is, it is also hampered by the cost to resolve false positive incidents. Any system based on this phase needs to aimed to reduce false positives to zero. Lastly, Amber still needs to improve the overall security posture of the environment that it is connected to. The Critical Success Factors (CSF) for the technical system would be as follows: _ Zero-interactive system (reducing the research cost for both Discovery and Action phase to near zero) _ A false positive rate as close to zero as possible Improves security posture of the environment _ B. Conceptual Implementation 1) Zero-Interaction In order to fulfil the zero-interaction CSF, Amber needs to implement the DtP security model’s Discovery phase, which requires the system to occupy unproductive Internet space. A suitable piece of Internet space was needed that served productive services that also had unallocated (and thus unproductive) space. the Internet, but to A /24 IPv4 subnet which housed a group of web servers, each housing multiple websites was identified as the segment to which Amber would play custodian to. An IP address that was not utilised by the web servers was selected and assigned to Amber as the non-productive Internet space that it would use to collect contexts. A webhosting subnet was chosen due to the low likelihood of any accidental neighbouring traffic, as websites are mostly accessed via their DNS name and not directly through the IP [15]. To maintain the zero-interaction CSF, a suitable Action phase had to be chosen that would lock into the DtP security model’s Presence based Discovery phase, but that did not suffer from the high research costs of its corresponding Action phase. The DtD security model Action plan was also not suitable because it relied too heavily on research done during its own Discovery phase, which is then not available as the DtP simple presence based Discovery phase was utilised. But it is possible to use the core idea of the DtD security model’s Action phase: leveraging off the work done in the previous phase. By identifying any traits during the DtP Discovery phase and determining which could be carried over into the Action phase, it was theorized that the cornerstone of a presence based Discovery phase (namely the idea that nothing should be interacting with the system as it resides on a non- productive Internet space) could also be used in the Action phase. If an information context was entering the Amber system while it resided on a piece of non-productive Internet space, the context itself could be judged as also being non-productive, and that judgement could be carried over to the Action phase. This theory allows Amber to use the rapid (and resource cheap) Discovery phase reasoning of the DtP security model in both phases, and links them together with the leveraging theory of the DtD security model. That is to say, it creates a hybrid security model, using the best of the DtP and DtD security models. 2) Zero False Positives The second CSF is a symptom of generalized classification – a part of centralized research. Systems that use the DtD security model are prone to it because the Detection done during the Discovery phase is based on a set of generalized criteria that try to suit as many nodes as possible. Given that the Action phase depends heavily on the research done during the Discovery phase, actions taken in phase two are as prone to false positives. As stated earlier, systems that use the DtP model are resilient to false positives because the Discovery phase is tailored to look at contexts that enter zones that have no productive use, and that are specific to the environment that they are attached to. It combats false positives because each context needs to be fully analysed as part of the Action phase, meaning every context requires manual intervention. Amber loses the confidence built by analysing every context, but because the Action is based on the false positive resilient Presence based Discovery phase, any actions taken in the Action phase would be based on an information context that had no productive use, making the probability of false positive extremely uncommon. To put it another way: given that all interactions during the DtP Discovery phase are unwarranted, it is unlikely for any action to return as a false positive in the Action phase, i.e. as non-malicious. Improves the Security Posture of the Environment 3) The system is still an information security system, which means that through all its zero-interaction optimizations it still needs to improve the security posture of the environment that it is associated with. Without satisfying this requirement, any system would be useless. As a standalone unit, Amber is not able to take action against anything that passes through the Action phase, because Amber does not use productive Internet space. As Amber is not attached inline or in front of, and does not share Internet space with, any productive entity, any action that Amber takes would only improve the security posture of the unproductive Internet space. To overcome this Amber would need to rely on an external enforcer that is situated in front of a productive Internet portion of the segment and that could take action on Amber’s behalf. Amber would log the source IP addresses of systems that targeted it and pass those IP’s in the form of a threat stream to the enforcer (such as a firewall). The firewall would then be able to drop packets that originated from the source IP addresses and thereby increase the security posture of the environment that the firewall was protecting. In a real world situation the inclusion of a firewall would be ideal, but because Amber underwent testing as a proof of concept intervening or interfering with the environment would limit the ability to fully test the application. live environment, actively in a As a proxy test, an inline system could be emulated by mirroring the uplink port on the switching device, and a packet sniffer could be attached to that port. The packet sniffer would be in a position to analyse all the data on the network, which would allow it to compare the threat IP address stream generated by Amber to all the traffic on the segment. It would then be possible to see if an IP address attempted to access productive Internet space after it had been marked by Amber as being a threat. It can then be assumed that if an external network enforcer had been in place (such as a firewall) the IP address would have been hindered and the overall security posture of the environment would have been improved. C. Technical Implementation Amber was to be built on top of an established operating system to save time and resources, and would utilize that OS’s available network stack and common packages wherever possible. Amber was written in a combination of Perl, Python and Bash depending on where the strength and development speed of each stage lay. 1) Discovery Phase: Listener In order for Amber to adhere to a Discovery phase that is similar to that of a DtP security model, it would need the ability to open an arbitrary amount of TCP/IP ports and accept connections to those ports. The connection’s source IP address then needs to be recorded for use in the Action phase. A network daemon was written in Perl and deployed onto an Ubuntu 12.04 x64 server. It would spawn a set of common TCP/IP ports on the listening interface and record the source IP address of those connections. The follow common Internet services ports were chosen based on the potential attack surface that each represents [16]: 445/tcp | (ms-ds) 135/tcp | (ms-rpc) 3389/tcp | (ms-term-serv) _ _ _ This introduced a problem into Amber that made it vulnerable to spoofing attacks [17]. If an attack knew the listening IP address of Amber, they could craft a packet with a source IP address that was not their own and send it to Amber. Amber would respond by classifying the attacker-chosen source IP address as a threat and streaming it to the network enforcers. This would allow an attacker to selectively deny a victim services that were protected by an Amber threat stream enabled network enforcer. To combat this the Linux kernel firewall was used to detect any incoming transactions that successfully established a TCP/IP three-way handshake through the SYN, SYN-ACK, ACK process [18]. If an incoming connection managed to establish a three-way handshake then it would indicate that the source IP address provided is the actual IP address from where the connection originated. While checking for the three-way TCP/IP handshake is a valid way to mark a connection as being established and not spoofed, an attacker would still be able to fool the method by guessing the correct SYN-ACK sequence number, as proposed by Bernstein in his paper on SYN Cookies [19]. Since then, Linux has by default enabled the proposed remedy for this, which are SYN cookies. But, in a paper by Dan Kaminsky, the author revisited the solution and noted that they are susceptible to a brute force attack due to the increase in possible Internet throughput [20]. It would take approximately eight million packets to successfully brute force a SYN cookie-protected system, which could take less than fifteen minutes on non-specialised hardware (assuming a packet rate of 100,000 packets per second). To address this, Amber incorporates the netfilter packet limit module, which limits the amount of packets entering the system. By instituting a limit of 800 ACK packets per second an attacker would have to expend 2.2 hours of brute forcing to successfully guess a SYN Cookie. To further defend against brute forcing SYN Cookies, Amber’s connection timeout was set to 600 seconds. An attacker could effectively launch a Denial of Service attack against Amber because of the low packet per second limit, but because Amber is not attached to any productive Internet space, a DoS attack directly against it would not harm the segment. 2) Action phase: Streamer Once the Listen has validated a source IP address, the Discovery phase ends and the Action phase starts. In order to implement the findings of the Discovery phase the IP address needs to be recorded and passed on. As Amber is unable to take action against IP addresses due to the restrictions placed on the environment in which it functions, the IP addresses are stored locally on Amber and compared to a window of segment traffic via the attached packet sniffer. The packet sniffer uses a separate Ubuntu 12.04 x64 as its base operating system, and uses the tcpdump binary to capture all incoming traffic to the segment via a mirrored uplink port. A truncated version of the packet is captured because an analysis of the complete packet contents is not needed. Amber would normally be active for 24 hours, during which time the packet sniffer would record all the traffic entering the segment. At the end of the 24 hour period, Amber’s daemon would have logged all the validated source IP addresses locally, and this list would be exported to the packet sniffer where it would search through its packet captures over the same period. For each IP address logged by Amber, the packet sniffer would check if the same IP address attempted to target another host on the segment, an action that would not have been possible if Amber’s threat IP address stream had been connected to an upstream network enforcer. Therefore, if the packet sniffer finds a source IP address accessing another system on the segment after it has already come in contact with Amber, it is reasonable to say that Amber can possibly improve the security posture of the segment, if used in combination with an upstream network enforcer. 3) Results The 24 hour long test was run a total of ten times over a six-month period from August 2012 to January 2013. Each source IP address that Amber validated and passed onto the packet sniffer was identified, on average, as accessing the rest of the segment 6.89 times. Therefore, Amber has the ability to improve the segment security posture, as measured by the method described above. Of the ports that the Listener opened the most frequently targeted was 3389/tcp, or Microsoft Terminal Services. _ _ _ 3389/tcp – 343 instances 4455/tcp – 199 instances 135/tcp – 50 instances D. Adding modular intelligence After proving that Amber had the ability to improve the security of a segment, the question was asked if the gains in security posture would scale proportionately to an increase in Amber nodes that were able to generate a validated source IP address threat stream. Unfortunately, due to limited IP address space on a segment and Amber’s DtP-based Discovery phase needing Internet space that had no other productive use, increasing the amount of Amber nodes on a segment reduces the amount of usable Internet space for that segment. A test was done where a second Amber node was added to the same segment, but the increased security posture (as measured by any additional validated source IP addresses that were seen by the second node, and which attempted to target another system on the segment) was less than 1%. Based on this minor increase in security posture by adding a second Amber node, and each node requiring the conversion of productive Internet space to unproductive space, it was decided that the practice of additional Amber nodes on the same segment was impractical. However, if the trade off of productive to unproductive Internet space could be offset, then any increase in the security posture could be relatively significant. It was also theorized that sampling Internet space that was geographically removed from the first node, would yield a validated source IP address threat stream that was significantly different for the first node’s stream. Two additional Amber nodes were set up on geographical areas that were unrelated to the first. The original Amber node resided in South Africa (named za-amber), while node 2 and 3 were placed in the United States (named us6) and Germany (named de3) respectively. Early on it became apparent that there might be a limited amount of correlation between threats recorded in these different locations. Reviewing the threat streams generated by us6 and de3 showed that none of their validated source IP addresses were appearing on za-amber’s packet sniffer. A new, precursor hypothesis was devised: is there any correlation between attacks seen between the US, DE and ZA geographical regions? This would act as a sanity check to the hypothesis of using distributed nodes to increase the security posture of a single segment. This updated hypothesis would also be trivial to test, and if it were false, then the distributed node hypothesis would automatically be false, due to its reliance on a correlation between regions to exist. It is impractical to monitor all the segments that the distributed Amber nodes live on, but a similar test for correlation could be achieved by comparing the source IP address streams generated by each node. Each node is built robustly and is able to function without intervention, reverting to a known trust state if a part of it fails, giving a high rate of listening coverage. A central command and controller server was built that would be able to log into each node and pull down the source IP address threat streams, archive it locally, and compare each IP address to the full history of IP addresses across all managed nodes.  1) Findings The correlation test was run over a period of three months, with each of the nodes operating continuously, minus some minor downtime due to hosting infrastructure. In addition to the original three TCP/IP ports being spawned by the listener, TCP/IP port 80 and 443 were used to gather data. These two ports were included to increase the potential connection surface. Since the objective was to measure correlation between the different geographic areas, incorporating these high traffic Internet ports [21] aids the objective without sacrificing any of the test’s integrity. While the use of caching servers have the potential to corrupt the tests that measure Amber’s ability to improve the security posture of a segment (caching servers hide the source addresses for numerous Internet users), it does not interfere with this hypothesis. The three nodes measured connections: the following successful _ ZA-Amber IP: 1132 IP addresses _ US6 IP: 3356 IP addresses _ DE3 IP: 557 IP addresses Each source IP addresses stream, from each of the nodes was compared to the other two node’s IP address streams. A match would indicate that a particular IP address was discovered by both nodes, and showed that there is a correlation between the nodes. TABLE I. CROSS CONNECTIONS Nodes ZA-Amber US6 DE3 ZA-Amber US6 DE3 0 3 0 11 3 11 The data in Table I shows that there was a very small set of IP addresses that were logged by two or more nodes. The German and United States based nodes logged eleven common source IP addresses, while the South African node only logged three IP addresses that also attempted a connection with the German node. The US and South African nodes didn’t share a single source IP address. Based on this investigation it would seem that there is no value derived from distributing Amber nodes in geographically different locations. While these results are on a minor scale, it does invite the question as to whether or not there is any value in applying threat countermeasures in one region based on intelligence collected in a different region. There are numerous commercial products that offer this service [22] and while it was not possible to test this at the same scale as the large security vendors, it would seem that the ability of a small threat stream to be useful outside of its region is extremely limited. IV. CONCLUSION to fulfil The two security models (Decision through Detection and Decision through Presence) both aim to improve the security posture of the assets that they are tasked to protect, but each manages that custodianship with a different methodology and different costs. Despite their differences, each of the security models can be factorized into two distinct phases: the Discovery phase and the Action phase. Each model provides a distinct cost advantage in one of the two phases, thus bestowing a differing competitive advantage to both. In the traditional implementations, DtP is able to assess a threat in the Discovery phase at a relatively low cost, but falls short in the Action phase due to the amount of intensive research that is required. DtD on the other hand requires a large amount of research in the Discovery phase. And while the research can be centralized and distributed to an infinite number of nodes, it does also increase the likelihood of false positives being detected as threats. Despite DtD’s expensive Discovery phase, it has a superior Action phase as it leverages off the research completed in its Discovery phase. Amber attempts to create a hybrid system that merges the strengths of both security models, while still improving the overall security posture of the assets under its custodianship. Using the DtP security model’s Discovery phase and the DtD security model’s Action phase methodologies, Amber was able to improve the security posture of the environment that it was connected to, as measured by its ability to detect a source IP that would try to connect to multiple other hosts on the segment. It was not practical to increase the amount of nodes on a single segment due to each node needing a piece of unused Internet space, but it was theorized that multiple nodes could be interconnected through a command and control server, if they were located in dissimilar geographically regions. A preliminary correlation test was devised that tested whether there was any link between source IP addresses logged between the multiple nodes, before a test was done to see if the security posture could be increased by increasing the nodes. The research yielded mixed results, showing that there was almost no link between source IP addresses logged by the different nodes. This means that there was no advantage to extending Amber’s source IP address stream to include that of multiple nodes across different geographical regions. This also questions the usefulness of initiatives that attempt to apply threat data from one region onto other regions. ",Scientific Journal,0
"Decorated ‚ÄòHero‚Äô Cop Caught Using His Authority to Steal $170,000 in State Fees","Home / Be The Change / Government Corruption / Decorated ‚ÄòHero‚Äô Cop Caught Using His Authority to Steal $170,000 in State Fees Decorated ‚ÄòHero‚Äô Cop Caught Using His Authority to Steal $170,000 in State Fees John Vibes October 28, 2016 Leave a comment 
Detroit, MI ‚Äì Disgraced Michigan State Trooper Seth Swanson was charged with embezzlement this week for pocketing thousands in false fees. 
The 31-year-old trooper allegedly stole $170,100 in vehicle fees through an inspections scheme that he ran, where he would forge documentation on potentially stolen vehicles. 
The Michigan Attorney General‚Äôs Office issued the following state ment detailing Swanson‚Äôs theft operation: 
‚ÄúPolice officers are given great trust and responsibility, and for that reason are held to a higher standard. When you break the trust you are given and in the process break the law, there are consequences, no matter who you are or what your profession. I want to thank the Michigan State Police and FBI‚Äôs Detroit Area Public Corruption Task Force for their hard work on this investigation.‚Äù 
According to investigators, Swanson was a state-certified salvage vehicle inspector since 2011. As an inspector, Swanson was responsible for overseeing salvage vehicle inspections, during which a $100 fee is collected. For over a year, Swanson allegedly pocketed these fees and forged the forms that authorized the salvage. 
Swanson is accused of applying this scam to 1,701 vehicles, bringing in a total of $170,100. 
After he was charged, Swanson was forced to resign from the police department. 
Police spokesperson Andrea Bitely told reporters that ‚ÄúOur office , in conjunction with the Michigan State Police and Secretary of State, are working together to make sure that all vehicles involved in this case have, actually have a proper salvage vehicle inspection, and we‚Äôll contact the registered owners of the vehicles to make sure we arrange for now inspection in a timely manner.‚Äù 
Prior to his crimes as an inspector, Swanson was praised in the media as a ‚Äúhero‚Äù in 2013 for being one of the first responders to a large pile-up. 
Swanson and his lawyers are attempting to use his past media fame as a defense in this most recent case, despite the fact that it is entirely irrelevant. 
Defense attorney John Freeman said that Swanson is still a ‚Äúhero.‚Äù 
‚ÄúThese charges don‚Äôt detract from the fact that Trooper Swanson was a real-life hero and was a good trooper. It‚Äôs easy for people to lose sight of that fact,‚Äù Freeman said. 
Swanson was released on $10,000 bond and is currently awaiting trial. 
Below is a video from 2013 in which Swanson was hailed as a hero. John Vibes is an author and researcher who organizes a number of large events including the Free Your Mind Conference. He also has a publishing company where he offers a censorship free platform for both fiction and non-fiction writers. You can contact him and stay connected to his work at his Facebook page. John is currently battling cancer naturally , without any chemo or radiation, and will be working to help others through his experience, if you wish to contribute to his treatments please donate here . Share Social Trending",thefreethoughtproject.com,1
Guidelines for Procedures of a Harmonised Digital Forensic Process in Network Forensics ,"INTRODUCTION Computer crime has been a challenge since the dawn of the Internet. In most cases perpetrators are successful in breaking security, it is impossible for a system to be 100% secure. All systems are prone to security breach, no matter how strong the security mechanisms that are in place. Whenever such incidences occur, the services of digital forensic investigators are required. The ability of digital forensic investigators to perform their function is heavily reliant on their ability to Sipho Ngobeni Defence Peace and Security and Security Council for Scientific and Industrial Research (CSIR) Pretoria, South Africa sngobeni@csir.co.za Thomas Fogwill Meraka Institute Council for Scientific and Industrial Research (CSIR) Pretoria, South Africa tfogwill@csir.co.za acquire digital evidence from computer systems and network devices. Unfortunately, computer system technologies are changing constantly, which has a direct impact on the ability of forensic investigators to identify and acquire digital evidence. This problem arises from the fact that new technologies in computer systems come with new data storage formats and storage locations that differ from the ordinary formats and locations with which digital forensic investigators have been accustomed. Criminals are constantly studying systems and developing new techniques to break into the latest systems, software products and patches. Digital forensic investigators, on the other hand, struggle to keep up with new developments in technology. This paper is one of a series of papers that aim to provide guidelines for digital forensic procedures in the cloud. The first paper that focuses on live digital forensics has been completed and submitted. The current paper focuses on digital forensic procedures in network forensics. The paper is organised as follows: Section II provides a brief background on network forensics, followed by a discussion of the harmonised digital forensic process and challenges that exist in network forensics. In Section III the authors present the proposed digital forensic procedures for network forensics. Section IV concludes the paper and also presents future work. II. BACKGROUND The authors’ ultimate goal is to study digital forensics in a cloud computing environment. Cloud computing is a new computing paradigm that builds upon virtualisation technology to provide infrastructure, platform and software as services [1– 3]. Cloud computing presents challenges to digital forensic investigators due to its virtualised and distributed nature. The goal therefore is to provide guidelines for digital forensic procedures and to provide a digital forensic solution in the cloud. The current paper focuses on digital forensic procedures for a network environment. Network forensic procedures form part of the procedures that need to be carried out in a cloud environment. In this section, the authors present a brief background on network forensics and network forensic challenges. The authors also present the harmonised digital forensic process on which the procedures presented in this paper are based. A. Network Forensics A number of different network types exist these days, but they all originate from two basic ones: Local Area Networks (LANs) and Wide Area Networks (WANs) [4]. These networks can also be deployed as wireless networks (e.g. WLAN), wired networks (e.g. Ethernet) and virtual networks (e.g. connection among virtual machines). In the context of cloud forensics, the network layer is a fertile ground from which digital evidence can be collected as all communications with cloud services occur via a local network or a wide area network (e.g. Internet). Digital evidence or data that can be obtained from the network may include the source and destination address of any communication, as well as the data (Internet Protocol (IP) packets) that is transmitted during the communication session itself. There are various key locations in the network from which such data can be captured and stored. These locations include network routers, firewalls and even workstations or network accessing devices. Information collected from these locations can be used for subsequent digital forensic purposes, if there exists a case. The field of network forensics, one of the forensics fields, is still in the limelight as a new emerging area of academic interest. Traditional computer forensics generally involves data acquisition from a storage medium such as a hard drive, while network forensics encompasses the capture, recording and analysis of network traffic that can be used for digital forensics purposes [5]. To conduct network forensics, especially in a cloud computing environment, one would need to follow a standardised digital forensic procedure; however, currently there is no such procedure. It is for this reason that the authors propose guidelines for a harmonised digital forensic process that would be applicable in the cloud computing environment. B. Network Forensic Challenges Network forensics presents a number of challenges. One common identifiable challenge is that generally there is only one opportunity to collect network traffic as it traverses the network. Hence, inadequate digital evidence collection may result in the loss of vital digital evidence that may not be recoverable. Also, since networks communicate data before transmission takes place, it becomes a challenge to reconstruct the large number of packet pieces to obtain its original form. Furthermore, network traffic comprises of various protocols and media types, which in turn add complexity to the already complicated source of digital evidence [6]. Fortunately, quite a number of tools have been designed to capture and analyse network traffic. As most of them were intended for network troubleshooting and identifying problems in a network instead of digital evidence processing, they have specific shortcomings from a network forensics point of view [7]. Another challenge that face forensic investigator while carrying out forensic investigations in a network environment is a lack of standardised procedures. There are efforts by other researchers [8–10] aimed at standardising network forensic procedures but they leave a lot to be desired. In the next section we present a harmonised digital forensic process on which the procedures proposed in this paper are based. C. Harmonised digital forensics process The harmonised digital forensics process is presented in ISO/IEC 27043 draft standard [11]. It comprises of eleven phases, i.e. planning; preparation; incident detection; first response; incident scene documentation; potential evidence identification; evidence collection; evidence storage; analysis, and presentation. In addition to the eleven phases shown in Figure 1, six actions run in parallel with these phases, namely obtaining authorisation, documentation, information flow, preserving the chain of evidence and interaction with physical evidence. In variance to the harmonised digital forensics procedures for network forensics proposed in [11], we consider and adjust only the first ten phases of the process in this paper. The presentation phase (the last phase) does not differ in network forensics and can be carried out as described in [11]. In the next section the authors present in detail the procedures that need to be carried out when conducting a digital forensics investigation in a network. III. NETWORK FORENSIC PROCEDURES Procedures presented in this paper are a sequence of activities that need to be carried out in their presented order to accomplish each phase of the digital forensic process. A lack of standardised forensic procedures leads to lack of confidence in an investigation being carried out [12]. In a worst-case scenario this increases the chances of evidence being thrown out of court or being not admissible at a hearing. We therefore address this issue by proposing procedures that can be used to conduct a successful investigation in a network environment. The procedures are depicted in Figure 1 as well as presented in the sections that follow. Each subsection contains a brief description of a harmonised digital forensics process phase. This is then followed by a bulleted list of the more specific digital forensic procedures. D. Incident detection phase This phase deals with the actual occurrence of an incident that requires forensic investigation. It comprises only one action, namely the reporting of an incident. _ Incident reporting. This action involves notifying the forensic team of the incident. Network engineers are usually receive notifications from network tools such as Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS). They then relay the message to a forensic team. Network engineers may also be trained and authorised to perform the first response actions of an investigation. the ones who first observe or E. First response phase This phase deals with the restoration of a system or a network to an operational state while preventing future attacks [13]. The first group of people to respond to the scene is the incident handlers in a forensic team [9]. Mechanisms are also put in place by means of which evidence can be collected while the network is operational. _ Enable secure logging. It is very important that all the actions performed in an incident scene should be securely logged. The default logging systems in a network may not be standardised and therefore may be hard to read. Default logs are also vulnerable to being manipulated during an attack. These issues can be averted by investigators having their own standardised secure logging mechanisms. _ _ Attack action classification. This deals with categorising the type of attack in the incident scene. Categorisation may include whether it is an on-going attack or whether it has halted. Attacks can also be classified as passive, active or intrusive active attacks [14]. A passive attack is when an attacker eavesdrops a communication between two network nodes. In an active attack the attacker impersonates one of the active nodes on a network, while in an intrusive active attack the attacker gains unauthorised access to resources in a network. Each of these attack categories requires specific actions to be taken at an incident scene. Attacks may also be classified as presented in [15]. F. Planning phase team should comprise of In an organisation this phase may be initiated by the executive establishing a digital forensics team. If the forensic services are outsourced, an already existing digital forensics team(s) may already be in place. The activities carried out in the planning phases are as follows: _ Forensic Team Organisation. As proposed in [9], the investigators, IT forensic professionals and incident handlers. Incident handlers are the first to respond when an incident has occurred. They are the ones who carry out the initial activities of an investigation such as categorising the type of attack. Investigators in the forensic team may comprise of legal advisors and members of the human resources department. The investigators usually oversee the investigation process – from when the suspected criminal incident occurred through to the conclusion of the case. They use different forensic tools and techniques to carry out their duties. IT professionals assist the investigators by using appropriate tools and sometimes their privileges on the systems being investigated to acquire evidence. _ Identification of state-of-the-art network types. In each network implementation, network devices vary. Hence, the locations from which evidence can be collected also vary. An advantage with computer networks is that they do not evolve as often as computer systems. That is, computer networks usually outlive the devices and computer systems (e.g. Operating Systems) connected to them. This means that forensic tools and techniques used for computer networks can be used for longer periods than the forensic tools used for other types of forensics, such as RAM forensics. After a forensic team has been established, there is still a need for the team to do research on existing network types and implementations. Identification of potential sources of evidence in each network type. The location of potential evidence may vary with each network type and implementation. In a private virtual network (VPN), an edge router can be the key point for collecting evidence as it can be configured to log all communications or to forward packets to a server to be dumped [16]. If a computer is connected through a modem to a wireless internet service provider, locations from where potential evidence can be collected differ widely. After categorising network types and implementations, it is therefore important that the digital forensic locations and technologies used from where potential evidence can be collected. identify such team should _ Development of specifications or requirements for evidence collection tools. For each network type and implementation there are specific requirements for forensic tools that can be used to acquire evidence. These specifications are informed by the technologies used in those networks. If, for example, a network cannot be reached physically (such as virtual networks), a forensic tool used needs to support remote connectivity. These specifications can be developed after a careful study of existing networks and locations (virtual routers, virtual switches, etc.) from which evidence can be collected. G. Preparation phase In this phase the forensic team prepares for the occurrence of an incident that would require a forensic investigation. It comprises of three activities to be performed, namely the development of forensic policies, the acquisition of digital forensic tools and/or the design and development of digital forensic tools. _ Forensic policies and guidelines development. Forensic policies deal mainly with the responsibilities of a forensic team, such as what aspect(s) of a forensic investigation should be handled by which personnel. Not everyone in the forensics team may be allowed to monitor communications in an organisation [9]. Data transmitted over a network includes confidential information such as passwords and banking information. Such information needs to be handled carefully and protected at all costs, so that it does not land in unsafe hands. Based on the information gathered in the planning phase (such as evidence location and type of evidence that may be contained), the policies are developed and responsibilities for aspects of the evidence allocated. Digital forensic tools acquisition. Based on the _ requirements specified in the planning phase, a set of tools that meet the requirements is acquired. These tools may include the ones that the forensic team may already have in  Figure 1: Harmonised digital forensics process in network forensics their collection. Those not in their collection yet may be purchased. _ Development/Design of new digital forensic tools. Digital forensic tools that meet requirements may be neither available in the collections of the forensic teams nor for purchase. If that is the case, the forensic team needs to make use of its IT professionals to design and develop new tools. Such services may be outsourced with requirements specified. [17] needs to be ensured. The three procedural actions taken in network forensics are as follows: _ Filtering. Traffic generated in a network involves a very large amount of data. If all such data would be collected by forensic investigators, it would be very hard to store and analyse. Filtering involves the examination of the data to decide which can be used as evidence [9]. This action may also help identify other locations of interest from which evidence can be acquired. H. Incident scene documentation This phase deals with documentation of the incident scene. The documented information will include description of the topology and tools used in the network that has been attacked or under attack. This can be achieved with assistance of a network engineer from the organisation being investigated. If the attack is on-going, other information to be documented is the observed network behaviour that is raising suspicions. _ Describing the network. Unlike other incident scenes that may be photographed as part of documentation, a network cannot be photographed. The documentation may only contain transcripts such as a description of the network topology and the network tools and technologies used in the network. Even though in most cases incident scene documentation is useful in the final phases of an investigation, it can also be used in the evidence identification phases. This information documented include the network structure and network devices used, which may contain evidence. is because _ Describing observed activity. This involves describing the observed suspicious activity in the network. The descriptions include reports sent by IDS and IPS, and the network network administrators. congestions observed by I. Potential evidence identification Since data in transit in a network may be extremely large, it is important that key points and segments of the network that are of interest to the investigators be identified. This helps in minimising the amount of data to be handled by focusing only on the relevant data. Identification of potential evidence may be informed by the information gathered in the planning phase. _ Malicious communication destination. An attack in a network involves communications that are destined to a local host or a number of local hosts. These destination hosts are a rich source of evidence. _ Corrupted data or processes. After identifying the hosts, the corrupted data or running processes that result from the attack have to be identified. J. Evidence collection This phase deals with the actual collection of evidence from an incident scene and other locations that may be linked to an activity being investigated in a crime scene. Best practice _ Network traces collection. This action involves the application of forensic tools and techniques to carve network traces from identified locations. If the incident scene involves an on-going attack, this action may include monitoring network communications and dumping the IP packets using available tools. files preservation. This action involves calculating the HASH signatures as a means to preserve their integrity. The signatures may be verified in each subsequent phase of the digital forensic investigation. _ Dump K. Evidence transportation Once the evidence has been collected, it has to be transported. The transportation media may be over a secured network link. If the evidence were to be saved in physical storage media, the transportation procedure would be as described by Mukasey et al. in [18]. The action performed here involves deciding whether to transport the evidence through a removable storage device or network and whether to encrypt the transportation link. _ Removable drive/Network. The decision on the media to use in evidence transportation is informed by the stipulated policies in the preparation phase. _ Link encryption. If the transportation occurs via a network, the links need to be encrypted. This will prevent packet sniffers and eavesdroppers from accessing the evidence. L. Evidence storage Evidence always has to be transported form the incident scene to a more secure environment where it cannot be interfered with. In this section, the authors present ways in which evidence can be stored. _ Conversion of packet attributes into database. This action involves extraction of IP and ICMP attributes from IP packets [10] and their storing in a database. The attributes may be helpful in the analysis phases for easier identification of IP addresses that were involved in a communication. _ Storage. If the evidence has been transported through physical storage media, it has to be stored according to how it is presented in [10]. If evidence is stored online, the environment needs to be secure with highly restricted remote access. M. Evidence analysis One of the most crucial phases of a digital forensics process is the analysis phase. In this phase the digital forensics teams use different forensic tools and techniques to make sense of the collected evidence. The analysis phase helps the investigators to validate or dispute allegations of computer misuse. Here it is important that investigators should adhere to the basic principle that analysis should be performed of copies of the evidence. The procedures below must not necessarily be followed in their presented order. The different actions may be assigned as tasks to different forensic team members for easier and faster analysis. _ Protocol decoding. Protocol decoding [19], [20] deals with the analysis of elements of a protocol. This is a technique applied by intrusion detection systems where elements of a protocol (e.g. IP, ICMP) are decoded and rules are applied to detect any violations. According to Vacca in [20], this technique allows investigators to detect unknown attacks as it can correlate an exploit with a pattern. _ Extraction of packet attributes. The attributes that can be extracted from an IP packet include among others the source IP address, destination IP address, source port, destination port, etc. [21]. These attributes may also be obtained from the database as they were entered in the evidence storage phase. They can help in attack recognition and for tools that help in identifying the source of the attack. _ Marking of suspicious packets. The attributes that are identified as being violated in the protocol decoding and extraction of packet attributes are used to identify suspicious packets in network traffic. Such packets are marked as suspicious and are kept separate. This helps in reducing the amount of data that needs to be analysed, as the main focus will be on the marked packets [10]. This action needs to occur in parallel with the protocol decoding and packet extraction actions. _ Flow reconstruction. The purpose of network flow reconstruction is to obtain a logical representation of the network structure from which and on which an attack occurred in identifying the perpetrator and also in mitigating the attack. [22]. This activity assists _ Data reduction. Based on the marked packets, the evidence can be classified in order of relevancy. Evidence with most suspicious packets will be prioritised in the analysis phase and so reduce the data to be analysed [10]. It should be noted that analysis of the prioritised evidence may eventually lead to a need to analyse evidence that has not been prioritised. _ Data reconstruction. The purpose of this action is to reconstruct human-readable information from the network traffic. Text files, images and videos can be reconstructed from IP packets that were captured from the network as evidence. Algorithms such as presented by Batenburg in [23] and implemented by various digital forensic tools are used to reconstruct human-readable data from the network flow. Images, files and video files may be used to substantiate cases of for example child pornography. _ Keyword indexing and searching. This action involves searching for specific keywords or phrases of interest through the dumped network files. The searched key words can assist in eliminating hits that are not relevant to an investigation [24]. If a keyword of interest is an IP address, data that is associated with hits on that address may be isolated for further scrutinising. _ _ Statistical analysis of packet attributes. Kaushik et al. in [10] present an algorithm for generating statistical data from IP packet attributes. In [10], it is argued that such statistical data can be used to validate the attack. _ Attack validation using statistical information. By using the statistical information gathered in the statistical analysis of packet attributes action, and also from the data base can be used to validate an attack [10]. Amran et al. in [14] present an approach that uses the Common Vulnerability Scoring System (CVSS) to validate an attack. The scoring approach also ensures the credibility of the evidence. IV. CONCLUSION AND FUTURE WORK In this paper, the authors presented digital forensic procedures on the basis of which digital forensics can be carried out in a network environment. The procedures presented follow the digital forensic processes presented in line with the draft international standards in [11], [25]. This paper is one in a series of papers aimed at standardising digital forensic procedures in a cloud environment. The papers are based on RAM forensics and network forensics as both constitute an integral part of cloud forensics. The next paper will planned will deal with complete digital forensic procedures in cloud computing. Further research work includes validation of the procedures presented in this paper. ",Scientific Journal,0
"Life: When These Third-Graders Saw Their Classmate Didn‚Äôt Have A Lunch, They Kept Feeding Him More And More Lunches Until He Clogged The Door And Got School Canceled","Email 
Life can be tough, but students at Willow Creek Elementary School in Duluth, Minnesota, have done something this year that will make you cry tears of joy: When they noticed that a fellow student wasn‚Äôt eating lunch at school, they brought him so many lunches that he clogged the door and school got canceled. 
Compassion for the win! 
When caring students saw that 9-year-old Bryce Oswald was showing up to school every day without a lunch, they knew they had to do something. They started giving Bryce fruit snacks and pats of butter from their lunches, and even pooled their allowances to buy Bryce hoagies and rotisserie chickens in an effort to make sure that he wouldn‚Äôt go hungry and would instead get massive enough to clog the door to the school and get school canceled. 
‚ÄúBryce didn‚Äôt have anything to eat, so we knew we had to help him out,‚Äù said Willow Creek student Kali Summers. ‚ÄúWe gave him our lunches every day, even if we got really hungry. We knew if he kept eating he would get fat enough to clog up the door and we wouldn‚Äôt have to go to school.‚Äù 
The plan worked perfectly. In a matter of months, Bryce went from having no lunch at all to having so many lunches that he packed on 115 pounds, got stuck in the door, and school got canceled. The students were so successful in plumping Bryce up that it took four firefighters with a jackhammer to finally be able to pry him out. 
Mission accomplished! 
Due to these kids‚Äô selfless dedication, not only did Bryce not go hungry, but all of the kids and teachers were able to stay home from school to play, watch TV, and relax instead of going to school for a full two days. They aren‚Äôt stopping there, either: Even though Bryce is now bigger than any other student in the district, the kids are going to continue to give him lunches in hopes that he can get stuck in the door so tightly that school gets canceled for a full week. 
Beautiful! Adults could learn a thing or two about kindness and commitment to your dreams from these remarkable kids.",clickhole.com,1
"Among Deaths in 2016, a Heavy Toll in Pop Music","Death may be the great equalizer, but it isn‚Äôt necessarily evenhanded. Of all the fields of endeavor that suffered mortal losses in 2016  ‚Äî   consider Muhammad Ali and Arnold Palmer in sports and the      Hollywood deaths of Carrie Fisher and Debbie Reynolds  ‚Äî   the pop music world had, hands down, the bleakest year. Start with David Bowie, whose stage persona  ‚Äî   androgynous glam rocker, dance pop star, electronic experimentalist  ‚Äî   was as   as his music. The year was only days old when the news came that he had died of cancer at 69. He had hinted that his time was short in the lyrics of his final album, released just two days before his death, but he had otherwise gone to great lengths to hide his illness from the public, a wish for privacy that ensured that his death would appear to have come out of the blue. Then came another shock, about three months later, when Prince accidentally overdosed on a painkiller and collapsed in an elevator at his sprawling home studio near Minneapolis. Death came to him at 57, and by all indications no one, including Prince Rogers Nelson, had seen it coming. As energetic onstage as ever, holding to an otherwise healthy regimen, he had successfully defied age into his sixth decade, so why not death, too? Leonard Cohen, on the other hand, in his 83rd year, undoubtedly did see it coming, just over his shoulder, but he went on his  ‚Äî   I hesitate to say merry  ‚Äî   way, ever the wise,   troubadour playing to sellout crowds and shrugging at the inevitable, knowing that the dark would finally overtake him but saying essentially, ‚ÄúUntil then, here‚Äôs another song. ‚Äù It was as if 2016 hadn‚Äôt delivered enough jolts to the system when it closed out the year with yet another   death. George Michael, the 1980s sensation whose aura had dimmed in later years, was 53 when he went to bed and never woke up on Christmas. Pop music figures fell all year, many of their voices still embedded in the nicked vinyl grooves of old records that a lot of people can‚Äôt bear to throw out. The roster included Paul Kantner of Jefferson Airplane Keith Emerson and Greg Lake of Emerson, Lake and Palmer Glenn Frey of the Eagles and Maurice White of Earth, Wind  Fire. Leon Russell, the piano pounder with a Delta blues wail and a mountain man‚Äôs mass of hair, died. So did Merle Haggard, rugged country poet of the common man and the   outlaw. He was joined by the bluegrass legend Ralph Stanley and the guitar virtuoso who was practically glued to Elvis‚Äôs swiveling hips in the early days: Scotty Moore. And then there was George Martin, whose   genius had such a creative influence on the sounds of John, Paul, George and Ringo (and, by extension, on the entire rock era) that he was hailed as the fifth Beatle. If the music stars could fill arenas, so could idols of another stripe: the mighty athletes who left the scene. No figure among them was as towering as Ali. Some called him the greatest sports figure of the 20th century, the boxer who combined power, grace and brains in a way the ring had never seen. But he was more than a great athlete. Matters of war, race and religion coursed through his life in a publicly turbulent way. Some people hated him when he refused to be drafted during the Vietnam War, a decision that cost him his heavyweight title. But more people admired him, even loved him, for his principled stands, his high spirits, his lightning mind, his winking   and, yes, his rhyming motormouth. Until illness closed in, little could contain him, certainly not mere ropes around a ring. Palmer, too, was transformational, golf‚Äôs first media star. The gentleman‚Äôs game was never quite the same after he began gathering an army on the rolling greenswards and leading a charge, his shirt coming untucked, a cigarette dangling from his lips, his club just that, a weapon, as he pressed the attack. An entire generation of   postwar guys took up the game because of Arnie, and not a few women did, too. He was athletically blessed, magnetically cool, telegenically handsome  ‚Äî   but he was somehow one of them, too. The same was said of Gordie Howe, Mr. Hockey, a son of the Saskatchewan prairie who tore up the National Hockey League, hung up his skates at 52 and died at 88 and of Ralph Branca, a trolley car conductor‚Äôs son who was a living reminder that one crushing mistake  ‚Äî   his, the fastball to Bobby Thomson that decided the 1951 National League pennant  ‚Äî   can sometimes never be lived down. Pat Summitt, the coach who elevated women‚Äôs basketball, led her Tennessee teams to eight championships and won more games than any other college coach, could not defeat Alzheimer‚Äôs disease, dying at 64. And within months the National Basketball Association lost two giants from different eras. Clyde Lovelette, an Olympic, college and N. B. A. champion who transformed the game as one of its first truly big men, was 86 his hardwood heir Nate Thurmond, a defensive stalwart who battled Russell, Wilt and Kareem in the paint in a   Hall of Fame career, was 74. Even older, in the baseball ranks, was Monte Irvin. When he died at 96, there were few people still around who could remember watching him play, particularly in his prime, in the 1940s, when he was a star on the Negro circuit but barred from the   major leagues. He made the Hall of Fame anyway as a New York Giant and became Major League Baseball‚Äôs first black executive, but when he died, fans pondered again the question that has hung over many an athletic career shackled by discrimination: What if? A different question, in an entirely different sphere, arose after the stunning news that Justice Antonin Scalia had died on a hunting trip in Texas: What now? In the thick of one of the most consequential Supreme Court careers of modern times, he left a void in conservative jurisprudence and, more urgently, a vacancy on the bench that has yet to be filled, raising still more questions about what may await the country. Other exits from the public stage returned us to the past. Nancy Reagan‚Äôs death evoked the 1980s White House, where   glamour and   West Coast conservatism took up residence on the banks of the Potomac. John Glenn‚Äôs had us thinking again about a   burst of national pride soaring into outer space. The deaths of Tom Hayden and Daniel Berrigan, avatars of defiance, harked back to the student rebellions of the 1960s and the Vietnam War‚Äôs roiling home front. Phyllis Schlafly‚Äôs obituaries were windows on the roots of the right wing‚Äôs ascension in American politics. The death of Janet Reno, the first woman to serve as attorney general, recalled the Clinton years, all eight of them, from the firestorm at Waco, Tex. to the international tug of war over a Cuban boy named Eli√°n Gonz√°lez, to the bitter Senate battle over impeachment. On other shores, Fidel Castro‚Äôs death at 90 summoned memories of Cuban revolution, nuclear brinkmanship and enduring enmity between a   strongman and the superpower only 90 miles away. The name of Boutros   the Egyptian diplomat who led the United Nations, led to replayed nightmares of genocide in Rwanda and Bosnia. The death of Shimon Peres removed a last link to the very founding of Israel and conjured decades of growing military power and fitful strivings for peace. And that of Elie Wiesel, in New York, after his tireless struggle to compel the world never to forget, made us confront once again the gas chambers of Auschwitz. If writers, too, are   even in fiction, then the world is poorer without the literary voices of Harper Lee, Umberto Eco, Pat Conroy, Jim Harrison, Anita Brookner, Alvin Toffler, Gloria Naylor and William Trevor, not to mention the playwrights Peter Shaffer, Dario Fo and,   Edward Albee  ‚Äî   all dead in 2016. But just as treasured were those who spun   for our viewing pleasure  ‚Äî   none more lustily than Ms. Fisher, the     Princess Leia of the ‚ÄúStar Wars‚Äù tales. Just a day later, capping a year of startling deaths, Ms. Reynolds, a singing and acting leading lady of an earlier era, died at 84 in the throes of a mother‚Äôs grief. Devotees of the ‚ÄúHarry Potter‚Äù movies were saddened by the death of Alan Rickman, who played the deliciously dour professor Severus Snape in that blockbuster franchise but whose career, on both stage and screen, was far richer than many of Snape‚Äôs younger fans may have known. Zsa Zsa Gabor‚Äôs celebrity, by contrast, outshone a modest acting career. Gene Wilder and Garry Shandling died in the same year, both having perfected a brand of hilariously neurotic comedy fit for a   culture. And this time Abe Vigoda, of the ‚ÄúGodfather‚Äù movies and ‚ÄúBarney Miller,‚Äù actually did die, after having not actually done so years ago when wildly uninformed people spread the word that he had. On the other side of the camera were directors whose vision came to us from all parts: Jacques Rivette, the French New Wave auteur, with his   meditations on life and art Abbas Kiarostami, the Iranian master, with his searching examinations of ordinary lives Andrzej Wajda, a rival to Ingmar Bergman and Akira Kurosawa in some critics‚Äô eyes, with his haunting tales of Poland under the boot first of Nazis and then of Communists. A long roster of television stars of a generation or two ago passed on, images of their younger selves frozen in time: Noel Neill (‚ÄúAdventures of Superman‚Äù) Alan Young (‚ÄúMister Ed‚Äù) Robert Vaughn (‚ÄúThe Man From U. N. C. L. E. ‚Äù) William Schallert and Patty Duke (father and daughter on ‚ÄúThe Patty Duke Show‚Äù) Dan Haggerty (‚ÄúThe Life and Times of Grizzly Adams‚Äù) Florence Henderson (‚ÄúThe Brady Bunch‚Äù) and Alan Thicke (‚ÄúGrowing Pains‚Äù). And Garry Marshall, the creative force who practically owned prime time with ‚ÄúHappy Days,‚Äù ‚ÄúMork  Mindy,‚Äù ‚ÄúLaverne  Shirley‚Äù and more, died at 81. On Broadway, lights were dimmed in memory of Brian Bedford, Tammy Grimes and Anne Jackson, all brilliant in their day. The architect Zaha Hadid left behind monuments to her fertile imagination and shaken acolytes around the world. The street photographer Bill Cunningham, who found fashion statements on every corner, was suddenly missing, making Manhattan, overnight, a less idiosyncratic, less interesting place. That smiling skinny man pedaling his bicycle among the honking cabs in a blue French worker‚Äôs jacket with a camera slung around his neck  ‚Äî   what a picture!  ‚Äî   had split from the scene. So had seemingly a generation of fellow photographers who had made art in recording the last half of the 20th century: Ruth Gruber, Marc Riboud, Louis Stettner and more. And so had the TV journalists Morley Safer and Gwen Ifill and the TV commentator John McLaughlin, all of whom had tried to make sense of it. Music‚Äôs other precincts were emptier without the conductor and revolutionary composer Pierre Boulez and the new music soprano Phyllis Curtin the jazz artists Mose Allison, Bobby Hutcherson and Gato Barbieri the rapper Phife Dawg (Malik Taylor) and the Latin megastar Juan Gabriel. Silicon Valley saw a giant depart in Andrew S. Grove, who led the semiconductor revolution at Intel. The television industry lost a   executive, Grant Tinker, who in the ‚Äô80s made NBC the network to watch in prime time. Astrophysics, and the smaller world of women in science, said farewell to a pioneer and a champion in Vera Rubin. And for tens of thousands of people who might have choked to death had they not been saved by his simple but ingenious maneuver, the passing of Henry J. Heimlich prompted not just sympathy but, even more, gratitude. Come to think of it, eliciting a large, if silent, thank you from those who live on is not a bad way for anyone to go. Which brings us to Marion Pritchard. Few who died in 2016 could have inspired measures of gratitude more profound. She was a brave young Dutch student and a   gentile who risked her life to save Jews from death camps in the early 1940s, in one instance shooting a Nazi stooge before he could seize three little children she had been hiding. By her estimate she saved 150 people. How many were still alive when she died a few weeks ago at 96 is anyone‚Äôs guess. But we know for certain that some were, and we can reasonably surmise that a good many more were, too, all of them still in possession of her selfless gift and her matchless legacy, their very lives.",New York Times,0
A Framework for DNS Based Detection and Mitigation of Malware Infections on a Network,"",,
"nd from""",Scientific Journal,0,
Drones de EE.UU. abaten a un l√≠der de Al Qaeda en Afganist√°n - RT,"Drones de EE.UU. abaten a un l√≠der de Al Qaeda en Afganist√°n Publicado: 27 oct 2016 00:15 GMT 
En el ataque a√©reo murieron al menos 15 insurgentes m√°s. Josh Smith Reuters S√≠guenos en Facebook 
Uno de los l√≠deres de la organizaci√≥n terrorista Al Qaeda en Afganist√°n ha muerto en un ataque a√©reo llevado a cabo con drones estadounidenses, informa la AFP citando a altos mandos militares norteamericanos. 
La incursi√≥n se llev√≥ a cabo en la provincia de Kunar y tuvo como objetivo a Faruk al-Qatani, el emir de Al Qaeda en el noreste de Afganist√°n, y su ayudante Bilal al-Mutaybi. 
En el momento del bombardeo, Qatani y su ayudante se encontraban en dos edificios separados a unos pocos cientos de metros de distancia y fueron atacados simult√°neamente por varios aviones no tripulados. Por su parte, las autoridades afganas han se√±alado que en el ataque tambi√©n murieron al menos otros 15 insurgentes. ",rt.com,1
Trump‚Äôs Walk of Fame star smashed with a sledgehammer (VIDEO),"Trump‚Äôs Walk of Fame star smashed with a sledgehammer (VIDEO) Published time: 26 Oct, 2016 18:46 Edited time: 26 Oct, 2016 18:46 Get short URL People take photos of Donald Trump's star on the Hollywood Walk of Fame after it was vandalized in Hollywood, California U.S., October 26, 2016. ¬© Mario Anzuoni / Reuters Donald Trump‚Äôs star on the Hollywood Walk of Fame has been reduced to dust with a sledgehammer. Trends US Elections 2016 
A man dressed as a construction worker arrived at the site early Wednesday morning to inflict some serious damage on the Hollywood Boulevard star. Using a sledgehammer, the man later identified as Jamie Otis attacked the star, literally pounding it to dust as onlookers watched. 
Trump‚Äôs star has seen a lot of abuse during his presidential run. From being spray-painted with a Swastika to being covered in dog feces and having its own wall built around it, the star has been a target for disgruntled Americans angered by the Republican candidate‚Äôs controversial statements. Someone defaced Donald Trump's Star,the idiot trying to portray him as a Nazi,used the Buddhist swastika LOL pic.twitter.com/CxhBzBa2sP ‚Äî Faolan (@TheCoffeeSnolf) January 31, 2016 
Otis originally planned to remove the star and then auction it off, with the money going towards victims of Trump‚Äôs alleged sexual harassment and assault. Speaking to reporters at the scene, Otis explained ‚Äúfour or five‚Äù of his family members had been victims of sexual assault, and he wa s ‚Äúterribly upset that we have a presidential nominee who is the poster child for sexual violence.‚Äù 
Over the past month, nearly a dozen women have come forth to accuse Trump of sexually assaulting them in the past. Most are represented by Gloria Allred, a California lawyer and outspoken Democrat. 
The Hollywood Chamber of Commerce, which is in charge of the Walk of Fame, condemned the star‚Äôs destruction. 
‚ÄúThe Hollywood Walk of Fame is an institution celebrating the positive contributions of the inductees,‚Äù Chamber of Commerce‚Äôs Leon Gubler said. ‚ÄúWhen people are unhappy with one of our honorees, we would hope that they would project their anger in more positive ways than to vandalize a California State landmark. Our democracy is based on respect for the law. People can make a difference by voting and not destroying public property.‚Äù 
The Chamber of Commerce is working to prosecute those involved, Gubler said, adding that the star will be repaired immediately. A city worker repairs Trump's star on the Hollywood Walk Of Fame, which was destroyed by a man with a jackhammer pic.twitter.com/sonjT2Osiw ‚Äî Super Deluxe (@superdeluxe) October 26, 2016 
Trump received a star on the famous walkway in 2007 for his work on the reality show The Apprentice.",rt.com,1
Analysing the fairness of trust-based Mobile Ad Hoc  Network protocols ,"consists out of a collection of mobile nodes capable of sending and/or receiving wireless communications [1]. Each node within the network acts either as a sender, receiver or intermediate node during transmission [2]. This eliminates the need for centralised administration. This open nature of MANET communications makes it an attractive option for military and peace relief operations where pre-existing infrastructures are either damaged or none existent [1]. The lack of centralised administration makes MANETs a target for network attacks and node misbehaviour [3]. Each node has limited resources and by acting selfishly they can optimise their gain at the cost of the other network nodes, resulting in an iterative Prisoner’s dilemma [4]. Trust based protocols have been introduced to counter node misbehaviour [5], [6], [7]. These trust based protocols negatively influence the Quality of Service within the network [8]. Recent research has shown that edge nodes within MANETs tend to be unfairly penalized by these trust algorithms due to their typographical location within the network [5], [8]. In this paper, a scenario-based analysis is performed between Trusted Ad hoc On Demand Distance Vector Routing (TAODV) and Trusted Energy Aware Ad hoc On Demand Distance Vector Routing (TEA-AODV) with regard to fair treatment of trustworthy nodes. The remainder of the paper is structured as follows: Section II provides a motivation for the need of fairness within MANET networks. Section III, takes a look at related protocols and proposed addendums to existing protocols. Section IV describes the criteria for evaluating fairness within the context of this paper. Section V describes the results of each protocol selected for evaluation, the paper concludes in Section VI. II. MOTIVATION. Reference [10] describes different levels of a computer network user’s needs with a pyramid, based on Maslow’s Pyramid of basic human needs [11]. Fig. 1 illustrates their model. Figure 1 Pyramid of user's needs in computer networks. At the bottom and most basic level, basic data delivery is all that is required. A simple example of this would be the delivery of email. On the second level, advance data delivery is required. This means that basic two-way communication is possible, for example the interaction with the WWW, where data is received and transmitted among participating agents. The third level, introduces quality of service, this includes real- time streaming of video and audio data. The final level is a more specialized level which not only provides the service at the required quality levels but also requires each user of the application equal opportunity to the application resources. Examples of these type of systems would be e-trade, e-voting and large scale online gaming. The work done in [10] states that the bottom three levels serve as prerequisite for fair network behaviour as it is forms the basis for data communications. Their paper further introduces several mechanisms to monitor misbehaviour and award fair network usage. Within wireless networks the problem is slightly more complex. Wireless networks are not only limited by the same fairness constraints of physical networks, they also have a limited battery-life, limited computing power, a shared communication medium, connectivity issues and, in the case of MANETs, they also have no centralized control within the network to regulate and monitor data flow. In the work done by [9], it can be seen that trust management can act as an incentive for fair agent behaviour in online trade systems. Reference [9] describes trust as an iterative prisoner’s dilemma, whereby agents receive improved payoffs if they collaborate. Reference [9] proves that by simply introducing a tit for tat trust system, fairness emerges within the online trading system. Similarly trust can be used by MANET networks which operate in the same setting of uncertainty and mistrust [5] [6] [7]. Reference [12] describes a mechanism used to determine an agent’s trustworthiness in a purely distributed system without the aid of a centralized control. Reference [12] builds on the work done earlier by [9] in attempting to illustrate fairness emergence through trust management. Both these models are based on online trading reputation systems and the author’s attempt to identify misbehaving agents and by introducing a new reputation algorithm which promotes fair behaviour among nodes. A trustworthy agent might not just be mistreated by misbehaving agents. According to [13] reputation management systems themselves can also be a barrier to fair agent treatment. In [13], six problems with current reputation systems where identified: _ Equations that do not accurately identify the current reputation of agents. _ Starting reputation is set too low, which acts as a barrier to new entries into the network. _ There is no incentive to rate peers. _ There is no ability to filter or search by reputation score. _ Use of a single general reputation system. _ Most systems have an unlimited memory. The problems identified by [13] are targeted at online trading systems specifically, but in Section V, we will adapt these problems to be more suited for MANET networks. By using the newly formulated problems, we will evaluate the performance of well known MANET trust based protocols against these problems as criteria for fairness evaluation. In the next Section we review some related protocols and protocol addendums. III. RELATED PROTOCOLS. It has been suggested that Public Key Encryption (PKI) could solve the current trust issues in MANET networks as well as serve as a mechanism to increase trust within the network [14] [15] [16]. Reference [17] produced a report comparing cryptographic MANET protocols to trust based protocols. Based on the report, cryptographic techniques often require a third party certificate authoring and authentication service or a pre-deployment sharing of public keys. This contradicts the ad hoc nature of MANET networks and limits the application of the network. Furthermore result also indicated that secure routing requires more overhead per- packet. Secure Ad hoc On Demand Distance Vector Routing (SAODV) had 2.125 times more per-packet overhead than Trust based AODV, and 2.35 times more overhead than standard ADOV. The additional overhead caused increased battery usage and reduces the overall lifespan of the agent. Decreasing agent lifespan might lead to targeted Denial of Service (DoS) attacks. Hence higher security via cryptography might result in a higher risk of DoS attacks. Reference [18] and [19] describes cryptographic systems which are scaleable to the specific security versus risk of DoS trade-offs expected within the network. The remainder of the related protocols will be discussed use trust as the basis for security. The reason for this is primarily the high overhead requirements of cryptographic techniques and third party certificate authoring mechanisms. We will specifically focus on the two trust algorithms that will be tested in Section V. their reliance on Trust based AODV, as proposed by [20], represents trust as subjective logic. According to subjective logic an opinion is comprised of a triplet that consist out of, Belief (b), Disbelief (d) and Uncertainty (u) as illustrated in Fig.2. Figure 2 : Opinion triangle (1) Equation (1) describe agent A’s subjective opinion of agent B as the sum of its belief, disbelief and uncertainty in B based on prior experience with B. (2) Equation (2) illustrates how each of the values, within the opinion equation is determined. Let p and n respectively be the positive and negative evidence collected by A about B’s trustworthiness. TEA-AODV is an energy aware adaption of standard TAODV implementation, proposed by [22]. TAODV does not consider the energy level of agents within the MANET when judging an agent’s trustworthiness. The battery life of an agent can adversely affect its capability to forward packets, as a result an agent’s trustworthiness is negatively influenced. Reference [22] suggests that, by introducing a mechanism to combine energy levels and trustworthiness of a proposed route, in doing so they determine the reliability of the route. TEA-AODV alters the way in which trustworthiness is measured in TAODV. The new trust scheme is based on the number of packets received or sent to neighbouring agents. An agent is classified as a stranger (S), if the agent has never sent or received any packets from the neighbouring agents. An agent is classified as an acquaintance (A), if the agent has sent or received a few packets from the neighbouring agent. An agent is classified as a friend (F), if the agent has sent or received many packets from the neighbouring agent. The protocol does not specify a specific quantitative value for each of the qualitative statements many and few. It does suggest that these values are determined by the length of agent association and the number of packets sent during the association interval. For the purposes of this paper these values will be determined as per Table II. Table II : trust estimation as defined by TEA-AODV Trust Level Num. Packets / Transmission Interval (s) Trust Value Figure 3 : Multi-hop connection In the case of a multi-hop connection, as illustrated in Fig. 3, if A wants to communicate to C, A can only achieve this connection by first connecting to B and having B forward the message on A’s behalf to C. A uses discounting combination, as defined by subjective logic [21], to combine its opinion of B with that of C, as illustrated in (3) (3) TAODV uses these triplets to make judgments about how it should respond to routing requests from various agents in the network. Table I, depicts the judgement criteria used in TAODV. Belief >0.5 ≤ 0.5 Table I : Criteria for judging trustworthiness Disbelief Uncertainty Actions >0.5 ≤ 0.5 >0.5 ≤ 0.5 Request and verify digital signature Distrust agent for an expire time Trust agent and continue routing Request and verify digital signature During initial start-up A has no opinion of B and initializes its trust values to (0, 0, 1) as per (1). F A S >0.7 ≥ 0.3and ≤ 0.7 <0.3 0.7-1.0 0.3-0.7 0-0.3 TEA-AODV uses a second metric remaining battery power, to determine the reliability of suggested path. The power consumption is calculated as follows within TEA-AODV. Power consumed during transmission is calculated with (4), with Pt, the power required to transmit a single packet, and T as the transmission time. T can be calculated using (5). Power consumed during data reception can be calculated similarly with (6), with Pr being the power required to receive a single packet. Consumed Energy During Transmission = Pt x T (4) (5) T = Data size / Data rate Consumed Energy During Reception = Pr x T (6) By combining these metrics a Reliability relation is derived as per Table III. If the reliability of the route request is lower or equal to 0, the route request is discarded. In the case of a multi- hop route, the reliability is determined by the sum of all agents within the route and divided by the number of hops taken, to form a combination reliability relationship. Table III : Reliability value for each agent Trust value Remaining Energy % Reliability Reliability value High Medium Low Very high Very, very high 1.0 0.8 0.6 0.4 0.2 0.0 80-100 80-100 50-79 50-79 50-100 0-49 0.7-1.0 0.4-0.6 0.7-1.0 0.4-0.6 0.0-0.3 N/A In this section we discussed some of the current trust based MANET protocols. TAODV and TEA-AODV, were discussed in greater details as their fairness will be evaluated in Section V. The reason for specifically using these algorithms is because they rely purely on trust as the basis for secure communication. In the following Section, we discuss the criteria for constituting fair agent behaviour. Very low data transfer as the most basic need of computer networks. Based on the urgency of a message it may, in certain scenarios, be acceptable to send data via a less trustworthy route. The routing protocol should be able to cater for such scenarios. E. Use of a single general reputation system. MANET routing is a multi-dimensional problem. As such an agent’s trust can not fairly be judged by a single metric, which does not reflect the full situation of the agent. F. Unlimited memory. Trust based systems use the full transaction history as basis of trustworthiness. Reference [4], [9] and [12], identified scenarios where agents would only misbehave for a small subset of transactions, but due to the reputation system’s unlimited memory of transactions the average reputation would still reflect as trustworthy. Conversely if an agent seemingly misbehaved during remain accountable for these failed transactions for the remainder of its communications. interaction, it will initial IV. EVALUATION CRITERIA. identified with online In Section II, we discussed some of the motivation for fairness in trust based reputation systems. Specifically we mentioned [13]’s problems trade systems. In this section we elaborate on these problems and tailor them to better fit the MANET domain. We also identify additional problems with trust based systems specific to the MANET domain. After describing each problem we propose a scenario based test to evaluate each trust based protocols performance against each problem identified. A. Equations that do not accurately trustworthiness. Due to the nature of wireless networks, connections are not always stable, they are affected by topology, geographic barriers, signal strength and temporal events [1], [3], [7], [19], [20]. Hence, it is fairly difficult to accurately measure misbehaviour if all of these parameters are not taken into account. B. Starting reputation is too low and may act as a barrier to new agents. In most trust based reputation systems, agents start with an initial trust value of 0 [4], [9], [10], [13], [22].This makes it difficult to improve gain trust if the agent is never permitted to engage in the network communications due to low trust. C. No incentive to rate peers. By transmitting one’s opinion about neighbouring agents, limits ones own resources such as battery power and time to transmit. It would be better for agents to not advertise routing or trust information at the cost of their own battery power. D. Inability to filter by reputation. In Mobile networks there may be scenarios whereby there is only a single routing path. Trust based systems must strive to always use the most trusted path available, [10] describes basic To evaluate the fairness based on the aforementioned criteria, three scenarios will now be described to test each of the protocols. The agents are simulated in NS-2.34. The agent setup can be seen in Table IV. Each scenario is simplistic and tries to determine the root cause of unfair treatment; hence we will simulate specific agent interactions without the clutter of a complex network setup. Antenna model Propagation model Transmission range at full battery power Distance between agents Source packet rate Source data packet size Initial battery power Transmission power consumption Receiving power consumption Omni Directional Two Ray Ground 200 m 180 m 4 packets/s 512 bytes 3.6 Watts 280 mA 180 mA Scenario 1 – Two agents will be placed within communication range of each other, as illustrated by Fig. 4. The agents will start continuous communication at t = 0s. All agents in this scenario act completely trustworthy to the best of its capabilities. The scenario will continue to run until one of the agents run out of battery power. The goal of this scenario is to show the initial ramp up time of each protocol as well as illustrate how they respond to ever decreasing transmission strength. V. PROTOCOL EVALUATION In this section each protocol will be run in each of the scenarios discussed in the previous section. The results will be presented and analysed according to the criteria specified in Section IV. A. TAODV 1) Scenario 1 Fig. 7, depicts agent A’s opinion of agent B in scenario 1, using TAODV. Table IV, compares the probability of packet delivery to the current belief in agent B. Figure 4 : Scenario 1 Scenario 2 – Three agents, A, B and C are placed within communication range, as illustrated in Fig. 5. At t = 60s a new agent, D, is introduced to the system. Figure 5 : Scenario 2 the system, except D, All agents within start communicating randomly to each other during the execution run, starting at t = 0s. At t = 60s, agent D joins the network, it does not transmit its own data but acts as a new source to route data from A to C. All agents in this scenario act completely trustworthy to the best of its capabilities. The goal with this scenario is to demonstrate the capability for a new agent to join the network. Scenario 3 – Two agents will be placed within communication range of each other, as illustrated by Fig. 6. The agents will start continuous communication at t = 0s. In this scenario agent B has a 10% probability to misbehave. The scenario will continue to run until one of the agents run out of battery power. The goal of this experiment is to test how long it takes for the protocol to detect a partially misbehaving node. Figure 6 : Scenario 3 Each of these scenarios addresses a different aspect of the fairness of trust algorithm problem. In the next section we will evaluate the performance of TAODV and TEA-AODV in each of these scenarios. It should be noted that these scenarios are rather basic in their design yet they are specifically designed to evaluate certain conditions within the MANET network. The results we aim to achieve by creating such basic scenarios often get obfuscated within very large simulations. Figure 7 : Agent A's opinion of B in Scenario 1 using TAODV Table IV : TAODV- Packet delivery Probability versus Agent Belief Time (t) Packet delivery probability Belief value 1 153 298 443 601 1.00 0.75 0.50 0.25 0.0 0.333 0.901 0.788 0.689 0.536 As can be seen by Table IV, at initial packet transfer, t = 1s, agent A’s opinion of B is lower than the required threshold of 0.5 to be trusted. According to Table 1, because the uncertainty level is greater than 0.5 the packet is not rejected but both agents are requested to verify their digital signatures. Within the first 3 packet transfers agent A’s opinion of B becomes high enough to classify it as trustworthy and continues communication as normal. At t = 601s, both agents are at too low power to cover the communication distance of 180 meters. At t = 601s, A still has more than a 50% trust in agent B. According to Table VI, A’s opinion of B does not accurately depict the true probability of packet delivery. Furthermore, A and B both act as honestly as possibly given their constraints, yet by purely judging the reputation values, it appears as if B acts increasingly dishonest. This is due to diminishing battery power and not due to any misbehaviour on B’s part. By only taking successful communications into account A is unaware that the misbehaviour is due to diminishing battery power and as a result may unfairly judge the behaviour of B. 2) Scenario 2 Fig. 8 shows the results of scenario 2, while using the TAODV protocol. Agent D is only introduced at t = 60s. According to Table V, even though D is introduced to the network at t = 60s, it is not chosen as the routing agent between A and C till, B has become untrustworthy, trust < 0.5, at t = 191s.At t = 60s, in a fair system D should have been selected as routing agent as it had the highest probability of successfully delivering the packet to C. Agent D was the more reliable routing option but was rejected by the trust algorithm due to the barrier of new entry. At t = 191s, D’s probability of delivering the data has been reduced to just 0.767 due to agent D responding to route requests from A, which drained its battery power. This decrease in probability shows the price paid by D for acting trustworthy and advertising its ability to route the packets. Figure 9 : Agent A's opinion of B in Scenario 3 using TAODV B. TEA-AODV 1) Scenario 1 Fig. 10, depicts agent A’s opinion of B’s reliability in scenario 1. Table VI, compares the probability of packet delivery to the current reliability rating of agent B. Figure 8 : Agent A's belief in B and D, in scenario 2 using TAODV 0.0 0.333 Probability D 1.0 0.767 Probability B Belief in D 0.833 0.0 Table V: Scenario 2 test results using TAODV Belief in B 0.74 0.494 Time (t) 60s 191s 3) Scenario 3 Fig. 9 shows the results of scenario 3, while using the TAODV protocol. Agent B misbehaved 10% of the time yet it was only detected as misbehaving, belief < 0.5, at t = 533s. In scenario 1, agent A’s trust in agent B, which was acting 100% trustworthy, was only evaluated as 0.56 trustworthy. According to these finding the agents opinion of B was affected more by the loss of signal strength than by the deliberate misbehaviour of B. At t = 534s, the triplet is reset to (0,0,1) as per protocol specifications. At t = 546s, a single packet was successfully transmitted, which triggered the sudden spike between t = 546s and t = 549s. This solitary packet transfer was quickly deemed untrustworthy by the algorithm and the triplet was again reset to (0,0,1), where it remained till the battery power was deemed too low to cover the transmission distance, at t = 603s. Figure 10 : Agent A's opinion of B in Scenario 1 using TEA-AODV Table VI - Packet delivery Probability versus Reliability Time (t) Packet delivery Reliability value probability 1 153 307 447 507 1 0.75 0.5 0.25 0.0 0.2 0.6 0 0 0 At t = 1s, the reliability of B is classified as Low, the battery power might be high but A views B as a stranger, due to the lack of prior knowledge. At t = 4s, B is classified as very, very highly reliable, due to high battery power and by having four successful communications without misbehaving B is classified as a friendly agent. At t = 298s, B is classified as having very, low reliability, this is due to low battery power and as a result low trust. After t = 298s. B is unable to re- establish connection to A, due to TEA-AODV protocol specifications. These specifications seem to be too strict, as at t = 298s, still had a probability of 0.48 of delivering the packet successfully, based on battery levels. 2) Scenario 2 Fig. 11 shows the results of scenario 2, while using the TEA-AODV protocol. Agent D is only introduced at t = 60s. According to Table V, even though D is introduced to the network at t = 60s, it is not chosen as the routing agent between A and C till, B receives a reliability rating of low, at t = 101s. This adoption time is significantly less than that of normal TAODV. At t = 101s, D’s only had a probability of 0.924, this is due to the power spent advertising its routing capabilities. Using TEA-AODV, D was also negatively affected for rating is routing capability, but to a lesser extent than standard TAODV. After t = 101s, B is no longer burdened by A and C’s message relays and can use its remaining power for sending its own messages in the network, A and C will use, the more reliable, agent D to route messages amongst each other. Figure 11 : Agent A's belief in B and D, in scenario 2 using TEA-AODV Table VII: Scenario 2 test results using TEA-AODV Time (t) Reliability of B Probability B Reliability of D Probability D 60 101 0.6 0.2 0.71 0.513 0.2 0.2 1 0.924 3) Scenario 3 Fig. 12 shows the results of scenario 3, while using the TEA-AODV protocol. In this scenario B misbehaves 10% of the time. Ironically, TEA-AODV rates the misbehaving agent with higher reliability than the trust worthy agent. This is due to the fact that by misbehaving slightly, B saves a lot of battery power, and by using TEA-AODV’s weighting system B is viewed as more reliable because of this. By misbehaving, B is classified as having very low reliability at t = 379s, as opposed to t = 298s when acting completely trustworthy. [22] did mention the possibility of adjusting the weighting system or adding an additional weight to counteract this behaviour in the future work section of their paper. Figure 12 : Agent A's opinion of B in Scenario 3 using TEA-AODV VI. CONCLUSION In this paper we investigated some fairness problems identified in online reputation system. We then adapted the problems to fit the domain of wireless networks; we used the adapted problems as criteria for testing the fairness of two trust based routing protocols, namely TAODV and TEA- AODV. Our results showed that trust based protocols can treat agents unfair in certain scenarios. In extreme cases such as scenario 3, using TEA-AODV, the trust based algorithm might even benefit a misbehaving agent. A. Future work. Identify more scenarios which may lead to unfair treatment of agents. Focusing specifically on the layout of the network. Simulation of large networks tend to hide scenarios within the network, which contribute to unfair treatment of nodes, such is suggested by [23]. Adapt the TEA-AODV protocol, to work with quantitative data rather than qualitative data. As well as, testing various weighting assignment applied to battery power versus trust. After adjusting the protocols for enhanced fairness within these small isolated scenarios, we plan to test the scalability of the problems identified by increasing the network size and enhancing the realism of the scenarios. ",Scientific Journal,0
Can Carbon Capture Technology Prosper Under Trump?,"THOMPSONS, Tex.  ‚Äî   Can one of the most promising  ‚Äî   and troubled  ‚Äî   technologies for fighting global warming survive during the administration of Donald J. Trump? The technology, carbon capture, involves pulling carbon dioxide out of smokestacks and industrial processes before the   gas can make its way into the atmosphere. Mr. Trump‚Äôs denial of the overwhelming scientific evidence supporting climate change, a view shared by many of his cabinet nominees, might appear to doom any such environmental initiatives. But the new Petra Nova plant about to start running here, about 30 miles southwest of Houston, is a bright spot for the technology‚Äôs supporters. It is being completed essentially on time and within its budget, unlike many previous such projects. When it fires up, the plant, which is attached to one of the power company NRG‚Äôs hulking   units, will draw 90 percent of the CO2 from the emissions produced by 240 megawatts of generated power. That is a fraction of the roughly 3, 700 megawatts produced at this gargantuan plant, the largest in the Lone Star State. Still, it is enough to capture 1. 6 million tons of carbon dioxide each year  ‚Äî   equivalent to the greenhouse gas produced by driving 3. 5 billion miles, or the CO2 from generating electricity for 214, 338 homes. From a tower hundreds of feet above the Petra Nova operation, the carbon capture system looks like a fever dream of an Erector set fanatic, with mazes of pipes and gleaming tanks set off from the main plant‚Äôs skyscraping smokestacks and busy coal conveyors. Petra Nova uses the most common technology for carbon capture. The exhaust stream, pushed down a snaking conduit to the Petra Nova equipment, is exposed to a solution of chemicals known as amines, which bond with the carbon dioxide. That solution is pumped to a regenerator, or stripper, which heats the amine and releases the CO2. The gas is drawn off and compressed for further use, and the amine solution is then cycled back through the system to absorb more CO2. Petra Nova, a   joint venture of NRG and JX Nippon Oil and Gas Exploration, will not just grab the CO2, it will use it, pushing compressed CO2 through a new pipeline 81 miles to an oil field. The gas will be injected into wells, a technique known as enhanced oil recovery, that should increase production to 15, 000 barrels a day from about 300 barrels a day. And since NRG owns a quarter of the oil recovery project, what comes out of the ground will help pay for the carbon capture operation. The plant, which has received $190 million from the federal government, can be economically viable if the price of oil is about $50 a barrel, said David Knox, an NRG spokesman. The company expects to declare the plant operational in January, Mr. Knox said. Aware of problems with carbon capture projects around the country and of the risks of hubris, he said: ‚ÄúWe‚Äôre not going to declare victory before it‚Äôs time. ‚Äù If the price of oil stabilizes or rises, and if tax breaks for developing the technology continue and markets for carbon storage develop, he said, utilities might ask, ‚Äúwhy would I not want to put a carbon capture system on my plant?‚Äù But developing   carbon capture has been neither straightforward nor easy. So far, problems have bedeviled major projects, often costing far more than projected and taking longer to complete. The federal government has canceled projects like Future Gen, which was granted more than $1 billion by the Obama administration. Carbon capture systems are not just expensive to build they tend to be   and make the plant less efficient over all  ‚Äî   a problem known as ‚Äúparasitic load. ‚Äù The Petra Nova carbon capture process gets its energy from a separate power plant constructed for the purpose, which NRG says makes the system more efficient than it would otherwise be, and frees up all of the capacity of the main power plant to sell all of the electricity it produces. The company estimates that the next plant it builds could cost 20 percent less, thanks to lessons learned this time around. If Petra Nova succeeds, it means a boost for carbon capture. Despite carbon capture‚Äôs problems, its supporters, including the Intergovernmental Panel on Climate Change and the International Energy Agency, call the technology, known as carbon capture sequestration, crucial for meeting emissions standards that can prevent the worst effects of climate change. ‚ÄúIf you don‚Äôt have C. C. S. the chance of success goes down, and the cost of success goes up,‚Äù said Julio Friedmann, an expert at the Lawrence Livermore Laboratories in California and a former Energy Department official. ‚ÄúIf you do have C. C. S. the chance of success goes up and the cost of success goes down. ‚Äù Carbon capture is proving itself, said David Mohler, the deputy assistant secretary for clean coal and carbon management at the federal Energy Department. Developing technologies often involves delay and cost overruns initially, he said. ‚ÄúYou cannot engineer all the bugs out from inside a cubicle  ‚Äî   you really have to do this stuff in the real world,‚Äù he said. Driving down costs, he noted, is what engineers and businesses do through research, development and production. He cited the plummeting cost of initially expensive technologies like solar power. ‚ÄúWe do figure things out as we go,‚Äù he said. What the Trump administration will do with carbon capture is, at this point, anyone‚Äôs guess. ‚ÄúThe technology only makes sense in a world where you are seeking to avoid putting CO2 into the atmosphere,‚Äù said Mark Brownstein, a vice president for the climate and energy program at the Environmental Defense Fund. But some supporters of the technology see reasons for hope. ‚ÄúI actually think it‚Äôs a moment of optimism,‚Äù said Senator Heidi Heitkamp of North Dakota, who met with Mr. Trump last month as a potential agriculture secretary. Ms. Heitkamp   legislation  with another Democrat, Senator Sheldon Whitehouse of Rhode Island, to expand and extend tax breaks for carbon capture projects. ‚ÄúWhat I saw with the   was a laserlike focus on jobs,‚Äù she said. ‚ÄúI think he was intrigued‚Äù about the economic opportunity that carbon capture could provide to keep coal power generation in the national mix, she added. One of the pillars of Mr. Trump‚Äôs campaign was his intention to revive the fortunes of the coal industry through support of   clean coal. And while the exact meaning of the   phrase is open to interpretation, it generally includes not just technologies that remove soot and   pollutants, but also carbon dioxide. Ms. Heitkamp said that businesses, too, were likely to continue development of carbon capture technology, since they planned their plant investments on a curve of decades and are loath to change course because of a single election. ‚ÄúThe decision they are making is not, what does the political outlook look like today? What‚Äôs it look like over the life of this plant?‚Äù Although she concedes that a   revival of coal‚Äôs fortunes is unlikely, carbon capture could be a way to extend the life of current facilities while keeping the nation‚Äôs energy mix diverse. Jeff Erikson, general manager at the Global C. C. S. Institute, which promotes the technology, said he did not expect to see a great number of new coal plants on the way. ‚ÄúI wouldn‚Äôt say carbon capture is going to rescue the coal industry,‚Äù he said, but pointed out that there is great potential for applying carbon capture to diverse natural gas plants and to industrial applications. Captured carbon can be used not just for oil production but a widening range of industrial processes, or can even be pumped into the ground. One of the most innovative approaches to carbon capture is being tried 50 miles east of the Petra Nova plant, in La Porte, Tex. where a consortium of companies is trying an entirely new approach to   power generation. In a $140 million, 50 megawatt demonstration project, the company, Net Power, will use superheated carbon dioxide in much the same way that conventional power plants use steam to drive turbines. This system, invented by a British engineer, Rodney Allam, eliminates the inefficiency inherent in heating water into steam and cooling it again. The power plant produces a stream of very pure, pressurized carbon dioxide that is ready for pipelines without much of the additional processing that conventional carbon capture systems require. The creators say that their technology will produce electric power at a price comparable to   power plants, the efficient   plants that burn gas to power one turbine and then use the excess heat from that process to generate additional power with a steam turbine. That means, in effect, that ‚Äúthe cost of being green is zero,‚Äù said Bill Brown, the chief executive of Net Power. ‚ÄúThe potential to capture CO2 at no additional cost would be a game changer,‚Äù said Fatima Maria Ahmad, a fellow at the Center for Climate and Energy Solutions, a think tank. Net Power combines the resources of Exelon Generation, a power company, the engineering and construction firm CBI, and 8 Rivers Capital, which developed the technology used in the project. The centerpiece of the project is a special turbine built by Toshiba and designed for the punishingly high pressure used in the process. And while any system that so closely resembles a rocket engine has the potential for what engineers delicately refer to as a RUD  ‚Äî   ‚Äúrapid unplanned disassembly‚Äù  ‚Äî   those working directly on the project say they have designed safety features that give them confidence. Even if the United States government shows little interest in reducing the nation‚Äôs carbon footprint under Mr. Trump and a Republican Congress, consortium officials say they expect to find ready customers from companies in the United States and around the globe, where the threat of climate change is fully acknowledged. ‚ÄúWe see this very much playing into all parts of the world,‚Äù said Daniel McCarthy, executive vice president for CBI‚Äôs technology operating group. Environmentalists remain divided on the issue of carbon capture, said John Coequyt, global climate policy director for Sierra Club. ‚ÄúThis is the issue where the biggest range of positions exists within the environmental community,‚Äù he said. Groups like the Clean Air Task Force favor it strongly. Other factions call clean coal a fig leaf to keep coal, with all of its environmental baggage, in the energy mix. And many suggest the billions of dollars spent on trying to capture carbon would be better directed to the technologies that don‚Äôt pollute the atmosphere in the first place. Dr. Friedmann, the former energy official, predicted that the technology would prove its usefulness. ‚ÄúIt‚Äôs convenient to just say ‚ÄòKeep it in the ground,‚Äô ‚Äù he said, referring to an   slogan. ‚ÄúWhat I prefer to say is ‚ÄòKeep it from the air. ‚Äô‚Äù",New York Times,0
La ni√±a afgana de National Geographic es arrestada en Pakist√°n,"La ni√±a afgana de National Geographic es arrestada en Pakist√°n   17:54 GMT 
La afgana de ojos verdes viene siendo investigada desde 2015 y ser√° trasladada a un reclusorio de mujeres. Twitter / @journalist11   
Sharbat Gula, la 'La ni√±a afgana' que protagoniz√≥ una de las portadas m√°s famosas de la revista 'National Geographic', fue detenida este mi√©rcoles en Pakist√°n por posesi√≥n ilegal de documentos, informa 'Pakistan Today' . 
La afgana de ojos verdes, inmortalizada por el fot√≥grafo Steve McCurry en 1985, es acusada de obtener documentos pakistan√≠es falsos para ella y sus dos hijos tras sobornar a varios funcionarios p√∫blicos. 
Al respecto, la Agencia Federal de Investigaci√≥n de Pakist√°n (FIA, por sus siglas en ingl√©s) se√±al√≥ que Gula, de 44 a√±os y que vive en este pa√≠s en calidad de refugiada, viene siendo investigada desde 2015 luego de que se conociera que numerosos ciudadanos afganos han intentado obtener la nacionalidad pakistan√≠ de forma ilegal para evitar ser deportados. 
La protagonista de la fotograf√≠a (un retrato que ha sido comparado con 'La Gioconda' de Da Vinci) ser√° trasladada a un reclusorio de mujeres hasta ser juzgada y podr√≠a pasar hasta siete a√±os en prisi√≥n.",rt.com,1
Nestle to launch new non-GMO products ... How shocked will Monsanto be?,"Nestle to launch new non-GMO products ... How shocked will Monsanto be? 
Thursday, October 27, 2016 by: Vicki Batts Tags: GMOs , Nestle , Monsanto (NaturalNews) One of the food industry's most prominent players recently announced that they will be expanding their line of non-GMO products, due to the ever-increasing customer demand for clean food. Nestle may have wowed consumers with their choice, but it may make some waves with their good friend, Monsanto .""The company is broadening its product offerings to give consumers more options with no GMO ingredients and identifying these products with the SGS-verified 'no GMO ingredients' claim,"" the food giant stated on Tuesday.""Nestle USA understands that consumers are seeking choice and many prefer to select products with no GMO ingredients,"" they declared.Of course, this is not Nestle's first move towards GMO-free products. In April, Nestle announced that they would be removing GMO ingredients from six of their top-selling ice cream products, as well. The company states it is trying to evolve along with consumer demands. It is great to see that companies are beginning to realize that consumers want options; no one wants to be forced to buy GMO products.It is easy to want to applaud Nestle for their decision to continue to expand their line of non-GMO products. However, it is also clear that this company is doing so out of their own financial interests ‚Äì not because they care about what people are eating. Organic, GMO-free foods are the newest trend, and smart manufacturers are beginning to see that they will not win anyone over by insisting that GM, pesticide-laden food products are safe. ""If you can't beat 'em, join 'em,"" is a philosophy Nestle has clearly taken to heart.Nestle is not an angelic company, even if they have decided to start serving up ""GMO-free"" options. Just three years ago, they donated millions of dollars to prevent and oppose GMO labeling in Washington state, along with Monsanto and other biotech firms. Truth Out reports that on October 18, 2013, the Grocery Manufacturers of America disclosed that several of their largest, most powerful players silently donated large sums of money to oppose Initiative 522. This bill would have required grocery items containing GMO ingredients to be labeled as such. The group chose to voluntarily release the names of the silent donors, after Washington state Attorney General Bob Ferguson filed a lawsuit against their concealment of corporate donors.Nestle was among the top three highest contributors, and donated a cool $1.5 million to keep GMO ingredients under wraps and off product labels. Nestle also made a large donation to oppose similar legislation in California the year before, in 2012. The bill ultimately failed, after Big Food and Big Biotech joined forces and together raised a staggering $46 million to prevent its passing. And we're supposed to believe they care?The controversial history of Nestle doesn't end with their consistent financial support of GMO labeling opposition efforts. It is a corporation that is wrought with wrongdoings and corrupt practices. Look no further than their outright theft of water in California.Given that the coastal state is currently being plagued by a devastating drought, you might be shocked to learn that just last year Nestle pumped a disturbing 36 million gallons of water out of one of the state's water sources, known as Strawberry Creek. Peter Gleick, president of the Pacific Institute and author of Bottled and Sold: The Story Behind Our Obsession with Bottled Water , estimates that Nestle is making millions of dollars in this way.""They're converting a public resource into private profit,"" he told Los Angeles Magazine .The most shocking thing is that their permit to pump water from the creek expired in 1988. The forest service has allowed Nestle to gouge the creek for water at will, so long as they continue to pay a minuscule access fee. While bottled water accounts for only a small fraction of California's water use, the overall environmental impact of what their practices are doing to a drought-stricken state have yet to be examined.Nestle has been subject to countless other controversies, including human rights violations, and has been host to many environmental and product safety issues.To put it simply: Nestle may be trying to win over customers with their non-GMO products, but they don't deserve to. Monsanto may be shocked, but only because Nestle is still one of their own. Sources:",naturalnews.com,1
Calling on Angels While Enduring the Trials of Job,"Angels are everywhere in the Mu√±iz family‚Äôs apartment in the Bronx: paintings of angels on the wall, ceramic angels flanking the ancient VCR, angels strumming lyres or blowing little golden trumpets on the bathroom shelves. As Jos√© and Zoraida Mu√±iz and their children have struggled to deal with a   series of trials and setbacks, including cancer, debilitating epilepsy, deep depression and near eviction, it has sometimes seemed as if angels and love were the only forces holding things together. Zoraida‚Äôs early life in Puerto Rico was like something from a tropical Dickens novel. She and her siblings and mother built a house by hand after a hurricane ravaged their home and the children‚Äôs father withdrew support. Then Zoraida‚Äôs grandfather  ‚Äî   the father of her absent father  ‚Äî   destroyed the house in a rage. She was barely a teenager when she met Jos√©, a Vietnam War veteran. With permission from her uncle, a judge, they were married. She was 14. He was 29. They moved to New York in 1983 and started a new life. He built boilers. She worked in construction, using skills she learned as a child, and in a clothing store. But in 1987, Mr. Mu√±iz began having violent seizures  ‚Äî   eight or 10 a day. They did not respond to medication. He could no longer work. She stopped working to take care of him. Still determined to live something like a normal life, they started a family. Their first child, Jos√© Jr. had a heart defect. By the time he was 2 he had had six   operations. That‚Äôs where the angels came in  ‚Äî   the first one was a painting, a gift from a cousin.  ‚ÄúWhen they operated on my son, they told me he was an angel, because he was supposed to die,‚Äù Ms. Mu√±iz, 50, said. ‚ÄúFrom there I figured that angels are taking care of me and protecting me and my family. ‚Äù All the angels are gifts from friends and relatives, or picked up off the street, just like all the furniture in the Mu√±izes‚Äô overstuffed apartment in a   complex on Westchester Avenue in the Bronx, much of it restored by Ms. Mu√±iz. ‚ÄúSo many people throw away things, so I don‚Äôt have to buy,‚Äù she said. A second son, Jesus, became epileptic at 3. A girl, Maria, completed the family. In 2007, Mr. Mu√±iz had what felt like a horrible, stubborn toothache. It turned out to be cancer of the lower jaw. Ms. Mu√±iz stayed in the room with her husband while he received radiation treatment. ‚ÄúI‚Äôm willing to take anything with him,‚Äù she said. ‚ÄúI never left him alone, and I never will. ‚Äù Radiation did not work. To save Mr. Mu√±iz‚Äôs life, surgeons removed his tongue and his lower jaw and cut a hole through his esophagus. Disfigured, depressed and unable to speak, he can consume nothing thicker than milk and needs   care. This is the household where the Mu√±iz children grew up. ‚ÄúWe‚Äôve been through every craziness,‚Äù said Jos√© Jr. 24, who has suffered depression so severe that he dropped out of college and confined himself to the apartment, ‚Äúevery up and down. ‚Äù For years at a time, the family held on, seemingly by a thread. Over the summer, the younger son, Jesus, 22, got a   job at a Zaro‚Äôs Bakery in Manhattan‚Äôs financial district. Because the family‚Äôs rent is tied to income, the rent tripled in August, to about $770 a month from $245. But Jesus had school bills to pay, and the family paid some of the funeral expenses for Jos√© Sr. ‚Äôs mother, who died over the summer, and things began to unravel. They fell behind on the rent and utilities. Food was often scarce. The family regularly skipped meals. It was around this time that Ms. Mu√±iz got in touch with Catholic Charities Archdiocese of New York, one of the eight organizations supported by The New York Times‚Äôs Neediest Cases Fund. It covered their back rent, got them warm coats and blankets and helped them apply for food stamps for the first time. And with $600 from the Neediest Cases Fund, the family paid its electric bill. Things are looking up in some ways. Jesus is returning to college, where he is on a   track and wants to be a paramedic. Maria graduated in December from a   nursing program. Thinking of her father and his illness, she wants to be an oncologist. Jos√© Jr. was just accepted to the New York Film Academy‚Äôs photography program. But Jos√© Sr. continues to battle cancer. Zoraida is severely depressed. What keeps her going? she was asked. She gestured toward her family, sitting beside her beneath the painted angels. ‚ÄúThey give me my strength, even if I have times I collapse,‚Äù she said. Jos√© Jr. agreed. ‚ÄúI use my parents and siblings as my motivation,‚Äù he said. ‚ÄúWe‚Äôre all there for each other,‚Äù Ms. Mu√±iz said.",New York Times,0
Mariah Carey‚Äôs Manager Blames Producers for New Year‚Äôs Eve Nightmare ,"Mariah Carey suffered through a performance train wreck in Times Square on New Year‚Äôs Eve as malfunctions left her at a loss vocally during her hit song ‚ÄúEmotions,‚Äù struggling to reach notes and to sync the lyrics and music. The trouble continued when she gave up on another of her   numbers, ‚ÄúWe Belong Together,‚Äù while a recording of the song continued to play, a confirmation that she had been  . But on Sunday, a dispute erupted between Ms. Carey‚Äôs representatives and producers of ABC‚Äôs ‚ÄúDick Clark‚Äôs New Year‚Äôs Rockin‚Äô Eve With Ryan Seacrest,‚Äù on which the singer was performing. Ms. Carey‚Äôs manager, Stella Bulochnikov, charged that the show‚Äôs producers had been aware of technical problems but did not fix them  ‚Äî   and chose to continue showing Ms. Carey‚Äôs messy performance ‚Äúto get ratings. ‚Äù ‚ÄúI will never know the truth, but I do know that we told them three times that her mike pack was not working and it was a disastrous production,‚Äù Ms. Bulochnikov told Us Weekly magazine on Sunday. ‚ÄúI‚Äôm certainly not calling the F. B. I. to investigate. It is what it is: New Year‚Äôs Eve in Times Square. Mariah did them a favor. She was the biggest star there, and they did not have their‚Äù act together. Dick Clark Productions, which produced the show, issued a statement on Sunday night saying that Ms. Carey‚Äôs performance woes had nothing to do with the production, and that any suggestion that the company ‚Äúwould ever intentionally compromise the success of any artist is defamatory, outrageous and frankly absurd. ‚Äù ‚ÄúIn very rare instances, there are of course technical errors that can occur with live television,‚Äù the statement said, adding that an initial investigation suggested that the production company ‚Äúhad no involvement in the challenges associated with Ms. Carey‚Äôs New Year‚Äôs Eve performance. ‚Äù A veteran audio producer, Robert Goldstein of Maryland Sound International, a company that has worked on the Times Square event for years, also said in an email that there had been no malfunctions with the sound equipment he oversaw. ‚ÄúEvery monitor and   device worked perfectly,‚Äù Mr. Goldstein said. ‚Äú I can‚Äôt comment beyond that and don‚Äôt know what her nontechnical issue may have been. ‚Äù A spokeswoman for Ms. Carey said on Sunday that the singer was not at fault for her performance. ‚ÄúUnfortunately there was nothing she could do to continue with the performance given the circumstances,‚Äù the spokeswoman, Nicole Perna, told The Associated Press. It was a rare meltdown on national television by one of the   recording artists of all time. Ms. Carey, a pop phenomenon in the 1990s who won five Grammys out of 34 nominations over the years, was the final   act on ABC‚Äôs ‚ÄúDick Clark‚Äôs New Year‚Äôs Rockin‚Äô Eve With Ryan Seacrest. ‚Äù She had just finished ‚ÄúAuld Lang Syne‚Äù when her star turn began to spiral out of control. ‚ÄúWe can‚Äôt hear,‚Äù she said in the opening seconds of ‚ÄúEmotions‚Äù after she sashayed down the stage before more than one million people who had gathered to watch the ball drop in Manhattan. Standing still with her left hand on her hip while music played, Ms. Carey told the audience that there had not been a proper sound check before her performance. Then she said, ‚ÄúWe‚Äôll just sing,‚Äù and noted proudly of her song, ‚ÄúIt went to No. 1. ‚Äù But she could not manage the notes that followed, and she either forgot lyrics or did not want to deliver a subpar performance. ‚ÄúWe‚Äôre missing some of these vocals, but it is what it is,‚Äù she said. ‚ÄúLet the audience sing. ‚Äù ABC quickly cut to shots of the Times Square crowd as Ms. Carey tried to perform some of her choreography. She continued suggesting fixes from the stage, and at one point seemed to defend herself. ‚ÄúI‚Äôm trying to be a good sport here,‚Äù she said. When the number ended, the crowd cheered her on. ‚ÄúThat was  ‚Äî   ‚Äù she said, pausing for effect, ‚Äúamazing. ‚Äù She seemed to recover at first with ‚ÄúWe Belong Together,‚Äù but there appeared to be another malfunction, and Ms. Carey again stopped singing. But this time, the prerecorded number kept playing. ‚ÄúIt just don‚Äôt get any better,‚Äù she said, and then left the stage. The cause of the problem was not immediately clear. After the performance, Ms. Carey posted a mildly profane slang phrase on Twitter with an ‚Äúupset‚Äù emoji, then wrote: ‚ÄúHave a happy and healthy new year everybody! Here‚Äôs to making more headlines in 2017. ‚Äù An ABC spokesman said on Sunday that the network would not comment on the problems with Ms. Carey‚Äôs performance.",New York Times,0
Enhancing Digital Business Ecosystem Trust and Reputation with Centrality Measures  ,"",,
"ironmen""",Scientific Journal,0,
Efﬁcient Enforcement of Dynamic Cryptographic Access Control Policies for Outsourced Data,"I. INTRODUCTION Organizations outsource their data to third-party service providers to reduce their growing data management costs. The service providers in turn take care of the management aspects, granting access only to users that are authorized to do so by the data owner. However, data outsourcing introduces concerns for conﬁ- dentiality [1]. This need for conﬁdentiality applies both to unauthorized users and the service provider (SP) because the data must be kept secret from its third party host [2]. For instance, organizations that outsource data concerning their customers’ personal information need a way to protect the information even from the SP in order to maintain adequate levels of trust from their customers. Hence, the data owner must protect the data that is sent to the SP but also allow authorized users to retrieve and read the information at a later time. A data outsourcing scenario in which the data owner trans- fers management of a set of data objects to a third-party SP is considered. In this data outsourcing scenario, the data owner grants access rights to subsets of the data objects to users who retrieve data objects from the SP. The asumption is that any modiﬁcations to an object are made by the users and the entire object is returned to the SP where it replaces the current copy. The SP enforces security policies to ensure that the data objects are visible only to the appropriate groups of users. Membership in these groups is dynamic so users can join and leave the groups. There are two problems that emerge here, ﬁrst, the data owner wishes to keep the data secret from the SP and second, the SP needs to protect his/her integrity by guaranteeing that only authorized users get to access the data that he/she receives from the data owners. Cryptographic access control (CAC) has been proposed as a way to solve these two security problems in data outsourcing. When cryptographic keys are used to secure the data, the data owner encrypts the data before it is transmitted to the SP. This key is then made available to all the users requiring access to the data but not to the SP. On the SP’s side, to facilitate data management, the data is categorized hierarchically by impos- ing a second layer of encryption on the data that is guided by a hierarchical cryptographic key management (CKM) scheme. The second key is transmitted to the users requiring access to the data, according to the rules of access deﬁned by the data owner. Therefore each user holds two keys, one that is used for authentication and the ﬁrst step of decryption on the SP’s end, as well as one that is used to decrypt the data into a readable format on the user’s end. The problem with this approach to securing outsourced data is that replacing keys to prevent security violations when group membership changes requires that the data owner and/or the SP re-encrypt the data with the new key to enforce data security. This procedure is expensive, particularly when large volumes of data are involved and/or the set of users with access to the data is dynamic. Therefore, a CAC scheme that circumvents the cost of re-encrypting the data when key updates occur is a desirable solution to the problem of securing outsourced data. This paper presents a novel and efﬁcient approach to secur- ing outsourced data that uses three keys. For convenience, the keys are denoted Bx, Kx, and Ax where 0 ≤ x ≤ n _ 1 and n is the maximum number of security classes in the access control hierarchy. The ﬁrst key, Bx is used by the data owner to encrypt the data before it is transmitted to the SP. This satisﬁes the requirement that the data remains secret from all unauthorized users, including the SP. In order to categorize the data received, the SP creates a second key, Kx that the SP uses to encrypt the data received from the data owners. The third key, Ax is generated by the SP and is used to encrypt the keys, Kx. The key Ax is then transmitted to the data owner and/or the users authorized, by the data owner, to access the data. Therefore each data owner and/or authorized user holds two keys, Ax and Bx. One that is used to encrypt the data before it is transmitted to the SP, Bx; and one that is used to retrieve the data from the SP’s end, Ax. Key updates are handled by updating the key Ax and re-encrypting only the data object containing the key Kx, instead of updating Kx and re-encrypting the data that was encrypted with Kx on the SP’s end. Moreover, avoiding data re-encryptions circumvents the problem of data unavailability that arises when a data owner attempts to access data while the SP is re-encrypting it with an updated key Kx. The rest of the paper is structured as follows. In Section 2, background material on the “Database-as-a-service” paradigm is presented with a focus on the aspect of CAC to outsourced data. Section 3 presents the proposed CKM approach and Section 4, presents a security analysis of the proposed CKM approach. Section 5 presents some performance results and concluding remarks are offered in Section 6. II. BACKGROUND The “database-as-a-service” paradigm emerged with the purpose of deﬁning methods of outsourcing data resources to SPs on the Internet. Most solutions focus on methods of executing queries efﬁciently and securely on encrypted outsourced data [3]–[6]. Typically, these methods are centered around indexing information stored with outsourced data. The indexes are useful for returning responses to queries without the need to decrypt the data (or response to the query). Only the party requiring and authorized to view the response should, in principle, be able to decrypt the result returned in response their query. The challenge in developing indexing techniques is in establishing a reasonable trade-off between query efﬁciency, exposure to inference, and linking attacks that depend on the attacker’s knowledge [7]. Additionally, there is the issue of a malicious user with access to the SP’s end being able to piece together information from parts of information gathered historically and then using this knowledge to transmit false information [2], [7], [8]. Other proposals avoid this by using cryptography to protect the data both from malicious users and from the SPs. Research on using cryptography to protect access to outsourced data began with the approach that Miklau and Suciu [8] proposed in 2003. In the Miklau et al. approach, different cryptographic keys are used to encrypt different portions of an Extensible Markup Language (XML) tree by introducing special metadata nodes in the document’s structure. Hence, the data remains secret to all the participants in the system and only those in possession of valid keys are able to decrypt the data. Although this approach secures the outsourced data and provides hier- archical access control, it does not address the problem of handling key updates and the need to categorize data at the SP’s end. De Capitani Di Vimercati, Foresti, Jajodia, Paraboschi, and Samariti [2], [7] build on this approach and consider the problem of authorization policy updates. The De Capitani Di Vimercati approach operates by using two keys. The ﬁrst key is generated by the data owner and used to protect the data initially by encrypting the data before it is transmitted to the SP. Depending on the authorization policies the SP creates a second key that is used to selectively encrypt portions of the data to reﬂect policy modiﬁcations. The combination of the two layers provides an efﬁcient and robust solution to the problem of providing data security in outsourced data environments. However, policy modiﬁcations or updates are handled by updating the affected cryptographic key and re- encrypting the data which is expensive encryption wise when large amounts of data are re-encrypted. The literature on CAC approaches, to addressing the prob- lem of secure data access and cost effective key management, has been investigated in the context of distributed environ- ments like the Internet [9]–[13]. Examples of applications in which access can be controlled using these CKM approaches include, PAY-TV, sensor networks and social networking en- vironments [14]–[16]. However, the case of controlling access to outsourced data differs from these cases in the sense that the SP needs to categorize all the data he/she receives in way that prevents malicious access and also minimizes the cost of authorization policy changes. Moreover, in the case of cryptographically supported access, when data re-encryptions involve large amounts of data, there is the risk of jeopardizing data consistency if data updates fail to be written onto a data version due to time-consuming re-encryptions. III. KEY MANAGEMENT Our proposed CAC scheme is based on the concept of hierarchical CKM and so we brieﬂy explain how hierarchical CKM schemes work before delving into our scheme. The proposed CAC scheme operates in two phases namely, the setup phase in which the keys are generated in ways that enforce the security policies and the update phase which determines how key updates are handled. A. The Hierarchical Key Management Model The concept of hierarchical key management builds on the lattice model of hierarchical access control that inspired multilevel access control mechanisms like the Bell-Lapadula approach [17], [18]. In the hierarchical key management model (HKM), a unique cryptographic key is assigned to each one of the classes in the hierarchy. A multilevel access control system, is enforced by allowing users, with keys belonging to higher level classes, to access information at lower levels depending on the rules of information ﬂow in the HKM. Standard methods of granting higher level users access to lower classes include the independent and dependent key management models. In independent key management schemes, each security class is assigned a unique key and access to lower classes is only possible if a user at a higher class holds the required lower class key [13], [15]. The dependent key management schemes on the other hand use a series of interrelated keys and access to a lower class is only possible if a user belongs to the lower class or holds a key that allows him/her to mathematically derive the required lower class key [10], [12], [18]. The key derivation process is performed through the application of a one-way function, so that a low level key can be derived (when this is authorized) from a high level key, but the reverse is not possible. Our proposed scheme assumes that, depending on the per- formance or security management beneﬁts sought, either one of these key management models can be used. The advantage of using the independent key management approach is that it is less encryption intensive than the dependent key management approach, since data is only re-encrypted at the security class affected by the key update. However, the independent key management approach requires re-distributing the updated key to all the classes requiring the new key. On the other hand, the dependent key management approach avoids distributing many keys by changing each of the keys in the sub-hierarchy associated with the affected key. Therefore, in the worst case the whole hierarchy will need to be updated and the data re- encrypted which, as mentioned before, is costly. The choice as to which approach to adopt depends on the context with which a security administrator is faced. The other assumption is that since the keys are typically generated by a central authority, or security administrator, secure key distribution can be handled by a key exchange protocol like the Diffe-Hellman key exchange scheme [19]. B. Phase 1: Setting up the System The data owner begins by categorizing users into exactly one of several groups (security classes) that are partially ordered. Additionally, each group Ui is associated with a data object di and a key Bi [18], [9], [10]. To encrypt the data before it is sent to the SP, the data owner uses the encryption function EBx (dx) = cx where 0 ≤ x ≤ n _ 1 and n is the maximum number of classes in the hierarchy. For instance in Figure 1, a two class hierarchy with data objects di, and dj that are encrypted with the keys Bi, and Bj to obtain encrypted data objects ci, and cj is considered. Moreover, the data owner can facilitate key management by using a dependent key management scheme to generate the keys Bi. In this case, therefore, each group is assigned a single key that can be used to either directly decrypt and access the data, or to derive the required key. For instance, in the totally ordered hierarchy depicted in Figure 1, Bi can be used to derive the key Bj that is associated with the group Fig. 1. An example of a totally ordered hierarchy with four security classes below Uj. The reverse is not possible because keys associated to lower level groups cannot be used to derive keys belonging to higher level groups. Once the data has been encrypted, the data owner transmits it to the SP for management. The SP deﬁnes a partial order hierarchy to categorize all it receives. The partial order hierarchy is enforced by re-encrypting each data object received with a second key, Ki that enables the SP to authenticate and grant access only to authorized users. The encryption function that the SP uses is as follows: the data that EKx (cx) = _x where 0 ≤ x ≤ n_1 and n is the maximum number of classes in the hierarchy. For instance in Figure 2, a two class hierarchy with encrypted data objects ci, and cj that are encrypted with the keys Ki and Kj to obtain double encrypted data objects _i, and _j is considered. The keys Ki can also be deﬁned Fig. 2. Data Hierarchy on the Service Provider’s End using a dependent key management scheme, in which case each class gets assigned only a single key. The reason for this is mainly to facilitate key management. Finally, to grant users access to the data on the SP’s end, the SP creates a second set of keys Ai that are used to encrypt the keys Ki using the encryption function EAx (Kx) = _x where 0 ≤ x ≤ n_1 and n is the maximum number of classes in the hierarchy. Thus an encrypted object _x is obtained and can only be decrypted with the correct Ax. The SP then transmits the keys Ax to the data owner who takes care of sharing the keys with the users according to the portions of data that users are authorized to access. For instance, as shown in Figure 3, the SP uses the keys Ai and Aj to encrypt the keys Ki, and Kj to obtain _i, and di Ui dj Uj Level i Level j ci cj EncryptionAfterEncryptionAfter ci Ui cj Uj Level i Level j i j After double encryptionAfter double encryptionij_j. The keys Ai and Aj are then shared with the data owner who in turn will take care of distributing the keys to the users belonging to the groups Ui, and Uj respectively. So, in this case, users in group Ui get assigned the key Ai and users in group Uj get assigned the key Aj. Fig. 3. Key Protection Procedure Each user holds two keys, say Ai, Bi and the user accesses data, say di, by submitting their key Ai to the SP for authentication. The SP will use the key Ai to decrypt _i and obtain Ki that will then be used to decrypt _i to obtain ci that is then handed to the user. To read ci, the user will use his/her key Bi and decrypt ci to obtain a readable form of di. So in essence, the only keys a user in Ui is ever aware of are Ai and Bi. C. Phase 2: Handling Key Updates For simplicity, assume that key updates are triggered by changes in user group membership and the objective of key updates is to prevent a user who has left a group from subsequently accessing a data object, say di. In this case, the data owner will send a message to the SP to alert him/her of the change. In previous approaches, key updates imply re- encrypting all the data objects associated with the affected key. Since the cost of data encryption is directly proportional to the size of the data object, re-encrypting large ﬁles is costly and time-consuming. Our approach overcomes this drawback by allowing the SP to replace the key Ai of the affected group and re-encrypt the key Ki. The new Ai is then sent to the users remaining in group Ui. Therefore, only the data object containing the key is re-encrypted and since this is a considerably smaller data object in comparison to the actual data, the overall cost of key updates is reduced. Since access to ci requires that a user presents a correct Ai to the SP, once Ai is updated, the old Bi would be useless to the user who has left the system, because he/she will be unable to retrieve ci. IV. SECURITY ANALYSIS To evaluate the theoretical security of our key management approach, three aspects are considered namely, the security of the keys that the users hold, namely Ax and Bx, the security of the data dx, and the security of the key Kx. Our objective here is to show that no single user in the system has enough knowledge of the assigned keys to be able to deduce the true contents of the data unless they hold the “correct” keys. In fact, once the data owner encrypts and transfers the data to the SP for management, he/she can not retrieve the data without the “correct” keys, Ax and Bx. A. Security of Bx and dx As mentioned in Section III-B, the key Bx is used to encrypt the data dx before it is transmitted to the SP and transmitted to all the users that have been granted access to the data dx. Since the key Bx is kept secret from the SP, the SP cannot read dx, and therefore the data stays secure. This satisﬁes the ﬁrst concern for security on the user’s end, namely the wish to keep the data secret from the SP. In selecting a key generation scheme, security against collusion attack is provided by selecting a scheme that is provably secure against collusion [9], [20], [21]. Another aspect worth considering is the case in which a malicious user, say Alice steals Bx during its transmission to the users in Ux. Since Alice does not hold Ax, she will be unable to exploit her knowledge of Bx to retrieve cx unless she presents the “correct” Ax to the SP. B. Security of Kx The key Kx is created by the SP and is never shared. All encryptions and decryptions with the key Kx are performed on the SP’s end. Therefore, Kx remains protected. C. Security of Ax With respect to the security of Ax two cases are considered, in the ﬁrst case a user who left a group attempts to retrieve and decrypt cx and in the second case, a malicious user intercepts Ax and attempts to retrieve and decrypt cx. In the ﬁrst case, to retrieve the current version of the data dx, the user will begin by submitting his/her key, Ax, to the SP for authentication. The SP uses the supplied Ax to decrypt _x which is not possible because Kx has been re-encrypted with an updated Ax. Therefore, even though the key Bx was not updated, the user will be unable to retrieve cx and decrypt it to read dx. The second case involves a malicious user attempting to gain access to dx by intercepting Ax. As shown in Figure 4, the malicious user, say Alice, intercepts Ax during its transmission to the users in group Ux. Alice then presents Ax to the SP and since Ax is correct, the SP retrieves Kx from the decryption of _x. The Kx obtained is then used to decrypt _x and retrieve cx that is returned to Alice. However, since cx needs to be further decrypted using Bx which Alice does not hold, obtaining cx is useless. Therefore, our approach to key management, gives a strong guarantee of securing outsourced data. The only case in which Alice can hope to retrieve dx, is that in which she steals both Ax and Bx. However, if Ax is updated fairly frequently the cost of stealing the keys should outweigh its beneﬁts. Moreover, a secure key exchange protocol based on an asymmetric cryptography technique [22], could also be used to circumvent key interception attacks. AA__Fig. 4. Malicious Attempt to Retrieve dx V. PERFORMANCE ANALYSIS The scalability of the proposed CAC scheme to controlling access to outsourced data is evaluated with a set of experiments conducted on an IBM Pentium IV computer with an Intel 3.00Ghz processor and 1GB of RAM. A. Comparison Strategies The performance of our proposed CAC scheme is compared to a naive approach and the approach proposed by De Capitani Di Vimercati et al. 1) Naive Approach: Our naive approach gives basically a worst case situation in which the data owner encrypts the data and sends it to the SP. The data is not re-encrypted on the SP’s end. The key management model is based on a series of partially ordered interdependent keys and so every time a user group’s membership changes the data owner reacts by updating the keys throughout the entire hierarchy, re-encrypting the data associated with the affected keys and re-transmitting the data to the SP. This approach is costly encryption wise and insecure because the key updates, data re-transmissions and key distributions create more opportunities for security attacks. This case is a baseline and the CAC scheme used is the one that Akl and Taylor proposed [18]. The reason for this choice is that the Akl and Taylor CAC is an example of a scheme that requires updating keys throughout the entire hierarchy in reaction to key updates. 2) De Capitani Di Vimercati et al: In this case the ap- proach proposed by De Capitani Di Vimercati et al. [1] is implemented using the scheme proposed by Atallah, Frikken and Blanton [9] to handle key management. Here the data is encrypted on the data owner’s end and transmitted to the SP where a second layer of encryption is applied to the data. Key updates are handled by updating only the key associated with the data as well as the keys that are used to authenticate higher level users before granting them access to the data. 3) Proposed Key Management Approach: Our proposed CAC scheme is implemented as outlined in Section III-B, and use the Akl and Taylor scheme [18] to handle key management. This is to show that our design approach to handling key management in outsourced data scenarios is such that key updates are handled effectively even in the worst case scenario. B. Comparison Metrics For clarity, some deﬁnitions of the performance evaluation terminology used is presented after which a description of the experimental platform is given. • Cost of System Setup: This is the cost of key generation and data encryption both on the SP’s end and the data owner’s end. This cost is the total time (in minutes per hierarchy size) that it takes to set up the system with respect to the size of the ﬁle. • Cost of Key Updates: This is the cost (time it takes in minutes per ﬁle size) of re-establishing security when a user group’s membership changes and the system needs to update the keys to prevent the departed user from continuing to access the system. C. Experimental Setup All three key management approaches were implemented on a Microsoft Windows XP platform using the Java 2 Standard Development Kit and Eclipse [23]–[26]. While our proof of concept implementation uses Triple Data Encryption Standard (Triple DES) keys for the encryption and decryption procedures, it can easily be replaced by the current Advanced Encryption Standard (AES) standard [27]. In Triple DES the key size is 168 bits and the block size 64 bits. However, due to the meet-in-the-middle attack the effective security Tripe DES provides is only 112 bits [28], [29]. In the experiments we use hierarchies with 3-73 security classes (groups). The hierarchies are formed in a way that allows roughly 2-10 sub-groups to be directly connected to a group. The hierarchy sizes and shapes are based on the computational limitations of the machine that we used for the experiments. It is also worth noting that different structures of hierarchies might yield slightly different results depending on the time it takes to select a valid set of exponents to use in generating the keys. The cost of system setup is evaluated for the different hierarchy sizes with a ﬁle of size ≈ 32MB. We also evaluate the encryption overhead generated in setting up the system with respect to the size of a ﬁle. In this case we used ﬁles of sizes 1KB,...,100MB. The experiment on cost of key updates is ﬁrst performed with a ﬁle of size ≈ 32MB and then with varied ﬁles sizes of 1KB,...,100MB. In the ﬁrst case using a standard ﬁle size of ≈ 32MB allows us compare the cost of updates with respect to different ﬁle sizes. While the second case allows us focus on the encryption overhead generated, in handling key updates to reﬂect authorization policy changes, with respect to varied ﬁle sizes. Henceforth, we use “the Vimercati approach” to refer to the method that De Capitani Di Vimercati et al. proposed [1]. D. Results Our performance analysis is centered on two aspects: cost of system setup and cost of key updates to reﬂect authorization policy changes. In the following sections we present results to highlight the performance enhancements offered by our proposed approach in comparison to a naive or baseline case and the Vimercati approach. 1) Cost of System Setup: The experiments here evaluate the time it takes each of the three key management approaches that we described in Section V-A to generate the required keys and encrypt the data associated with every security class in the hierarchy, both on the data owner’s and the SP’s ends. The standard deviation for each point plotted is ±0.83 minutes. As shown in Figure 5 the cost of generating keys in our approach during system setup is highest in comparison to the naive and Vimercati approaches since our scheme generates three keys instead of two as the other schemes do. We note, however, the cost of key generation in our proposed scheme is reasonable since it takes less than a minute to generate keys for a hierarchy of 73 security classes. In fact, a closer look at Figure 5 and Figure 6 indicate, that data encryption is the more cost intensive operation and that the the costs of key generation and data encryption grow linearly with the hierarchy size. that 73 security classes) create a wider divergence between our proposed scheme and the other two. This is because the cost of generating the third key and encrypting the associated key ﬁles has a bigger impact on the overall cost as the size of the hierarchy grows. Fig. 6. Hierarchy System Setup: Cost of Key Generation and Data Encryption per Our second experiment evaluates the encryption overhead generated in setting up one security class on the system. As shown in Figure 7, in this case we consider the effect of different ﬁle sizes on the cost of data encryption using each of the three approaches. As expected, we note that with all the three approaches the overhead generated during encryption is proportional to the size of the ﬁle. The encryption overhead generated using our proposed scheme is marginally higher than that in the Vimer- cati and naive approaches. However a closer look indicates that the difference is roughly 0.1 minutes which is due to having to encrypt the key ﬁle on top of encrypting the data ﬁles. 2) Cost of Key Updates: The experiments in this section evaluate the time it takes our proposed key management approach to generate a new key in response to a key update request. In the ﬁrst experiment, we consider the worst case Fig. 5. System Setup: Cost of Key Generation Only per Hierarchy For smaller sized hierarchies it can be noted that the setup costs are relatively similar while larger hierarchies (i.e. Fig. 7. Encryption Overhead: Cost of system setup with respect to ﬁle size in which keys throughout the whole hierarchy need to be Hierarchy Size (Number of Security Classes - 32MB File per Class)Time (Seconds)updated. The performance of our approach was compared to the Vimercati approach and the naive approach. The standard deviation for each point plotted is ±0.83 minutes in the Vimercati and naive approaches and ±0.5 minutes in our proposed approach. As shown in Figure 8 the cost of key updates in the naive approach remains unchanged from the cost of setting up the hierarchy and is the most costly of the three approaches. It is also worth noting at this point that this approach is the most insecure because the data is exposed brieﬂy each time a key is updated. Additionally, since the data needs to be constantly re-transmitted to the SP, frequent key updates open up more possibilities for malicious interceptions. Fig. 8. Key Updates: Cost of Replacements and Re-encryptions The Vimercati approach performs much better than the naive approach, which is to be expected since the encryption time is cut roughly by a half. Only the keys on the SP’s end need to be updated and this cost grows linearly with the hierarchy size. Our proposed approach outperforms the previous two be- cause updates are handled by replacing the key Ax and re- encrypting the key Kx, associated with each class in the hierarchy. This key ﬁle is smaller than the data ﬁles, hence the gains in performance. Our second key update experiment looks at the impact of ﬁle sizes on the overhead generated during key updates. In this case a single security class in the hierarchy that is associated with a ﬁle of size 1KB,...,100MB, is considered. As shown in Figure 9, the observation is that the performance of the three approaches is similar for the smaller ﬁles, i.e. 1KB,...,1MB. This conﬁrms our observation that the size of the ﬁle is directly proportional to the encryption overhead generated. However, as the ﬁles get larger the performance of the naive and Vimercati approaches grow worse while our proposed approach’s performance stays fairly constant. While the cost of updates is proportional to the ﬁle and hierarchy size, the differences in cost are more signiﬁcant in the Vimercati and naive approaches than in the proposed approach. This is because the key ﬁle size is fairly constant and so the encryption costs relatively the same. Fig. 9. Key Updates: Cost of Replacements and Re-encryptions per ﬁle size VI. CONCLUSIONS In this paper, a solution is presented to the problem of han- dling authorization policy modiﬁcations when access control is enforced cryptographically in situations where data is stored and offered to clients by an external server. The discussion of the background on this topic showed that most solutions have tended to focus on indexing techniques that strike a balance between query efﬁciency and controlling exposure to inference attacks. However, a key concern in outsourced data environments is that of protecting the secrecy of the data effectively. Data owners want guarantees that their data will not be read and/or modiﬁed by the SPs and SPs need some way of categorizing the data to facilitate data management while ensuring that malicious access is prevented. More recently, De Capitani Di Vimercati et al. [1], [2] proposed a solution to addressing the concern for data security on both the data owner’s and SP’s ends. Their approach uses a two layered encryption model that allows the data owner to encrypt the data before it is transmitted to the SP where a second encryption layer is applied to the data. However, their solution faces the drawback that key updates are handled by requiring the SP to generate a new key and re-encrypt the affected data. Moreover, as shown in Figure 9 since re- encrypting large data objects is time consuming, issues related to copy consistency are likely to arise if users attempt to write updates on to the data during the encryption. This problem was addressed by designing an access con- trol approach that is supported by three cryptographic keys, Ax, Bx, and Kx. The keys, Bx and Kx are used to protect the data from both unauthorized users and the SP by allowing the data owner and SP to apply two layers of encryption. The last key, Ax, is used to encrypt the key, Kx, that the SP used to apply the second layer of encryption to the data and is distributed to the users authorized to access the data. To access the outsourced data, a user needs to present the key, Ax to the SP for authentication. Once this test is passed the user is handed a data object that can only be decrypted into a readable format if the user holds the key with which it was encrypted. So in essence, each user holds two keys,",Scientific Journal,0
NBA Team Cancels Anthem Singer After She Tries Wearing Controversial Race Shirt Onto Court,"‚ÄúI was never given any kind of dress code. I was never asked beforehand to show my wardrobe.‚Äù Streeter (left) and the 76ers home court at the Wells Fargo Center in Philadelphia 
‚ÄúI also felt it was important to express the ongoing challenges and ongoing injustice we face as a black community within the United States of America ‚Äî that‚Äôs very important to me,‚Äù Streeter said. ‚ÄúYes, we live in the greatest country in the world but there are issues that we cannot ignore. This can‚Äôt be ignored.‚Äù Advertisement - story continues below 
‚ÄúI was angry, extremely, extremely angry and disappointed and honestly brought to tears by all of it. It broke my heart,‚Äù she said. ‚ÄúHonestly, I was very excited about being able to perform the national anthem. I was really looking forward to that.‚Äù 
Firstly, pro tip: The more times you say ‚Äúhonestly‚Äù about something, the more it becomes exponentially less likely to the listener that they will believe you‚Äôre being honest. In this case, I believe Streeter honestly wanted to make a point, not sing the national anthem, which is the matter of contention here. 
The national anthem is sung to honor the heroes who have served (and died) for our country before major events. It isn‚Äôt time to make a point or ‚Äúhave a discussion‚Äù (a discussion, may I add, that usually takes the form of a monologue from whatever social justice warrior is giving it ‚Äî who usually acts surprised and disappointed when people have a real discussion and push back against their disrespect of the flag). 
If you‚Äôre singing the national anthem, it isn‚Äôt about your opinions. It isn‚Äôt about your talent. It isn‚Äôt about what you‚Äôve done. Advertisement - story continues below",conservativetribune.com,1
Project Veritas Releases Fourth Video Exposing Illegal Contributions and High Level Political Corruption,"Home / Be The Change / Government Corruption / Project Veritas Releases Fourth Video Exposing Illegal Contributions and High Level Political Corruption Project Veritas Releases Fourth Video Exposing Illegal Contributions and High Level Political Corruption Jack Burns October 27, 2016 1 Comment 
As The Free Thought Project has reported, Project Veritas ‚Äô undercover sting operation, of the Clinton Campaign and the Democratic National Committee‚Äôs alleged subversive tactics using Democracy Partners and Americans United for Change, has now been exposed for the whole world to see. This week, PV released the fourth video in their series, and once again, Democracy Partners‚Äô founder, Robert Creamer, is at the center of the controversy (It must be noted DP makes no mention on its website of Creamer as its founder and currently has him listed as a consultant ). 
Creamer, who visited the White House 342 times (47 of those with Barack H. Obama), was caught on PV‚Äôs fourth video, and in emails, arranging an illegal $20,000 foreign contribution to Americans United for Change (AUFC), the same group which proudly stated was creating chaos at Trump events. 
Activists at PV arranged the donation with the knowledge it was illegal, to see if AUFC would take the bait. They did. And in return, they agreed to unwittingly take on another PV activist as an AUFC staffer. In other words, PV paid AUFC $20K, and was allowed to place the pseudo-donor‚Äôs ‚Äúniece‚Äù inside the organization, a move PV felt was worthwhile to be able to expose the organization‚Äôs dirty deeds. It worked. 
PV‚Äôs first video revealed AUFC‚Äôs Scott Foval coordinated and arranged for paid instigators to go out and ‚Äústart shit‚Äù with Trump protesters, all of which was caught on tape for the mainstream media to use in its echo chamber to paint Trump and his supporters as racists, xenophobes and bigots. 
PV‚Äôs second video showed how AUFC‚Äôs organization and its affiliates commit intentional mass voter fraud. The second video is an instructional video of sorts on, ‚Äúhow to successfully commit voter fraud on a massive scale,‚Äù according to PV president James O‚ÄôKeefe. 
In the third PV video, Creamer can be seen stating Hillary Clinton herself wanted ‚Äúducks on the ground‚Äù in some bizarre idea to stage anti-Trump demonstrations by activists dressed as Donald Duck. The DNC‚Äôs Donna Brazelle was also implicated in collaboration with the scheme. 
But it‚Äôs the fourth video today that‚Äôs getting all of the attention now, as it purports to show AUFC knowingly received a $20,000 illegal wire transfer out of Belize. Creamer, who we know has a close relationship with the White House via visitor logs, also brags about his friendship with Barack Obama, and intimates he can arrange meetings with the president and the former secretary of state, for a price. 
‚ÄúI‚Äôve known the president since he was a community organizer in Chicago,‚Äù Creamer proudly stated adding, ‚ÄúEvery morning, I am on a call at 10:30 that goes over the message being driven by the campaign (Clinton) headquarters.‚Äù With Creamer having spent nearly a year inside the White House, and his relationship with the president, it may be easy for some to conclude the Obama administration was also involved in Creamer‚Äôs schemes. 
PV invented the donor and shell company based in Belize, Charles C. Roth III of Repulse Bay Company, who the organization claimed to Democracy Partners was a rich donor who wanted to make a difference and stop Trump from becoming the next POTUS. Creamer then tapped the president of AUFC, Brad Woodhouse, to iron out the details of the illegal contribution. The wire transfer was arranged and transmitted. Creamer confirmed as much. 
On a recorded phone call Creamer can be heard saying the funds ‚Äúabsolutely came through.‚Äù In exchange for the donation, the outspoken friend of the Obamas granted an internship to Roth‚Äôs niece, an undercover PV journalist, to work inside Democracy Partners. 
In another recorded video session, Creamer can be seen and overheard describing what he and his partners at AUFC do. Creamer stated he gets his marching orders at his 10:30 conference call to troll the Trump/Pence campaign creating incidents for the ‚Äúearned media‚Äù to cover which echo the message the Clinton campaign wanted to project. That ‚Äúearned media‚Äù, as we‚Äôve come to realize through the latest email dump by Wikileaks, is the mainstream media which is apparently in the tank for Clinton. 
Creamer also discussed allegations of sexual misconduct by Trump accusers and intimated those allegations came first through his organization‚Äôs connections, which is a contention Trump made at the third and final debate with Clinton. Creamer also hinted around at the possibility of arranging a meeting between Roth and Obama. ‚ÄúI do a lot of work with the White House on their issues,‚Äù he stated plainly adding, ‚Äúhelping to run issue campaigns (immigration, ACA, gun violence).‚Äù 
While the PV sting was focused on the presidential election of 2016, and the White House, the DNC, and the Clinton Campaign‚Äôs involvement in painting Donald Trump as a racist, xenophobe, and bigot, Creamer‚Äôs comment on his involvement in working with the White House on issues of gun violence, leaves more questions than answers. 
As an aside, here you have a well-connected political hit man like Creamer, working with AUFC to create violence, chaos and anarchy at Trump rallies, discussing his role in promoting gun violence issues for the White House. Taken together, considering Creamers comments and actions, all may lend credibility to conspiracy theorists‚Äô claims the Obama administration has been actively promoting false flag school shootings, on-air shootings of reporters, and mall shootings, all in an attempt to sway the American public against the people‚Äôs right to keep and bear arms (2 nd Amendment to the U.S. Constitution). Creamer admitted as much saying he was, ‚Äútrying to make America more like Britain when it comes to gun violence issues.‚Äù 
Turning again to PV fourth video, upon realizing PV was soon to go public with its expose‚Äô, AUFC returned the 20k foreign company shell company donation on September 9 th . O‚ÄôKeefe, as an epilogue to the video, openly wondered why AUFC held onto the funds for a month before returning what they undoubtedly knew to be an illegal contribution. Share",thefreethoughtproject.com,1
US Votes 'No' As UN Adopts Landmark Resolution Calling to Ban Nuclear Weapons,"Co-sponsored by 57 nations, L41 calls for a 2017 conference ‚Äòto negotiate a legally binding instrument to prohibit nuclear weapons, leading towards their total elimination‚Äô The United Nations on Thursday adopted a landmark resolution calling for the ultimate elimination of nuclear weapons worldwide.
Resolution L.41 (pdf) was accepted by a vote of 123-38, with 16 member nations abstaining. The vote was held during a meeting of the First Committee of the UN General Assembly, which deals with disarmament and international security matters.
‚ÄúFor seven decades, the UN has warned of the dangers of nuclear weapons, and people globally have campaigned for their abolition. Today the majority of states finally resolved to outlaw these weapons,‚Äù said Beatrice Fihn, executive director of International Campaign to Abolish Nuclear weapons (ICAN).
Setsuko Thurlow , a survivor of the Hiroshima and leading proponent of a ban, also celebrated Thursday‚Äôs vote.
‚ÄúThis is a truly historic moment for the entire world,‚Äù Thurlow said. ‚ÄúFor those of us who survived the atomic bombings of Hiroshima and Nagasaki, it is a very joyous occasion. We have been waiting so long for this day to come.‚Äù
As expected, nuclear powers including the United States, France, Canada, Israel, Russia, and the United Kingdom, as well as several of their European allies, were among the nations who voted against the ban.
It was a long journey to get to this point, but totally worth it! #goodbyenukes #FirstCommittee pic.twitter.com/SM5mFzSXzf
‚Äî Michael Hurley (@mdghurley) October 27, 2016",anonhq.com,1
A Compliant Assurance Model for Assessing the Trustworthiness of Cloud-based E-Commerce Systems ,"VeriSign The systems were hacked Apr 2011 Global Sony Jul 2011 Jul 2011 China Alibaba South Korea ESTsoft Oct 2012 Global Google Security and privacy breach Inadequate gold supplier verification measures where goods that customers ordered were never received Information system security breach by hackers Cloud service outages that resulted in slow responses and server unavailability, which affected various other vendor websites is [4]. Another challenge E-commerce assurance models, such as policy statements on an online vendor’s site or a third party assurance seal, have been devised in an attempt to promote online consumer trust. Unfortunately, there are some short-comings that need to be addressed by future web seal designers. One of the shortcomings is that assurance models do not seem to provide the desired assurance, but only perceived assurance, to consumers that e-commerce assurance models are static and do not provide continuous compliance assessments to applicable legislation and standards by online vendors or cloud service providers. In an environment where the changes to the technology platform and configuration may be frequent, such as the cloud, continuous assurance would be more beneficial [15].The cloud has numerous challenges, such as failure to disclose how the data collected via the cloud will be used and continuous availability of the cloud resources [14]. A cloud assurance model should be designed in such a way that these challenges are addressed in order to encourage online consumer trust. The objective of this paper is to present a comprehensive survey of a cloud-based e- commerce assurance model and to propose a compliant cloud- based assurance model for e-commerce. The major contributions of this paper are as follows: _ Development of a compliant cloud-based assurance rating [AHP] and page model based on cooperative (analytical hierarchy process ranking) for e-commerce trustworthiness intelligent Impact Undisclosed information was stolen An estimated 77 million play station accounts were hacked and confidential information such as credit card information was exposed This resulted in financial losses estimated at $1.94 million 35 million people's personal information was exposed Loss of productivity as a result of slow response of systems _ Knowledge generation as a to understand e-commerce trustworthiness in general and cloud-based e-commerce in particular detail. assurance models reference guide This paper is arranged as follows: section II discusses the cloud-based assurance models, section III proposes the compliant cloud-based assurance model, section IV, descriptive deployment analysis, lastly the conclusion is presented in section V. II. CLOUD-BASED ASSURANCE MODELS AND ASSOCIATED CHALLENGES A. Cloud-based e-commerce assurance models Cloud-based assurance models are in the form of certifications or manual compliance checks achieved by the cloud service provider to numerous standards and even laws such as privacy laws. Reference [1] is an example of a cloud service provider that provides its customers with an assurance model that lists the type of certification received by the service provider, such as International Organization for Standardization (ISO) 27001 and Payment Card Industry certification. The declaration of certification by [1]’s web services is aimed at encouraging trust among its clients or merchants to use its technology platform. In a nutshell, the main objective of assurance models is to reduce fear to transact among team consumers and rather encourage online customers to trust the process. Sections 1 to 3 discuss the different types of assurance models in cloud-based e-commerce environments. that can be found 1) Cloud-based policy assurance models The policy assurance models are provided as self-assurance measures in a cloud-based environment by the online vendor or the cloud service provider. Policy statements are normally displayed at the bottom of the website home page, hyperlinked to the policy detail. The advantage of a policy assurance model is that it is a common form of e-commerce assurance. The disadvantage is that in a cloud-based e-commerce environment policy assurances may not entail all the information pertaining to the applicable privacy laws [22] and this is an area that needs to be improved. Examples of policy assurance models can be found in [1] and [5]. Static cloud-based assurance models 2) A static assurance is a seal that is displayed on a certified cloud service provider’s site; it requires a user to click on the website to read the detail. The Trusted Cloud Data security certification is an example of a cloud-based assurance model for service providers within the European Union domain, where both the company and the service provider are regulated by the European Union Safe harbor principles [23]. While this type of assurance model can be useful in providing assurance to countries that are EU-resident, it eliminates non-EU countries because it is not adaptive to accommodate the laws of other countries. The disadvantage of this type of seal is failure to provide continuous assurance in a dynamic cloud-based e- commerce environment. information concerning 3) Continuous assurance models A continuous or variable seal assurance model provides regular online updates on the assurance status of a particular website. An example of a cloud-based assurance model is the [14] web seal. This has been designed to provide security assurance in the cloud environment where it scans the website for any known vulnerabilities. The advantage of this model is that it provides online real time assurance. What needs to be done to improve these seals is to factor in compliance to best practice standards and regulatory laws for the users to trust the system, since providing assurance on known vulnerabilities may not always be sufficient. B. Challenges of Current Assurance Models the cloud-based computing environment, In there are numerous challenges, which must be addressed in order to encourage online consumer trust. One of the ways in which these challenges can be addressed is by providing e-commerce assurance, which will encourage online consumer trust. Reference [22] has identified some of the key assurance challenges in the cloud environment and these are discussed in sections 1 to 4 below. 1) Risk of unauthorized use of personal information as an assurance challenge The cloud presents a challenge to customers regarding privacy of their information. particularly on how the data that customers divulge will be used or stored by the cloud service provider. In the event of a data privacy breach, it often becomes very difficult to ascertain which laws are applicable, more especially if it was not specified at the time of concluding the agreement between the online vendor and the cloud service provider [9]. In order to address this challenge, the proposed cloud-based assurance model assesses adaptive laws on which the vendor and the cloud service provider agree to be applicable in the event of a breach. 2) Security of the data as an assurance challenge Information security in the cloud is paramount. When a security breach occurs, the impact of that breach is huge, since it is likely to affect more than one website affiliated to that cloud service provider. Amazon is an example of a cloud service provider that prides itself on its ability to provide a secure website environment for business purposes. However that does not mean that every website that is affiliated to Amazon will be protected by a standard seal program. To illustrate this fact, we examined the assurance status of the different websites affiliated to Amazon, such as [3], which has two assurance seals, i.e. Norton Secured and Bizrate, whereas [19] has no seal displayed, notwithstanding the different business types. The proposed model would assist in ensuring that the cloud service providers provide standard assurance levels for areas such as information security and compliance with specific laws and security standards. 3) Availability of cloud services as an assurance challenge Many online vendors who are using cloud services rely on the service provider to keep the e-commerce environment available. The impact of system unavailability in the cloud environment is serious, since it affects more than one vendor, as in the case of Google, where its cloud services became unavailable and multiple vendors were affected [25]. In order to create a trustworthy environment, users need to be aware that the website they are transacting from is a true vendor site which can be accessible on demand. The proposed cloud- based e-commerce assurance model checks the cloud environment servers, reports on the availability status and provides an aggregated rating, which takes into account other website attributes. A site that is constantly available will yield a positive rating, marked with a green status on the website. 4) Descriptive survey on laxities of assurance models This section discusses the weaknesses of some of the existing e- commerce assurance models and how they are perceived in terms of trustworthiness. Figures II (a) and (b) provide a summary of the findings of the research that was conducted by [8]. The research was conducted to determine whether seal- accredited websites were more trustworthy than unaccredited sites. A tool was used to check for policy compliance issues on the certified websites. The findings revealed that on the accredited websites, 4% of the sites were not trustworthy because of failure to meet certain requirements as expected by the seal accreditation bodies. The findings revealed that for the Truste accreditation [24] and the BBB online seal, not all the websites displaying the seals were in actual fact trustworthy. This finding raises some doubt in terms of the credibility of seal accreditation bodies because when an analysis was done of uncertified sites, only 3% of the sites were found untrustworthy, which is less than 4% of the certified sites that were found untrustworthy (refer to figure II (a) and (b). The challenge presented by these findings is that it raises the possibility that some untrustworthy sites are posing as trustworthy sites because they display web seals. Figure II. Do existing e-commerce assurance models require improved trustworthiness? These results highlight a need for more work to enhance the existing e-commerce assurance models, including the cloud- based e-commerce assurance models. III. PROPOSED COMPLIANT CLOUD-BASED ASSURANCE MODEL The proposed model consists of the following assurance measures: adaptive legislation, adaptive ISO standards, policies and advanced security login and website availability. The term “adaptive” is used show that the attribute is not fixed but rather flexible in such a way that it accommodates revised legislation or different legislation, provided it is specific to the e-commerce environment. Figure III presents the proposed cooperative assurance rating model based on page ranking and AHP techniques. The mathematical model deals with the level of e-commerce assurance based on PageRank modelling by [20] as an e-commerce assurance rating (EAR). Our EAR model is shown in equation (1). EAR (A) = (1-d) + (1) Where t1, . . . , tn are websites linking to website A, C is the number of outgoing links from a website (out degree) and d is a damping factor, usually set to 0.85 this value avoids accidental infinite series of websites linking infinite number of other sites. Since PageRank assigns a high score to a node, if it is pointed by highly ranked nodes, it is highly applicable in advancing website trustworthiness based on e-commerce assurance. The AHP [12] is a technique that is used in environments where complex decisions need to be made. For the purpose of this study AHP is used to reach clear decisions on the state of every attribute of the proposed assurance model. AHP is used to impart some structure to the unstructured assurance model attributes, such as policy, legislation and the ISO standard [21] and package these attributes in a manner that makes it feasible to make a decision on the website’s overall state of trustworthiness. The technique firstly aims to classify the attributes into distinct groups with the aim of constructing a hierarchy of attributes where the lower hierarchy is limited by the upper one specifically; pairwise comparison matrices are generated with respect to our attributes for first rating. One of the important decision steps in AHP is shown in equation (2). (2) Where Z=comparison matrix. The basic principle of our method is cooperative usage or parallel activation of the AHP and page ranking techniques in order to use the strength of the second to the benefit of the first. This provides an accurate rating and instils in cloud-based e-commerce environments. The cooperative assurance rating status is as follows: green is for a trustworthy site, yellow denotes a risky site and red a completely untrustworthy site; these indications guide the customer in transacting from a site or not doing it. in customers trust A) Uncertified websitesB) Certified websitesC) Attacks due to laxity on e-commerce assurance models))()((........))()((11tntnttcEARcEARd________________________nnnnijzzzzzZ....:....:1111AHP Page Ranking Figure III: Cooperative intelligent rating model for cloud-based e-commerce assurance The proposed model is aimed at providing useful information for decision-making to a customer concerning the website’s trustworthiness. In order to transact from a website, the proposed model (as shown in figure III) requires the creation of strong online login credentials. Thereafter, a customer provides input through a short survey, to determine if the customer has read the policies displayed and secondly if the customer has experienced a bad or good shopping encounter through the website in the past or not. The model aggregates the user’s input together with the following attributes: policies, legislation, ISO standard and website availability. The model checks the presence of the policies and the last policy review date to determine currency and provide a rating. Furthermore, to Electronic Communications and Transactions (ECT) legislation will be conducted to produce overall website rating. Adoption of the ISO standard, specifically the ISO 27001, will be confirmed by the model on transactions, specifically encryption. An online check to determine if the website is consistently available and has the latest anti-virus software and latest patch is conducted to ensure that it is not vulnerable to online attacks. The model combines all the attribute information using the cooperative rating based on AHP and page ranking to strengthen the assurance level. A trustworthy site flashes green and an untrustworthy one flashes red. The proposed model’s benefits are as follows: the security of online compliance a check for a) It is comprehensive and interactive in that it assesses compliance to legislation and also does online checking of technological compliance in terms of checking for the latest anti-virus software and website availability. b) It is interactive and visible through the colour display on the website. c) A dashboard (green, yellow, red) is provided as a result of a cooperative rating, which strengthens the assurance level. The detailed discussions of the model attributes are presented in sections A to E. One can see that the proposed model has more beneficial enhancements than the existing ones such as the trusted cloud data security certification and the KYPLEX model [14] A. Adaptive legislation as assurance measures In the trans-border cloud environment the application of laws gets very complex in the event of a privacy or security breach if the applicable laws have not been specified from the onset [11]. In terms of providing e-commerce assurance, laws have been found to be an assurance measure [13]. Different e-commerce laws that are specific can be used with the model. For the purposes of this study, South African legislation [9] will be used because of its specific provisions in terms of e-commerce environments. The ECT Act provisions will be used as measures for aggregation in the e-commerce environment, where the final rating will alert the customer if the website that is hosted in a cloud environment is safe to transact from or not. A testable null directional correctional hypothesis that is set for this attribute is: H : The level of adaptive legislation of a country is not positively associated with other assurance measures 10 In testing this proposition, explanatory research is required, as shown in table IV. B. Adaptive ISO security standard as an assurance measure standard international benchmarking According to [10], the ISO 27002 standard is regarded as the e-commerce for information security, which is thus suitable for inclusion in the cloud-based environment. As a result, sections 10.9.1 to 10.9.3 of ISO 27001 have been identified as vital for inclusion in the proposed model. The model checks for the security of transactions in an online environment by checking for the encryption of transactions on the website. This check, together with the other attributes, will be assessed and aggregated in order to show the final website assurance rating. A testable null directional correctional hypothesis that is set for this attribute is: H : The level of adaptive ISO standards and other assurance measures is negatively associated In testing this proposition, explanatory research is required, as shown in table IV. C. Policies as an assurance measure Policies have often been used by various websites to provide assurance to prospective customers concerning privacy of information and other related practices. In terms of the South African ECT Act, certain policies are required by law to be displayed on vendor websites. These include policies such as privacy and refund policies, which must appear on the online vendor’s site. Policies in a cloud environment are critical so that consumers can know and understand how their personal information will be handled in terms of privacy and to know which laws will apply in the event of a breach. Specifying the laws applicable to policy statements will be an improvement on existing policy models. A testable null directional causal hypothesis that is set for this attribute is: H : The level of policies of a business enterprise does not positively influence other assurance measures. In testing this proposition, explanatory research is required, as shown in table IV. D. Advanced security features as an assurance measure security in a Information cloud-based e-commerce environment is crucial in order to gain and maintain online customer trust. Cloud service providers need to provide leading edge security and auditing capabilities to keep up with meeting the customer’s assurance needs. Customers need to feel secure from the login phase to check out. A testable null directional causal hypothesis that is set for this attribute is: H : The level of strength of advanced user security does not influence other assurance measures In testing this proposition, explanatory research is required, as shown in table IV. E. Site availability as an assurance measure Website availability is crucial in the cloud-based environment. Service disruptions ought to be minimal for customers to gain and maintain trust in an online vendor store. A testable null directional correctional hypothesis that is set for this attribute is: H : The level of e-commerce site availability and other assurance measures is negatively associated In testing this proposition, explanatory research is required, as shown in table IV. IV. DESCRIPTIVE AND DEPLOYMENT ANALYSIS A. Descriptive analysis and results Table IV contains statistical data, which were arrived at by conducting a survey of journals from the IEEE, Science Direct databases based on the criteria of whether they were an E- commerce transacting site or not. The survey was conducted to determine if the following attributes had been identified as assurance measures in any of the sampled journals and e- commerce websites: adaptive ISO standards, policy, availability and advanced security logon. The sampling frame is October 2012 to March 2013 and the journals were sampled based on their relevance to the subject of this article where specific keywords were used. The main aim was to determine the number of articles in support of or against the proposed attributes as assurance measures. legislation, adaptive Table IV Sampled dataset from journal articles and real life data from e-commerce sites 20304050Keys:db1=IEEE database,db2=Science direct database,wrt=with respect to,Y=Yes,UND=Undecided. The Pearson’s correlation coefficient (R) in [16] was implemented and confirmed with excel macro to compute the relationships in Table IV. Any two attributes are chosen at random to test for possible relationships as shown below: 1) H1: The level of adaptive legislation of a country is not positively associated with other assurance measures FINDINGS: The result of the macro implemented implies: Corr(Legislation: ISO) = 0.2; DECISION: Since the correlation result> 0,Corr (Legislation: ISO) = 0.2; The above proposition is rejected, which implies that the level of adaptive legislation of a country is positively associated with any other assurance measure 2) H2: The level of adaptive ISO standards and other assurance measures is negatively associated FINDINGS: The result of the macro implemented implies: Corr(ISO: Availability) = 0.1769; DECISION: Since the correlation result > 0, the above proposition is rejected, which implies that the level of adaptive ISO standards and other assurance measures is positively associated. 3) H3: The level of policy of a business enterprise does not positively influence other assurance measures FINDINGS: The result of the macro implemented implies: Corr(Policies: Availability) = 0.12; DECISION: Since the correlation result > 0 suggests the rejection of the above proposition. This implies that the level of policy of a business enterprise does positively influence other assurance measures. 4) H4: The level of strength of advanced user security does not influence other assurance measures FINDINGS: The result of the macro implemented implies: Corr(Security : Availability) = 0.0353; DECISION: Since the correlation result > 0 suggests the rejection of the above proposition. This suffices to prove that the level of strength of advanced user security does influence other assurance measures. The correlational graph that emerged from these direct or indirect interrelationships is shown in figure IV (a). Figure IV(a): Emerged correlational graph of the assurance measures The results above suggest necessary or supporting conditions to say that the assurance measures could serve as the building blocks of the intelligent model in figure III and are compliant for accessing the trustworthiness of cloud-based e-commerce sites. B. Deployment scenario of the cloud-based assurance model 1) Scenario 1 –Untrustworthy e-commerce website As shown in figure IV (b) Joyce is considering making a book purchase through a cloud-based e-commerce website. She has created an online account. Following logging in, Joyce is asked through a short survey whether she has seen the policies (privacy and refund) on the website and she responds positively. Joyce had no previous shopping experience through the website and the system does not ask questions concerning her last shopping experience. Unfortunately, soon thereafter, a red button starts flashing at the bottom right-hand corner of the screen, signalling to her that the website is untrustworthy. Joyce clicks to read more information and it is highlighted that the website has been unavailable for an extended period of time and is not compliant with cloud-based e-commerce legislation. As a result, Joyce abandons the sale and signs out because the website is not trustworthy. In a nutshell the model interacts with Joyce through a short survey on the attributes whilst the model simultaneously carries out background checks on the site based on the assurance attributes to come up with an overall website rating. The proposed model could be used to address this scenario. Figure IV (b). Red website rating V. CONCLUSION E-commerce assurance is an area that has not been thoroughly researched, more especially cloud-based e-commerce assurance. As a result, very few journals discuss cloud-based e-commerce assurance models and propose robust models. In this research work, the focus was on examining the existing e-commerce assurance models and placing particular focus on cloud-based assurance with the aim of identifying gaps and addressing them by proposing a cloud-based e-commerce assurance model. The proposed cloud-based assurance model consists of the following assurance attributes: policy, adaptive legislation, adaptive ISO standards and advanced user security login. The AHP and page ranking techniques are used to achieve cooperative ranking of the attributes, which is displayed on the website for customer guidance regarding the website’s trustworthiness, as shown in figure IV(b).The major improvement in the proposed model is to have an intelligent cloud-based assurance rating, which the existing e-commerce assurance models do not have. The proposed cloud-based e-commerce assurance model can be used by online customers, vendors, cloud service providers and also law enforcers. ",Scientific Journal,0
WHO cancer agency under fire for withholding ‚Äòcarcinogenic glyphosate‚Äô documents,"WHO cancer agency under fire for withholding ‚Äòcarcinogenic glyphosate‚Äô documents Published time: 27 Oct, 2016 01:32 Get short URL ¬© Philippe Huguen / AFP The International Agency for Research on Cancer (IARC), facing criticism over its classification of carcinogens, has reportedly been advising its scientific experts not to publish internal research data on its 2015 report on ‚Äúprobably carcinogenic‚Äù glyphosate. 
The IARC urged its scientists not to publish research documents on its 2015 weedkiller glyphosate review, according to Reuters. The agency told Reuters on Tuesday that it tried to protect the study from ‚Äúexternal interference,‚Äù as well as protect its intellectual rights, since it was ‚Äúthe sole owner of such materials.‚Äù 
The scientists had been asked earlier to release all the documentation on the 2015 report under US freedom of information laws. 
The groundbreaking review, published in March 2015 by the IARC ‚Äì a semi-autonomous agency of the World Health Organization (WHO) ‚Äì labeled the glyphosate herbicide as ‚Äúprobably carcinogenic to humans.‚Äù Glyphosate is a key ingredient of Monsanto‚Äôs flagship weedkiller well-known under the trade name ‚ÄòRoundup.' It is one of the most heavily used herbicides in the world and is designed to go along with genetically-modified ‚ÄúRoundup Ready‚Äù crops, also produced by Monsanto. Read more EU may ban Monsanto weedkiller over health concerns 
The IARC‚Äôs report caused problems for both the notorious agrochemical giant and the agency itself. 
The report sparked a heated debate around the use of Roundup, and caused several EU countries ‚Äì including France, Sweden, and the Netherlands ‚Äì to object to the renewal of the glyphosate‚Äôs EU license. The vote on prolonging the glyphosate license for 15 years failed several times in June 2016, but the license was temporarily extended for 18 months during last hours before its expiration. 
The controversial report has seemingly made the IARC a target for attacks from multiple directions, and raised scientific, legal, and financial questions. 
Various critics, including those in the chemical industry, said the IARC's evaluations are fuel for ‚Äúunnecessary health scares,‚Äù since the IARC allegedly studies the potentially harmful substance itself, and not a ‚Äútypical human‚Äù exposure to it. It remained unclear whether the critics urged a WHO body to test the potentially carcinogenic chemical on humans. 
The critics also brought up other controversial statements from the IARC, over whether such things as mobile phones, coffee, red meat, and processed meat could cause cancer. 
The agency defended its methods as scientifically sound and ‚Äúwidely respected for their scientific rigor, standardized and transparent process and...freedom from conflicts of interest.‚Äù Numerous freedom of information requests by the Energy & Environment Legal Institute (E&E Legal), a US conservative advocacy group, have since been turned down with this reasoning. Read more Bayer vows not to use reputation to impose Monsanto‚Äôs GM crops on Europe 
E&E Legal told Reuters that it is pushing a legal challenge over whether the documents in question belong to the IARC or to the US federal and state institutions where some of the experts work. Basically, it‚Äôs being decided whether the IARC, as part of the WHO, is truly independent and free from ‚Äúconflicts of interest.‚Äù 
According to Reuters, officials from the US National Institutes of Health (NIH) will be questioned by a congressional committee about why American taxpayers fund the cancer agency, which faces much criticism over its allegedly faulty classification of carcinogens. 
‚ÄúIARC‚Äôs standards and determinations for classifying substances as carcinogenic, and therefore cancer-causing, appear inconsistent with other scientific research, and have generated much controversy and alarm,‚Äù a letter from US Oversight Committee Chairman Jason Chaffetz to NIH director Francis Collins states, as quoted by Reuters. 
The Oversight Committee demanded a full disclosure of NIH funding of the IARC, and even money spent in relation to the cancer agency‚Äôs activities. 
READ MORE: Conflict of interest? Members of UN panel on glyphosate have Monsanto ties 
IARC opponents from scientific circles vowed to provide their data on the matter. The European Food Safety Authority (EFSA), which believes glyphosate is ‚Äúunlikely pose a carcinogenic hazard to humans,‚Äù promised to release its raw data on the subject as part of its ‚Äúcommitment to open risk assessment.‚Äù The food safety watchdog made this statement in late September, and still has to deliver the promised information.",rt.com,1
News: Democracy Win: Volunteers Across The Country Are Oiling Up The Sidewalks To Help Voters Slide Uncontrollably To Their Polling Place,"Email 
Despite the fact that voting is the cornerstone of our democracy, only 60 percent of eligible voters turn out in a presidential election. But this year we may see that number spike, because groups of incredible volunteers are doing everything they can to get out the vote: They‚Äôre hitting the road and oiling up the sidewalks to help voters slide uncontrollably to their polling places. 
Democracy FTW! 
You might not know their names, but these hardworking men and women are out there in the trenches from Maine to California, slathering thick, black oil on the ground so every American can skid down the road, windmilling their arms wildly to try and stay upright, and careen straight through the doors of a voting booth. Republican or Democrat, these patriotic volunteers want every American to have access to a frictionless slick of oil that leads right to a polling place. 
Yes! These incredible volunteers are absolutely killing it! 
Rather than letting our election be decided by a select few, people from across the nation are volunteering their time to make sidewalks well-lubricated and slippery for as many voters as possible. As of this morning, footpaths have been oiled in all 50 states, giving people who would not otherwise have the time or desire to vote the opportunity to lose their balance and slide forward at alarming speeds with their arms flailing and legs kicking, until they crash face-first into a registered polling location. 
We might live in a democracy, but we wouldn‚Äôt have a truly representative election without the help of a select few everyday heroes. So when the polling stations close and every last slippery, oil-covered voter has put their ballot in the box and slid back out the door, remember those brave volunteers, because we couldn‚Äôt do it without them.",clickhole.com,1
Fecal Pollution Taints Water at Melbourne‚Äôs Beaches After Storm,"SYDNEY, Australia  ‚Äî   The annual beach pilgrimage during the height of summer in Melbourne, Australia‚Äôs   city, is threatened by an unsettling phenomenon: shores where the tides are tainted with excrement. The Environment Protection Authority in the state of Victoria said on Monday that heavy rains had caused fecal pollution to wash into Port Phillip from rivers, creeks and drains. It advised against swimming at 21 beaches because of poor water quality. ‚ÄúIt‚Äôs poo in all its luxurious forms that is causing the problem,‚Äù said Anthony Boxshall, the agency‚Äôs manager of applied sciences, noting that the waste was coming from people, dogs, horses, cows, birds and other animals. Fecal pollution can cause serious health problems, including gastroenteritis. Mr. Boxshall said much of the waste had been washed down the   Yarra River that runs through Melbourne into Port Phillip, affecting the city‚Äôs bayside beaches the most. The agency, which takes regular water samples, rates beaches. A ‚Äúgood‚Äù rating means that the water is suitable for swimming. ‚ÄúFair‚Äù means that rainfall has affected the water. ‚ÄúPoor‚Äù means people should avoid it. Residents said that the pollution had deterred them from indulging in a favorite summer ritual. ‚ÄúWhen the temperature gets above 86 Fahrenheit, Melbournians typically pack the family in the car with food and drink and spend the day at the beach,‚Äù said Sam Riley, who lives in the city. ‚ÄúI was going to take my two young boys to the beach myself over the summer, but now I‚Äôm concerned about whether the water is clean. ‚Äù Mr. Boxshall said any improvement in the beaches‚Äô water quality was uncertain as long as the rain continued. The agency says it usually takes between 24 and 48 hours for the waters to clear after the rain stops.",New York Times,0
Mike Pence Is Fine With Trump Sexual Assault But Offended By Voter Suppression,"By Sarah Jones on Fri, Oct 28th, 2016 at 12:08 pm Donald Trump's Republican running mate Mike Pence has finally found something he's offended by, but it's not Donald Trump bragging about sexual assault or insulting African Americans or Mexicans or women or a Gold Star family. Nope. Pence is offended by a news report he hasn't read about their voter suppression efforts. Share on Twitter Print This Post 
‚ÄúThat‚Äôs offensive to me, that kind of language. It‚Äôs not our operation,‚Äù Indiana Governor Mike Pence said Friday morning on MSNBC‚Äôs Morning Joe . 
Yes, Donald Trump‚Äôs Republican running mate Mike Pence has finally found something he‚Äôs offended by, but it‚Äôs not Donald Trump bragging about sexual assault or insulting African Americans or Mexicans or women or a Gold Star family. 
Nope. 
Did Pence read the story? No. But he‚Äôs offended. So offended. Because that kind of language is offensive. Not the actual suppression of the votes, but the language, because it‚Äôs not ‚Äúour operation.‚Äù 
Watch here via MSNBC‚Äôs Morning Joe : 
Twice saying he hasn‚Äôt read the article, Pence proceeded to launch into a diatribe about how offended he was by something he hadn‚Äôt read, ‚ÄúThat‚Äôs offensive to me, that kind of language. It‚Äôs not our operation,‚Äù Pence denied. 
‚ÄúDonald Trump and I want every American who has the opportunity to vote to vote in this election. And that‚Äôs our message, is to tell the American people that this country really belongs to them. That we can have government as good as our people again, but it‚Äôs going to take all of us.‚Äù 
‚ÄúAnd you saw Donald Trump ‚Ä¶ say that people who haven‚Äôt traditionally voted Republican, we‚Äôve got an agenda to bring our cities back‚Ä¶‚Äù 
Pence is twice offended, so super offended: 
‚ÄúI‚Äôve never heard anybody in this campaign talk that way. Frankly, you know, it was offensive to me to hear that being reported in the news because that‚Äôs just not the approach Donald Trump has taken to this campaign. It‚Äôs not the approach we‚Äôre taking. We‚Äôre reaching out to every American.‚Äù 
Pence then claimed Independents and even Democrats are breaking for Trump. In fact Trump and Pence are even losing groups that voted for Mitt Romney in 2012. Pence then belied his own confidence by trying to shame Republicans into voting for them by saying, ‚ÄúIt‚Äôs time for Republicans to come home‚Äù because Trump won the primary. 
Mike Pence laughed at the idea that the race is over and done, because ‚ÄúIt‚Äôs just not what I see out there.‚Äù 
Trump also loves to cite what he ‚Äúsees‚Äù and reads, and often times those things turn out to not even exist ‚Äì like videos he claims to have watched. 
This explains why Mike Pence and Donald Trump ended up together. It turns out, they are not that different after all, in spite of the Republican establishment‚Äôs efforts to persuade the voters and themselves otherwise. Both Pence and Trump don‚Äôt care a whit about facts. This is more than typical campaign spin, it‚Äôs appalling in context. 
The context is that Mike Pence is willingly standing next to a man who brags about grabbing a woman‚Äôs ‚Äúp*ssy‚Äù without her permission, but he is publicly saying he‚Äôs offended by language used in a report about their voter suppression efforts. 
Pence didn‚Äôt even bother to read the article, which suggests that he doesn‚Äôt care if their campaign is really suppressing voters. But why would he? 
Pence didn‚Äôt care about any of the groups Donald Trump insulted ‚Äì not enough to take a public stand, so why start now. But he took a public stand was when the insult was aimed at his own campaign. 
Like Trump, Pence is also insulted by language calling out bad behavior instead of the bad behavior. 
Image: Screencap via MSNBC‚Äôs ‚ÄúMorning Joe‚Äù 
Mike Pence Is Fine With Trump Sexual Assault But Offended By Voter Suppression added by Sarah Jones on Fri, Oct 28th, 2016",politicususa.com,1
Trump Thug Chants ‚ÄúJews S A‚Äù at Arizona Rally,"By Adalia Woodbury on Sun, Oct 30th, 2016 at 1:43 pm A Trump supporter sporting a ‚ÄúHillary for Prison‚Äù t-shirt chanted ‚ÄúJews S-A‚Äù in support of his candidate during a rally in Arizona on Saturday. The target was the pen of reporters who have been fodder for Trump‚Äôs rhetoric at every rally. Share on Twitter Print This Post 
A Trump supporter sporting a ‚ÄúHillary for Prison‚Äù t-shirt chanted ‚ÄúJews S-A‚Äù in support of his candidate during a rally in Arizona on Saturday. 
The target was the pen of reporters who have been fodder for Trump‚Äôs rhetoric at every rally. 
The context: While the crowd was chanting ‚ÄúUSA‚Äù one man chanted ‚ÄúJews S-A‚Äù. Words like that can‚Äôt be spun or read into. The intent and meaning were obvious to everyone. 
Watch here on video obtained by The Huffington Post . 
This sort of thing comes as no surprise considering that Donald Trump spent the last year spewing hate filled venom in every possible direction. It may seem superfluous to restate every category of people for which Trump has shown contempt, but it is not. 
Muslims, Jews, immigrants, labor, taxpayers, Latinos, Hispanics, Mexicans, African-Americans, women, POW‚Äôs, veterans, people with disabilities, Gold Star Families, and the military have felt the sting of Trump‚Äôs words and the disdain in his heart. 
Throughout this campaign season, the vitriol against Hillary Clinton saw no limit. Signs and T-shirts too disgusting to quote in this article. Chants of ‚Äúlock her up‚Äù at the Republican convention and since. Trump promised his base of thugs, lowlifes, and haters he would weaken the first amendment, strengthen the second and establish the alt-right utopia. 
Hillary Clinton was already the most qualified candidate in this race. She also proved to have more stamina than any previous candidate needed. Not only was she competing against a vicious and pathological liar, she was competing for who we are and who we could be. For all the Trump claims that the media is rigged in Clinton‚Äôs favor, it was Trump who got free advertising, who framed the narrative and whose words were the primary ‚Äúnews‚Äù stories of the campaign. 
There is no doubt that Hillary Clinton will keep her promise to fight for us. She has been doing it from the beginning of this campaign. Every time Hillary Clinton called out Donald Trump‚Äôs sexist comments, she was fighting for all women who were attacked, humiliated and sexually abused by men like Trump. Every time Clinton shamed Trump‚Äôs horrific and disgusting comments about the Khan family she was standing up for every Gold Star family. She was also standing up for every Muslim-American. The same is true every time Clinton condemned Trump for mocking a disabled reporter or resorted to alt-right stereotypes about African-Americans, inner-cities, and African-American communities. 
Every time Trump spewed hate in whatever direction, Hillary Clinton defended the target. 
So no, this isn‚Äôt about choosing the lesser of two evils. Like every election, this is about choosing our next president. More importantly, it‚Äôs about voting for who we are and who we aspire to be. 
Image: Screengrab from video.",politicususa.com,1
Guidelines for the creation of brain-compatible cyber security educational material in Moodle,"INTRODUCTION Most current approaches towards information security education fail to pay sufficient attention to pedagogical theory [1]. The creation, distribution and manipulation of information is integrated throughout all aspects of modern society. Users; members of the general public, businesses and other facilities; require access to information in order to successfully conduct various daily in a personal and/or professional capacity. Information is a valuable resource and any loss, unauthorized alteration, or corruption of it can have serious implications for all the users who rely on the information. It is therefore imperative that the protection of the confidentiality, integrity and availability of information as a vital resource is given appropriate recognition and attention before a problem can occur. transactions, both Security is a multi-faceted problem, the comprehensive to which will normally encompass physical, solution procedural and logical forms of protection [2]. From an organizational perspective, many international standards, such as ISO/IEC 27002 deal with the protection of this resource. For people in their personal capacity, there are currently no specific information security standards available. However, it could be argued that the protection of information in either a personal or organizational context would have to be based on the same general principles. Standards such as ISO/IEC 27002 suggest the use of various information security controls as countermeasures to avoid, counteract or minimize security risks faced by informational suggested controls are categorically defined as physical, technical or operational in nature. Operational administrative, managerial, and procedural and various other types that relate to the role(s) humans play in the information security process. resources. These controls include Physical controls provide a physical layer of protection. For example, a lock on a computer or server room’s door could be described as a physical control. Technical controls would consist of technological countermeasures. For example, requiring users to authenticate themselves by logging into a system before being allowed to access the resources could be described as a technological control. Operational controls are controls which address the role(s) of humans. An example of such a control could thus be a policy statement requiring users to lock their office door when not in the office, or to logout of the system when they leave their computers. Both the physical and technical controls would therefore depend on the humans involved in the processes for their effectiveness. People involved in the information security process thus play a vital role in the effectiveness of the information security process and must therefore be educated about their necessary role(s) and responsibilities. One of the most important pillars on which any serious effort towards information and cyber security is based, is the level of awareness (knowledge) that those responsible for managing the effort have. Therefore the issue of proper awareness, training and education of the involved people is of paramount importance. Most standards recommend that it is the lack of knowledge; regarding the roles people play, the responsibilities people and threats that exist in relation to the information; which must be addressed via information security awareness, education and training programs. However, these programs are often constructed by security specialists who are not necessarily educationalists and might to adequately educate the users involved. thus fail This paper will focus on pre-emptive education as a way to control information security risk. Furnell, Gennatou and Dowland [2] state that in businesses the inclusion of security- related issues is an integral part of any organizational training strategy and mechanisms to promote awareness during day-to- day activities. This extends to the members of the general public. The education of “average”, daily users; members of the public; about information security and its related issues is vital to protect the information used in their daily lives. The educational material should be based on a set of sound information security principles. This paper will critically evaluate a general information security education program aimed at end users at the Nelson Mandela Metropolitan University (NMMU). It will then recommend guidelines for a more pedagogically appropriate presentation method of the subject-matter based on brain compatible learning theory principle and techniques. The paper takes the form of a case- study. The next section will examine current approaches towards information security education in order to identify problems in these approaches. This will be followed by a brief overview of brain compatible learning. The paper will then present a case study demonstrating how brain compatible learning principles can be incorporated into an existing information security educational program. II. CURRENT APPROACHES TOWARDS INFORMATION SECURITY EDUCATION Alnatheer and Nelson [3] state that security training and awareness programs are fundamental components of effective information security. Although this statement was made in the context of an organizational environment, it’s extendable to the general public. The purpose of all these programs is to educate a certain target user group regarding information security. The scope of the subject’s content matter and the extent to which the program aims to educate the user is vastly dependent on the “level” of the educational program. Security educational programs usually aim at one of three “levels” of education, namely; security awareness, training, or education [4]. The complete and comprehensive education of the users in cyber security involves a continuum of three levels of education. Each level advances the knowledge gained from the previous level and ultimately a comprehensive knowledge base of understanding is developed for and taught to the user, answering questions such as “what”, “how” and “why” [4, 5]. These levels correspond to different types of learning, mostly delineated by differences in the degree of comprehension and detail, both of which increase as we move from awareness towards education. The awareness level consists of a set of awareness activities. As explained by Katsikas[5] and the NIST Special Publication 800-16[4], awareness level activities are not a form of informal informative training, they are simply activities Institute the National which aim at attracting the attention of individuals to the subject. The targeted audiences (the general users i.e. the members of the public) are mostly passive recipients of information and the knowledge gained through them tends to be short-term, immediate and specific, unless the activity is repeatedly exercised. This level answers the “what” question. Examples of possible presentation formats are posters, flyers, video tapes and promotional trinkets branded by motivational slogans. For the content of the information security awareness level NIST; for Standards and Technology offers a full array of customizable awareness materials and resources for utilization. NIST [4, p. 15] states that “Effective IT security awareness presentations must be designed with the recognition that people tend to practice a disassociation (tuning-out) process called acclimation”. As a result of this in order to prevent the target learners(members of the general public) from ignoring the awareness campaigns materials, the presentations must be on-going, creative, and motivational, with the objective of focusing the learner’s attention so that the learning will be incorporated into conscious decision-making. The purpose of this level is to enable the user to recognise cyber security threats. The training level is more formalised and aims to develop the user’s knowledge and skills, so as to allow them to take appropriate action against the identified threats. The training level of the learning continuum aims at building knowledge, thereby producing relevant and needed security skills [4,5]. This level involves the development of high-level concepts and skills and therefore its execution and completion period is usually more extended than that of the awareness level [4,5]. The knowledge taught throughout the training process teaches the user how to solving problems by using the skills which are developed by the training process. This level answers the question of “how”. Unfortunately an innate flaw of training as discussed by Katsikas [5] is that because training activities are directed towards exposing trainees to available knowledge and techniques for problem- solving, knowledge gained through them tends to be long term, but quickly becomes obsolete and therefore its impact time frame is only that of an intermediate timespan. Training must take into consideration that not all trainees require the same level of training. This is because the baseline security requirement for different individuals varies according to each user’s personal requirements, experience and purpose for undergoing the training. “The level of training one receives in any particular training area, according to the individual’s background and already acquired skills. This may be covered by assigning a degree of difficulty (or insight) to each course, ranging from beginning, through intermediate to advance” [5]. Examples of training formats are lectures, interactive demos, case studies and self-practice.  As explained by Katsikas and NIST [4,5] the education level of the learning process aims at creating expertise necessary for information systems security specialists and professionals. This level integrates all of the security skills, theory and knowledge taught by the previous levels into a single body of knowledge and it expands upon it to form a complete and comprehensive understanding of the entire subject [4,5]. A less technically intensive format than explained by NIST for professionals could be adapted for general public users. The objective of the educational level is to develop the users’ understanding of what has been taught on the previous levels, it answers the “why” question. The impact timeframe of this level is long-term. Examples of formats which the educational levels presentations can be are seminars, discussions, research and reading.   A. Problems with current security education  Many existing information and cyber security education courses are not completely successful. This is a result of several issues surrounding the courses, their presentation formats, and their content. One of the first problems is that people simply are not aware about their own need to take such an educational course. If people are not aware of the reason for the educational course, they will not make an effort to understand or learn from the course. The second problem relates to the subject matter itself, although the technical controls themselves exist, the operational procedures related to these controls often do not exist and therefore educational material relating to them is either non-existent or too vague to matter. This problem is extended by the third problem, which is that even if there are operational guidelines available for utilization in a business environment, there are normally no operational guidelines in existence for the general public, the source of the businesses work force. The other problems are mainly people (user) oriented or related. One of the major problems is that people are not motivated to learn about information security. In an organizational context, Valentine [6] supports this by explaining that many employees, specifically those that do not interact with sensitive information, do not care about information security or social engineering. This is true in the public domain as well. The next problem is that people tend to forget what they have learnt in the educational course and therefore it results in the attendance and completion of the course being useless as the person has not truly learnt anything. Why people forget what they have learnt shall now be expanded upon. There are numerous reasons for people forgetting what they have learnt, however these reasons may be categorized to relate to the course material, the course presentation or the learners themselves. The first reason could be that because the course material is set by security experts who do not necessarily have educational experience, and as such the material may be presented in a manner which is difficult to learn .This point is supported by research from Puhakainen [1]. Puhakainen states that most current information security educational approaches lack a sound theoretical basis. Goucher [7] states that security awareness training for the non-IT staff is often best delivered by security specialists who are not IT focused themselves. Conversely material set by educators may not be comprehensive enough to cover the subject; and although it would follow pedagogical theory no useful knowledge would be gained by the learner. Another problem could be that; as highlighted by Schultz [8]; the security training and awareness effort could simply be subpar; appearing as a figurative compliance to a standard of providing education and training; instead of being of a suitable standard to provide practical and useful education about needed skills and knowledge which could aid in an attendees. Schultz explained this in a business context, but the concept can be extended to carry over into the public domain teach information and cyber security knowledge and skills need in member of the public’s daily life[8]. these courses should to say that The next reason is people may forget what they have learnt; this relates back to the actual development if the course material. Many training and education programmes do not consider the learners entry knowledge and skills, this is explained by Schultz and NIST. NIST specifically states that “material developers and trainers should consider the likely education and experience of their target audience and adjust their presentation approach and content accordingly.” However many materials developers fail to take this into account and therefore many traditional employee security awareness programs utilize a very “one-size-fits-all” approach [6,8]. This ubiquitous approach is quickly becoming obsolete [6] and therefore leaves many attendees puzzled and many others bored, disappointed, and even hostile because they have learned nothing new [8].  The next possible reason is that course content is not interesting and therefore the learners could simply fail to pay attention to the lessons and therefore fail to benefit from what is being taught. Finally, regardless of the interest level in the course content, people may simply fail to remember what they have learnt because the course material may not have been presented in a memorable manner and therefore the course content may fail to make an impression with the learner. This reason can be supported by the fact that different people have different learning styles. This argument is supported by NIST which states “Individuals learn in several ways but each person, as part of their personality, has a preferred or primary learning style. Instruction can positively or negatively affect a student’s performance, depending on whether is matched, or mismatched, with a student’s preferred learning style”[4]. NIST then elaborates on the types of learning styles people may prefer and how they may be applied in a course. The learning styles mentioned are visual, auditory and kinaesthetic- tactile in nature [4]. it In order to maximize the impact of such educational programs it is essential to not only focus on the content of these programs, like many security specialists do, but to also focus on the delivery of the content. One promising pedagogical approach which could improve the retention of knowledge by the “learners” in an information security program is brain- compatible learning. The next section will briefly introduce this pedagogical approach. III. WHAT IS BRAIN-COMPATIBLE LEARNING? A. How the Brain Learns  When presented with the concept of the learning process, one automatically associates the process with the mind and the brain. Therefore a brief explanation about the brain is necessary, but an in-depth discussion is beyond the scope of this paper.  The brain is divided into two linked hemispheres. The left hemisphere of the brain handles activities such as language processing, while the right often deals with music and spatial awareness. Whatever we are doing, however, both sides of the brain are nearly always involved in some way, and information flows back and forth in a continual dialogue [9]. According to Caine and Caine [10] the physical structure of the brain changes as the result of experience, this is called “brain plasticity”. This means that as the brain is stimulated the brain will grow physiologically. According to Scoffham “learning and cognition appear to consist in establishing patterns between brain cells” [9], Scoffham explains this concept further. Modern scientists believe that the cortex is organized into several hundred million neural networks or modules [9]. These modules extend vertically through the cortex in small columns, which are then linked up into more complex structures. New experiences physically change the brain by causing neurons, the brain cells principally involved in cognition, to sprout new branches, or dendrites, and thus increase communication among neurons across microscopic gaps called synapses. The synaptic leap of an electrical impulse between the axon of one neuron and the dendrite of another is the physical basis of learning and memory. Once the cellular pathway has “fired” repeatedly a chemical change occurs which makes it more likely to trigger in future and become increasingly efficient. This is called ‘Hebbian learning’, and is the physical goal of brain compatible learning techniques.  B. Defining Brain-Compatible Learning  Leslie Hart defined the term “brain-compatible” as the education designed to match “settings and instruction to the nature of the brain, rather than trying to force (the brain) to comply with arrangements established with virtually no concern for what this organ is or how it works best.”[11] Jensen defined brain-compatible (brain-based) education as the ""engagement of strategies based on principles derived from an understanding of the brain."" Therefore we can safely devise that brain-compatible learning is learning based on brain- compatible education principles, methods and techniques which endeavor to teach subject matter in a manner and format which is naturally complimentary to the brains physical and psychological processing functions.  In the context of a classroom environment Rogers and Renard [12] explain that brain-compatible, research-supported teaching techniques allow students to move around the classroom, address multiple modes of learning, acknowledge outlets for creative presentation of learning, provide enough contrast to preclude boredom, and contribute to a motivating context. It also allows teachers to employ summarization; provide anticipatory and closure strategies; and ensure a tight alignment among curriculum, instruction, and assessment. This means that brain-based education uses evidence from all disciplines to enhance the brains of students [13]. Brain-compatible learning has several principles as a general theoretical foundation [10]. The principles are simple and neurologically sound. Applied to education the brain compatible principles guide educators in the defining and of selecting and methodologies [10]. Some of these principles will now be discussed. educational appropriate programs the need One of the most important principles of brain-compatible learning is that “Emotion is the gatekeeper to learning”[14]. Everything learners “learn is influenced and organized by emotions and mindsets involving expectancy, personal biases and prejudices, self-esteem and for social interaction”[10]. A learner’s emotional state is resultant of their body’s chemical messaging system. This state affects their attention and ability to focus and therefore it affects their ability to learn. Ornstein and Sobel, Lackoff, McGuinness and Pribram and Halgren et al. (cited by [10]) state that emotion and cognition cannot be separated. Implication of this principle in relation to cyber security education is therefore that the course content should be implemented and presented in a manner that provides a supportive environment, which is no- threatening learner and encourages cooperative approaches to learning.  the to Another brain compatible principle is that the brain stores most effectively what is meaningful from the learner’s perspective [14]. Caine and Caine [10] explain this as “patterning”. During the learning process, the brain attempts to distinguish and understand patterns which appear in what is being learnt or experienced. The brain naturally integrates and assimilates information, which may initially seem random and disconnected, provided that the initially isolated information is related to a particular student. Implications for the education of the learners are that the subjected material should be presented in a format which encourages problem solving and critical thinking.  to what makes sense Other brain-compatible education principles do exist, but a complete in-depth study and analysis of the brain-compatible principles is beyond the scope of this paper, however specific principles will be dealt with in more depth as part of the case study. IV. METHODOLOGY The remainder of this paper will take the form of a case study, as described by Cresswell [15] and follows the structure suggested by Creswell. This structure is as follows: Introduction _ Entry vignette _ _ Description of the case and its context _ Development of issues _ Detail about the selected issues _ Assertions _ Closing vignette V. INTRODUCING SEAT   A cyber security training and education programme which is presented in a memorable format and complies with sound (SEAT) programme at pedagogical theory is needed. The aim of this section is to perform a critical evaluation of the Security Education and Training the Nelson Mandela Metropolitan University (NMMU). It will then aim to propose a set of brain compatible learning guidelines for the creation of cyber security educational material which can be used to update and improve the current SEAT programme. A. Introduction In the first section of this paper it was established that information is important, and members of the general public need to be taught how to protect it. It was then further discussed and a need for cyber security training and education programme which is presented in a memorable format and complies with sound pedagogical theory; namely brain- compatible education theory; was established. An existing information security programme is the SEAT training and Education programme at Nelson Mandela Metropolitan University (NMMU), however this programme is not presented in a brain-compatible educational manner. The programme must be evaluated and brain-compatible guidelines should be proposed, so that the SEAT’s education material may be edited and updated to be presented in a brain-compatible manner so as to increase the success rate of the courses students. VI. THE SEAT DESCRIPTION A. The Overview SEAT is a security education and training course at the Nelson Mandela Metropolitan University (NMMU). Its objectives are as follows: 1. To improve the awareness of the need to protect system resources and an organizations end users. 2. To develop the skills and knowledge of computer their computer they may perform users so activities more securely. 3. To allow online access to a rich source of security related best practices. 4. To help end users understand why security is part of their responsibilities, and how they impact the security of the organizations they work for. The course material covers nine modules; each of which covers a specific topic. Each module has a corresponding quiz to test the learners understanding of the covered material; to progress through the course each chapters quiz must have been successfully completed and passed to allow progression to the next chapter. Once all the modules and quizzes have been completed, there is a final exam, covering all the material covered. The module topics are as follows: 1. Security in General – an introductory explanation of security and security related terminology Information Security – an explanation of the importance of security information and provides methods to secure said information 2. 3. Password Security - provides the learner with best practices for choosing secure passwords 4. Virus Awareness - provides information on various types of viruses, virus detection and prevention 5. Data Storage and Backup - provides best practices about how to secure data during storage 6. Computer Ethics – outlines appropriate behaviors for computer users 7. Office Discipline - outlines how system users should behave in the office environment 8. Hardware Security - addresses the protection of hardware devices 9. Social Engineering - covers Social Engineering and the techniques to combat such attacks  Currently each module lesson is presented in a single flash file presentation. Each topic’s presentation module covers several related sub-topics. The module’s flash slides content presentation consists of text and representative images. The text is presented mainly in bullet points and brief explanations, while the images are embedded throughout the presentation. The quizzes consist of multiple choice questions.  This course is currently running, however it has been noted that this courses material and presentation method is outdated and SEAT needs to be redeveloped. It is therefore a sensible solution to update the presentations to be presented in a more brain-compatible manner, so as to improve the success rate of learners who take the course. The section will outline the courses specific issues which will need to be improved, as part of the courses redevelopment. VII. DEVELOPMENT OF ISSUES   The SEAT course has three categories of issues which must be rectified during the courses re-development. This section will briefly outline them all, but it will focus on the category which relates to the presentation of the course content, as the other two categories fall beyond the scope of this paper. The first category of issues relates to the actual deployment and maintenance of the SEAT programme. The original SEAT program is a program which was developed in-house at the university a few years ago. Over time certain issues have been identified relating to the running and updating of the program. The first issue relating to its deployment and continued functioning is that the program has a problematic tendency to “crash” or cease functioning correctly. The other issues all relate to the maintenance and continual updating of the program and its content. The first maintenance issue is that the programme is generally difficult to maintain. The updating of the content the modification of code is impossible. The following issues detail why the first issue exists. One of the major issues is that the original developers of the SEAT platform are no longer in the employ of the university and they are therefore not available to time-consuming and unstructured, is the perform required updates and modifications. The programme development and maintenance instructions were poorly documented and no other developers have been able to sufficiently understand what was documented enough to risk modifying what was in existence. The modification itself has been deemed impossible as the SEAT program was developed using an older version of the development tools used by the university and the code is not compatible with the current toolsets. The second category of issues relates to the actual content which is covered in the SEAT course. The main content issue is that the course’s material is outdated. Although the scope is large and covers many aspects of information security, the course fails to cover many of the newer security threats that have come in to existence in that last decade and conversely still covers many security threats which have been mitigated by their progression into an obsolete existence. An example of the continued education about obsolete threats is the continued focus on floppy-disk media and an example of the threats which are not covered is the failure to highlight the threats from social networks, portable storage media such as flash drives, and the new portable technologies such as iPods and portable media players. The third category is the one that this paper is mainly focusing on; the presentation of the course. According to a brain-compatible education principle educational material should engage multiple sensory channels in order to have maximum impact for the rehearsal of new learning concepts. SEAT only has one mode of presentation, in the format of the modular flash files; consisting of text and image contents; which only has a visual impact and does not cater for all learners ideal learning style. The SEAT programme currently provides no feedback to the learner aside from mark earned at the end of the quiz and a display of the questions which were incorrectly answered. This should be rectified to comply with the principle which states that feedback is vital. The material being presented seldomly utilizes learner’s positive emotions to aid learning and recall. The content is presented in a factual manner, to which the student may not necessarily relate. A brain-compatible principle which is related to this issue states that all new material should be relatable back to old material which may have meaning to the learner. The material is only covered in two tasks. First the topic is taught using the single flash file presentation and then a short quiz is administered. This is an issue because there is a principle which suggests that material should be repetitively reviewed to solidify recall and recognition of the taught material. the The issues identified relate to seven selected brain- issues will be compatible education principles. These elaborated on in the next section. VIII. DETAIL ABOUT THE SELECTED ISSUES In   this section the aforementioned deployment and maintenance, content and presentation issues will be discussed. The first deployment and maintenance issue of the in-house deployment has been mitigated by the transference of the SEAT course material to Moodle. Moodle is an open and modular system of educational content distribution and user management [16]. It is a cost effective solution since the software Moodle is available from General Public License (GNU). All instability issues from the previous SEAT course with relation to stable deployment have been subsequently resolved. This transference to the Moodle platform also prevents the issues of the unavailable developers, the outdated and unsupported development tools and the lack of system development documentation since Moodle is an open-source fully documented project and all development issues are tracked by Moodle Tracker.  Valentine states “the traditional employee security awareness model provides a static solution for a fluid problem” [6]. He explains that information security is a constantly evolving field because of the continuous creation of new threats, techniques, countermeasures and philosophies and therefore any security awareness course can only provide an adequate explanation of information security as it currently exists if the curriculum is completely up to date. The course material covers old security risks and threats; such as floppy disks, and does not cover new security threats; such as social network threats, newer technology threats such as iPods, iPads, kindles and portable media players etc. As previously stated SEAT’s course material needs to be updated, however this is beyond the scope of this paper.  The other issues are all related to the presentation of the course and will be solved by the redevelopment of the course and the application of brain-compatible education principles. The first brain-compatible education principle states that the rehearsal of new learning concepts should involve a combination of multiple sensory channels. This principle is supported Scoffham [9] who states “rich experiences can",,
"promo""",Scientific Journal,0,
Woman awarded $70M after contracting cancer from Johnson & Johnson talcum powder,"Woman awarded $70M after contracting cancer from Johnson & Johnson talcum powder 
Wednesday, November 02, 2016 by: David Gutierrez, staff writer Tags: talc , ovarian cancer , jury award (NaturalNews) Once again, a St. Louis jury has ruled that Johnson & Johnson damaged women's health by engaging in a decades-long coverup about the potential risks of talcum powder (""baby powder"") as a feminine hygiene product. On October 27, Johnson & Johnson was ordered to pay $70.1 million to Deborah Giannecchini of Modesto, California, who received an ovarian cancer diagnosis in 2012.For 40 years, Giannecchini had used Johnson & Johnson's Baby Powder to keep her genital area dry, a use promoted by the company.The main ingredient in talcum powder is talc, a mineral widely used in paints and plastics as well as cosmetics, where it is used to absorb moisture. Some evidence suggests that regular exposure to talc, particularly in the genital area, can increase the risk of ovarian cancer.The jury found Johnson & Johnson guilty of negligence for failing to warn customers of this fact. Conspiracy to conceal risks Ovarian cancer is a rare but highly lethal disease. Well-established risk factors include obesity, not having children, estrogen therapy after menopause, and a family history of ovarian or breast cancer.The evidence linking talc to ovarian cancer is compelling but not yet conclusive. The International Agency for Research on Cancer lists talc as a ""possible"" carcinogen.Among the robust studies suggesting a connection are two meta-analyses that found a roughly one-third increase in ovarian cancer risk among women who were regularly exposed to talc . The first, published in 2003, found the connection in all cases. The second, published in 2013, found it only in women who applied talc directly to the genital area.The case marks the third guilty verdict against Johnson & Johnson over this issue. St. Louis juries have previously slapped the company with $55 million and $72 million judgments.The first case was filed by the family of Jackie Fox of Birmingham, Alabama, who had died of ovarian cancer after long-term use of talcum powder. In that case, the jury found the company guilty not just of negligence, but also of ""failure to warn and conspiracy to conceal the risks of its products.""Another 2,000 lawsuits are pending. Thirty years of deception Even after the recent verdict, Johnson & Johnson continues to insist on the safety of its product, including for genital use. In its home state of New Jersey, the company has successfully gotten two lawsuits over the issue dismissed. It is appealing all three guilty verdicts from Missouri.Investors seem to believe the company will prevail. Its stock price seemed unaffected by the recent guilty verdict.Alexandra Scranton, director of science and research at Women's Voices for the Earth, has characterized Johnson and Johnson's behavior as typical of Big Pharma and other companies that go to extreme lengths to keep selling products even as evidence mounts of their dangers.Scranton said that documents uncovered during the Fox trial show that for decades, Johnson & Johnson sought to take advantage of the scientific uncertainty over the talc-ovarian cancer link, downplaying the potential risk rather than pursing a ""clearly more ethical role, to take a precautionary approach."" Indeed, Scranton notes, the company ""poured money over years into defending talc.""Among the documents revealed during the Fox case are internal memos showing that Johnson & Johnson had been preparing to be sued over the health risks of talc for 30 years. In one 1997 memo, a medical consultant warned that anyone who continued to deny a connection between ovarian cancer and genital talc use would eventually be seen as on par with tobacco companies denying a cancer link: ""denying the obvious in the face of all evidence to the contrary.""Another memo laid out a strategy to counter falling talc sales caused by health concerns by more aggressive marketing to minority communities. Sources for this article include: http://www.naturalnews.com/053112_Johnson_&_talc_ovarian_cancer.html Health Ranger Approved AquaTru Water Filter Back in Stock 
I've secured 500 units of the amazing AquaTru at $100 off for Natural News readers (while supplies last). 
Breakthrough filter removes nearly 100% of hundreds of contaminants. No plumbing needed. SHIPS TODAY .",naturalnews.com,1
George Michael‚Äôs Freedom Video: An Oral History,"Pop music and fashion never met cuter than in George Michael‚Äôs ‚ÄúFreedom! ‚Äô90‚Äù video. Sure, other catwalk favorites starred in music videos during that era, and fashion photographers like Steven Meisel and Herb Ritts were recruited by stars such as Madonna, Chris Isaak and Janet Jackson to burnish their visuals. But the convergence of a   maker‚Äôs irresistible rhythms and lyrics, a group of models at the peak of their fame and a director on the verge of his own runaway success gave ‚ÄúFreedom! ‚Äô90‚Äù a jolt of stylish energy that has yet to fade after more than a  . Directed by David Fincher, who was at the start of his film career, and filmed over several days at Merton Park Studios in London, the video from Mr. Michael‚Äôs 1990 album, ‚ÄúListen Without Prejudice Vol. 1,‚Äù brought together five models  ‚Äî   Naomi Campbell, Linda Evangelista, Christy Turlington, Cindy Crawford and Tatjana Patitz  ‚Äî   who were then the queens of fashion. It runs for six and a half minutes, and its production also gathered a team of novice professionals who would go on to become major forces in fashion, including the stylist Camilla Nickerson, the hairdresser Guido Palau, the makeup artist Carol Brown and the model John Pearson. Shot in moody, romantic   tones, ‚ÄúFreedom! ‚Äô90‚Äù had been viewed more than 37 million times on YouTube by the time of Mr. Michael‚Äôs unexpected death at 53 last Sunday. Reached by phone this week, some of the participants recalled their experiences on set, and some of the video‚Äôs most ardent fans reflected on its impact. Naomi Campbell, model. We‚Äôd done a British Vogue cover with Peter Lindbergh with all of us. I met George in L. A. and he said: ‚ÄúI‚Äôve been told that I need to speak to you to get the girls for the video. What would it take to get you?‚Äù We were living in America, so I said you‚Äôd have to fly us in, basically. I said yes, and then Christy and Tatjana and everyone came in. The night before the video, I didn‚Äôt sleep at all, because we did five shows for Thierry Mugler in Paris. He had 76 models, and it was the big finale of him designing, although none of us knew that then. The last show finished at around 3 o‚Äôclock in the morning, and George was there. So from there I went back to my hotel, packed my bags and went on the first flight to London. David Fincher knew exactly what he wanted. He didn‚Äôt really give us parts, but he knew exactly which part of the song he wanted each and every one of us to sing. I was more the active one. Cindy was sultry. I don‚Äôt think any of us knew what it would become. We knew the song was a hit, but we didn‚Äôt know in any way what effect it would have in terms of videos, the way it would affect people. We were all really excited on the day it was going to be aired. They all had great premieres back in the day. They don‚Äôt have them anymore. Not that I‚Äôve seen. MTV‚Äôs changed. Hasn‚Äôt it? I got to see George a little bit more after that because Kate Moss was neighbors with him. When we did the Olympics, he sang there in London. We all did the finale, and he was rehearsing and Kate and I went to watch him with this amazing choir singing ‚ÄúFreedom. ‚Äù After, he invited us all back to his home and he had this really nice   in the garden, and we all got to light these wishing lanterns and we all did that together. That‚Äôs the memory I‚Äôm going to keep. Guido Palau, hairdresser. I don‚Äôt know how I got the job. George Michael and David Fincher had seen some of my work in British Vogue. It was a different time then. It was a big, big production, a   video because they had to bring in all the girls. Every girl had a day, though Christy and Linda were there together. My part was to make the girls the best they could look as who they were. They weren‚Äôt playing characters. They were playing themselves. And each had their own personality: Linda the comedian, Christy much more classic, Cindy the pinup, Tatjana this kind of film noir, and Naomi a very strong kind of woman. We extracted that from them. They weren‚Äôt prodded at all, though there were some surprises, like Linda‚Äôs hair, which she‚Äôd done for a job for someone else. It wasn‚Äôt like I said, ‚ÄúOh, dye your hair blond,‚Äù not at all. At the time, we really didn‚Äôt realize how iconic the video would become. I was probably a bit na√Øve about the whole thing seeing how it was a bit of a   job for me. What I remember most is the days being very long, and at the end of the day, the red wine would come out. There we‚Äôd be in the location van drinking and singing with George. Tatjana Patitz, model. I was in my own zone. I had to kind of slide up and down the wall for part of the day. The feel of the set was so   this big, loft kind of vibe. There was another setup with me laying on a chaise longue with a black smoking jacket. I think I may have had a bustier on. And I was smoking, even. People still smoked in videos then and even in films. George Michael wasn‚Äôt on the set the day I was there, but I‚Äôd met him a few times in L. A. Herb Ritts shot a cover of me and him for a magazine  ‚Äî   I‚Äôm trying to remember what magazine it was. So many magazines have come and gone. Glamour was very present in fashion in those days, feminine glamour, people looking at inspiration from the movie stars of the ‚Äô40s and ‚Äô50s  ‚Äî   Audrey Hepburn, Grace Kelly and Ava Gardner. A lot of magazines were pairing models with pop stars or movie stars to shoot covers. It was very different those days. The business was much smaller and wasn‚Äôt as saturated, not as fast as it is now. Models were known by their first names, and suddenly the glamour we embodied at that time crossed over to pop music videos and film. We were part of the entertainment industry, which made it really fun. George was very nice, almost a little shy. Maybe that‚Äôs the wrong word. He was mellow and kind. I was quite   at the time because I grew up with Wham and their music he was one of my first teenage crushes. In the five or so times I met him, he was always very pleasant and sweet but timid, not one of those vivacious,   people who take a room by storm. I remember the shoot being in fall or early winter. I flew in on the Concorde for the day and flew back to New York right after that. It‚Äôs funny. A lot of people were using models in videos then, but this one in particular stood the test of time. It‚Äôs not like you look at ‚ÄúFreedom! ‚Äô90‚Äù now and say, ‚ÄúOh, my God, it‚Äôs so ‚Äô80s!‚Äù It‚Äôs not like ‚ÄúWorking Girl. ‚Äù John Pearson, model. My agent called up and said: ‚ÄúHey, do you want to do this video? All the big girls are doing it. ‚Äô‚Äù Of course I said yes. I was a big George Michael fan. I used to see him all the time in London at the clubs. I arrived at the studios at 3 o‚Äôclock in the afternoon and met David Fincher briefly, and then basically sat around all day and watched while Christy and Linda were being shot. There‚Äôs one shot where Linda puts her head underneath her sweater that‚Äôs amazing. That wasn‚Äôt rehearsed. Linda really knows how to use her body to communicate in an elegant way, never cheap and tawdry. George was there, very warm and nice but very shy. The day went long, and the producer said at the end of the day, ‚ÄúCan we shoot you tomorrow?‚Äù I didn‚Äôt have another job booked, so I said, ‚ÄúSure. ‚Äù Then he said, ‚ÄúDo you mind doing it for nothing?‚Äù I said, ‚ÄúNo way. ‚Äù It was a little bit bolshie of me, but I knew what the girls were getting. In the end, I was paid $15, 000 for the day, which is not bad to hang out with these fabulous, beautiful girls. But I‚Äôd been working nonstop for three years, and I wasn‚Äôt going to do it for free. And it turned out to be one of those fabulously easy,    jobs, everyone riding on this wave of semistardom and recognition for the models. I really didn‚Äôt realize how big it all was at the time. Candis Cayne, actress, model and former drag personality. There was a group of girls in N. Y. C. in the ‚Äô90s, and we didn‚Äôt want to model ourselves on anything other than the supermodels. Linda and Naomi and Christy. So when the video of ‚ÄúFreedom!‚Äù came out and they were all in it, we were obsessed. I used to do shows at Boy Bar, so I decided I was going to do ‚ÄúFreedom!‚Äù and got Lina and Mistress Formika and Sherry Vine to play the various parts. I was Linda. She was my favorite. We did the whole thing naked under white sheets. After the very last chorus, we dropped the sheets and were completely naked holding our groin areas. That song meant a lot to us. There were singers we knew were gay then, but no one really talked about it. We just grabbed onto songs and artists who knew who and what we were, whether it was Madonna with ‚ÄúVogue‚Äù and ‚ÄúTruth or Dare‚Äù or George Michael with ‚ÄúFreedom!‚Äù I don‚Äôt know if he was openly gay or not, then. I guess not. But it was almost O. K. because we all knew and it was such a homophobic time. It was so oppressive, particularly when you factor in the AIDS epidemic. To be open would have been a career killer. But going to a club, listening to ‚ÄúFreedom! ,‚Äù it was an escape. Alan Hunter, one of the original MTV V. J. s. It was a funny moment in my life. I was driving my kid to day care on a day where I didn‚Äôt know where my career had gone. I‚Äôd left MTV. My agent wasn‚Äôt calling. I was depressed. And I turned on the radio, and on came George Michael‚Äôs new song. It was George Michael at the peak of his writing skills. It was such a brilliant   capstone I‚Äôd ever heard. It was his    song. He was saying, ‚ÄúEverything that brought me to pop stardom I now disavow. ‚Äù Then I saw the video shortly thereafter with all that iconography he‚Äôd used in his past, and he literally sets those things on fire and explodes them. The leather jacket he‚Äôd worn and the jukebox from ‚ÄúFaith. ‚Äù And he just steps away. He wasn‚Äôt even in the video. It showed how serious he was, even while he said it all with a lot of love. It was such a joyous song. I actually felt happy. Zac Posen, fashion designer. I was 10 years old, 10 or 11. I‚Äôd see it in my living room waiting for Madonna to come on. I probably wanted to be the child in ‚ÄúOpen Your Heart. ‚Äù But what I remember about ‚ÄúFreedom!‚Äù was that it was kind of a seminal predecessor to grunge. Because you have these incredibly glamorous beauties in a very industrial, Corinne Day,   space. There‚Äôs water dripping down the walls and George Michael‚Äôs leather jacket is on fire, and the models are naked. It was really the glamour of the ‚Äô80s transitioning into something more raw that was to come.",New York Times,0
Exploring the human dimension of TETRA ,"INTRODUCTION The security of digital networks is of grave importance in the society we live in today. Our networks are under constant attack in attempt to gain information. Organizations have information they must protect, which its lifeblood. Different organizations have different information to protect, for example, banks have the is often described as the target to acquire the network information and could financial information of customers. The information is the responsibility of the bank and must be protected at all times. An attacker could attempt to gain access to various levels of the bank_s network to least gather protected area of the information wanted. As a protector of information all possible scenarios of exploitation must be considered in order to protect against it. Computer networks are being used in almost all organizations today, so the security is under massive pressure to be kept up to date. In the context of computer networks security, it is often said that nothing is 100 percent secure and according to Mitnick, even the most technically secure computer networks (e.g. CIA, FBI, MIT) can be cracked with knowledge in human manipulation [1]. Similarly to computer networks, radio networks also have sensitive information that needs to be protected, whether it is being stored or transmitted. Further, in a radio network that is being used for public safety and security, for example, a breach in confidential information could be devastating. II. TETRA to in improvements that conforms radio network One communication standard that ensures the secure transmission of data over radio is the Terrestrial Trunked Radio or TETRA standard. TETRA is a digital the requirements of a Professional/Private Mobile Radio (PMR) category system. Traditional PMR networks use an analog form of communication. TETRA provides digital the form of integrated voice and data communication and the sharing of a single network by multiple users. TETRA is a digital two-way transceiver, using Time Division Multiple Access (TDMA) to allow multiple encrypted speech channels for communication and can be both point-to-point and point-to-multipoint, while allowing digital data transmission [2]. TETRA is currently in use by organizations all around the world. Germany has a nationwide adoption of TETRA for the police, fire brigade, ambulance and coastal guard under the organization for services transportation BOSNET. The United Kingdom is almost complete with the full roll out of TETRA networks for the police, fire brigade, emergency services and armed forces under the organization Airwave. Further, CONNECT is a TETRA network currently being used in London. According to the 2010 figures compiled by TETRA association, networks using TETRA are found in 118 countries around the world. In South Africa, TETRA was used by the Eastern Cape Police for secure digital communication, and to ensure public safety and security, in the Nelson Mandela Bay Stadium during the FIFA 2010 Soccer World Cup. TETRA a European Telecommunications Standards Institute (ETSI) standard, which has considered previous mobile communication standards and, as was mentioned, is used extensively throughout the world. The standard allows for a wide spread adoption, creating many uses for TETRA; however this paper will focus on the aspect of public safety and security. is A. ATETRA Network interfaces and As mentioned, TETRA is a suite of standards that cover various aspects of a digital network, like air interfaces, network the TETRA services and facilities. Figure 1 illustrates these aspects in a common TETRA network, as found in the TETRA Release from TETRAMOU, an association of TETRA specialists. information 1 Figure 1. TETRA Network Overview [3] The first aspect to consider is the Switching and Management Infrastructure (SwMI). SwMI is used to classify all the equipment and sub-systems in a TETRA network. The SwMI includes Base Stations (BS), Gateways, Controllers and Inter System Interface (ISI). The SwMi also has interfaces to the Network Manager, local dispatchers and external in Figure 1 Public Switched Telephone Networks (PSTN). The ISI enables inter-operation between other SwMI_s. The BS provides air interfaces between the SwMI and the TETRA terminals, although the terminal can have its own optional air interface with another terminal in Direct Mode Operation (DMO). Number 4 indicates Peripheral Equipment Interface (PEI), which standardizes the connection of the radio terminal to an external device. PEI supports data transmission between applications installed on the device and the connected TETRA terminal. Local dispatchers are commonly found in a dispatcher center, indicated by the number 5 in Figure 1 [3]. Dispatchers can monitor, record, terminate, join and initiate calls to users with TETRA terminals. In addition, dispatchers can monitor the location of TETRA terminals and also make a forced call to a terminal that will not ring or notify the user of the terminal, allowing the dispatcher to listen in on the terminal. A dispatcher will typically receive an emergency call from TETRA users in an emergency and can patch them through to relevant emergency services if needed. Through the network discussed above, TETRA provides critical information for the services we depend on in society, namely the public safety and security services. B. TETRA Adoption the invested latest TETRA in a brand new Training facilities can be found all around the world, but currently the only facility found on the African continent is at the Nelson Mandela Metropolitan University. INTEGCOMM and EADS Defense & Security top class infrastructure at the university, which is used for the various courses of training on campus and for The SAPS Eastern Cape TETRA Project. The mentioned project aims to equip the Eastern Cape of South Africa with technology for improved secure digital communication for police officers. The facility for training is known as the TETRA Academy, which is the first and only of its kind internationally, that offers TETRA training in collaboration with a Higher Education institution. The courses include various aspects, for example System Course, End User System Course, Dispatcher Workstation and Base Station. The courses stretch from understanding how the TETRA system works to educating dispatchers how the system operates and assisting technical staff with being responsible for commissioning and maintaining the TETRA Base Stations in the network. The above mentioned transfer critical training educates students to information safely and securely over the TETRA network [4]. It must be noted that the TETRA training mentioned above is of a technical nature and teaches users of TETRA how to operate various equipment. C. Security safety and through Confidentiality, security. Therefore, As discussed in previously, the communications over a TETRA network are typically of a crucial nature for public these communications must be secured. TETRA provides security Integrity, Authentication, Availability and Accountability using Mutual Authentication, Air Interface Encryption (AIE) and End to End encryption. Mutual Authentication is a service that allows the TETRA system to control access to the network. A TETRA Subscriber Identity Module (SIM) can be used to uniquely identify terminals, providing control of which terminals may access the TETRA network. Another important service addresses the encryption of communication between the TETRA radio devices is AIE. AIE ensures that communication over the „air_, for example, from dispatcher to terminal, is encrypted. AIE supports four different TETRA Encryption Algorithms (TEAs), namely TEA1, TEA2, TEA3 and TEA4. The AIE protects all signaling and identities, including user speech and data [5]. TETRA also supports End to End encryption using a variety of encryption algorithms that are specified by the organization. TETRA provides recommended solutions form of International Data Encryption Algorithm (IDEA) and the newer Advanced Encryption Standard (AES) algorithm for End to End encryption. TETRA also provides a wide range of security management capabilities to control, manage and operate the individual security mechanisms in a network [5]. Therefore, TETRA is considered very technically secure as the actual data is intrinsically very well protected as it is transmitted. However, there seems to be very little research or focus on the human dimension of TETRA. for organizations in the III. HUMAN DIMENSION OF SECURITY For most organizations, the protection of information assets is of vital importance. In order to secure information assets, various controls are used. Controls are typically identified as being physical, technical or operational. Physical controls include the use of patrolling guards, security gates and locks and provide physical protection. In order to bypass physical controls the employees must provide necessary identification or keys. Physical controls allow for very little room for exploitation when implemented properly. The next category of controls identifies devices that provide technical controls for information. Technical controls usually have physical controls to protect them. A simple example is the use of a firewall to prohibit unauthorized entry via technology from remote destinations. The last category of controls considers the human dimension in daily operation, namely operational controls. Organizations can develop education, activities or policies to attempt to control the human aspect of information [6]. Mitnick proved to the US Congress that he could often obtain passwords or other pieces of sensitive information by just asking for it. Mitnick explains that he used various forms of persuasion and manipulation over telephone lines to exploit the most technically advanced security systems. Security is only as secure as the weakest link; and the human dimension is often considered the weakest link [1]. A. Social Engineering Social Engineering is often used to exploit this human dimension. Social Engineering is a non- technical form of intrusion that relies heavily on social interaction. A person that exercises Social Engineering is considered a hacker. Like any other person, a hacker shares the human goals of money, social advancement, and self-worth [7]. Social Engineering uses the power of persuasion and manipulation to divulge sensitive information that a victim believes should be provided to the hacker. Typically, a Social Engineering victim does not realize he/she has been hacked until a good while later. The following subsections will describe some of identified by Microsoft_s whitepaper [7] as can be related to Mitnick_s work [1]. techniques these Intimidation The attacker impersonates an authority figure in the relevant organization or a superior in an organization (e.g. officers of the law). Intimidation proves as an easy way to force a target to comply with the request by using mild threats. Persuasion Persuasion is the act of influencing a target by means of argumentation or false reasoning. This technique requires good the forms attacker experience with social interaction to detect the hidden agenda of the attacker. Ingratiation This technique is based on a longer term attack, where a relationship with the victim to gain trust and encourages the victim to eventually divulge information. Assistance The social engineer attempts to assist the victim. The notion of assistance can build a short term relationship between the hacker information and victim, vulnerability. The uses that vulnerability to divulge information. in hacker resulting The techniques listed above are just a few of the more common examples of the many techniques a social engineer could use to obtain unauthorized information [7]. B. Exploitation through Social Engineering and to real threat related software computing is a very lot of sensitive in Social Engineering Information Technology and there are many examples of Social Engineering attacks where information has been compromised. Below are examples of Social Engineering attacks relevant to the techniques identified in the previous sections; intimidation, persuasion, ingratiation and assistance. Consider an organization with an Information Technology (IT) Helpdesk. The IT Helpdesk is a taskforce that specializes in troubleshooting problems in the organization. This taskforce, perhaps unknown to the employees, has a information available about the fellow employees. A hacker can intimidate the employees at the helpdesk by claiming to be a superior. The hacker can then request information employee_s occupation. A sensitive or perhaps new employee, who has been trained to be helpful and friendly, will feel obliged to provide the expected information and will then be unknowingly hacked. This is a common form of intimidation Social Engineering. The mentioned IT Helpdesk can also be the target for a persuasion Social Engineering attack. The hacker can physically approach the helpdesk under the guise of an employee in the building. Through various clever forms of argumentation or even a sympathetic approach the hacker could persuade the helpdesk employee into resetting an account for the hacker. Alternatively, the hacker may telephone the helpdesk threatening by the forming relationships with employee and use persuasion techniques to acquire information. In many cases, all the hacker requires is a name and surname which could be acquired through an easily obtained company telephone directory. A rather difficult form of Social Engineering is the use of a long term plan, as a lot more dedication is required from the hacker. This attack is known as the ingratiation Social Engineering attack. Consider a hacker that applies for a janitor position at a company, fellow colleagues over a short period of time, for example two to three months. The hacker now has a good relationship, causing the hack to be a lot easier. A friend will assist a colleague that is having trouble with logging into the network, for example. A friend will also allow a colleague to be alone in a room while cleaning it, as the relationship includes trust, allowing the hacker to complete his hidden task of compromising the network. When a person is assisted with a problem, they usually become thankful. A short term relationship is then formed between the two people that interacted in solving the particular problem. A hacker can pose as a person who may assist you in your problem, which in turn provides the hacker with the upper hand when attempting to hack information from the person in need. For example, a hacker could purposefully cause a network or computer issue and then contact the affected person to offer assistance in fixing the issue. When the hacker contacts the person again and asks them to download a program for his/her computer, for example, the person will be more inclined to assist the hacker.This is a form of assistance Social Engineering attack. The examples described can all be related to IT environments, but the techniques used are not solely meant for hacking IT environments. The following examples will attempt to sketch scenarios in TETRA networks that could have happened during the FIFA 2010 Soccer World Cup at the Nelson Mandela Bay Stadium with the South African Police. The dispatcher center is normally a room filled with dispatcher stations and dispatchers. Dispatchers control and monitor various calls in the network, for example an emergency call from a South African Police (SAP) officer will be immediately answered by a dispatcher and required services will be notified, be it emergency medical units or even the fire brigade. This room is filled with information about TETRA subscribers and external services. A hacker couldassume the guise as a chief technical advisor the in result limitless task.From the hidden with a legitimate looking TETRA badge and tailgate his way into the room. The hacker could then intimidate a dispatcher into relieving their position behind a dispatcher station, allowing the hacker to complete the dispatcher workstation, the hacker could compromise many areas of the TETRA network, and consequently public safety and security. For example, the hacker could acquire location of specific TETRA terminals or even install intelligent software code to compromise the dispatching station remotely. Control over a dispatcher_s computer can be considered as the ultimate compromise in a TETRA network, as the consequent scenarios are terms of exploitation. The FIFA 2010 Soccer World Cup generated a lot of job opportunities for various services during the tournament. The tournament required an increase in for public safety and security. New officers employees in various occupations pose as an advantageous target for Social Engineering hackers, as a slight increase in emotion is expected in the new environment. This could in a slightly pressurized environment for the new officers, more- so as an influx of people locally and from all over the world was expected. A hacker can easily use persuasion Social Engineering attack on these new officers, allowing him to collect a TETRA handheld terminal that is live on the network. A similar scenario to the IT ingratiation can be used for TETRA. As mentioned, during the FIFA 2010 Soccer World Cup many new employees were hired. A hacker could have easily become, for example, a janitor. Clever placement could land the hacker in the same building as the new TETRA dispatching center, allowing him unrestricted access. The hacker can also relationships with other new employees, resulting in a considerable easier hack at the cost of a bit more time. As mentioned previously, the FIFA 2010 Soccer World Cup welcomed an increase in various job opportunities. Similarly to the IT assistance Social Engineering attack, a hacker can attempt to assist one of the new SAP officers with potential technical issues on their TETRA terminal. On agreement of the newly appointed SAP officer, the hacker is allowed to inspect the terminal by possibly plugging it into a laptop for “closer inspection”. TETRA terminals have a Subscriber Identity Module (SIM), which forms part of the technical security in TETRA and is physically located in the terminal. This provides authentication to the TETRA radio network and will allow the hacker to exploit the network once copied form or stolen after assisting the new SAP officer with inspection of his terminal. C. Social Engineering Awareness in TETRA inside Many computer networks are considered very technically secure. These computing networks have the latest devices to ensure security, for example Cisco ASA firewall products to prohibit unauthorized remote access and hardware encryption modules for superior confidentiality the network. As discussed earlier, Social Engineering has been used to exploit users of computing devices. Therefore, the original target of exploiting the network has shifted from attacking technical security controls to the users of the computing network in order to bypass highly sophisticated security techniques. Similarly, TETRA digital networks address the needs of the PMR users, providing security services such as End to End encryption, Mutual Authentication and Air Interface Encryption. TETRA is a technologically secure form of communication, with little room for technical exploitation. Further, just as hacking in the computing environment has largely shifted from technical hacking to Social Engineering; it can be argued that a TETRA environment could be subject to the same Social Engineering issues. In computer environments, Social Engineering Training and Awareness is often used to attempt to educate users about typical Social Engineering attacks for increased security of computing networks. As has been mentioned, TETRA training and education focuses primarily on the technical aspects and usage of TETRA networks. It can be argued that, just as Social Engineering Training and Awareness is needed in a computing environment, so too does TETRA focus on Social Engineering Training and Awareness.This would attempt to ensure the users of the TETRA networks are aware of potential Social Engineering attacks, which could ensure public safety and security. require a training IV. CONCLUSION Even though a TETRA network is considered technically secure, it is vital for the users to be aware of potential Social Engineering attacks. Social Engineering in a TETRA network could potentially lead to confidential information being compromised and, consequently, compromise public safety and security. In order to protect the network against Social Engineering attacks, the users of the network must be educated. Therefore, a Social Engineering should be Training and Awareness program developed and tailored specifically for TETRA users. ",Scientific Journal,0
‚ÄòPlan-B‚Äô ‚Äî Obama Paves Way to Give Syrian ‚ÄòModerate Rebels‚Äô Anti-Aircraft Weapons to Fight Russia,"By Claire Bernish at thefreethoughtproject.com
President Obama contemplated arming the already controversial ‚Äòmoderate‚Äô Syrian rebels weapons to ‚Äúdefend themselves against Russian aircraft and artillery‚Äù as his Plan B for Syria when all else failed in the effort to oust Assad ‚Äî but although this treacherous plan has been effectively shelved, it remains an option for whomever next ascends to the White House.
According to a new report from the Washington Post , Obama‚Äôs Plan B has neither been ‚Äúapproved or rejected‚Äù in the U.S.‚Äô disintegrating and increasingly complex campaign to depose Syrian President Bashar al Assad ‚Äî which has, of course, precipitated a perilous proxy war with staunch Syrian ally, Russia.
Skepticism over allowing the CIA to provide various anti-Assad terrorists ‚Äî euphemistically termed moderate rebels by the U.S. ‚Äî with ‚Äútruck-mounted antiaircraft weapons that could help rebel units but would be difficult for a terrorist group to conceal and use against civilian aircraft‚Äù apparently forced the plan to be tabled, the Post reports.
In the U.S. program to topple Assad, the CIA has armed and trained defectors from Syrian military known as the Free Syrian Army ‚Äî whose own defectors went on to form the Islamic State ‚Äî as well as other terroristic groups virtually indistinguishable from one another except for their political ideologies.
These issues factored into restrictions on weapons the U.S. would provide to their sponsored rebels; but, as the Post explains:
‚ÄúRebels chafed at the restriction, complaining that it left them vulnerable to air attack by Assad and, more recently, Russia.‚Äù
Plan B was envisaged as a compromise by the CIA.
But the complex state of affairs in Syria has led not only the FSA, but other ‚Äòmoderates,‚Äô to ‚Äòradicalize‚Äô ‚Äî complicating U.S.‚Äô goals and frustrating Russian efforts to purge the embattled nation of terrorists. While the Pentagon‚Äôs public goal has duly sought to wrest control from Assad and fight the Islamic State, the CIA program of arming rebels is widely considered counterproductive ‚Äî if not, perhaps, nefarious.
Plan B, despite potentially devastating repercussions, retains stolid support from top officials, including unsurprisingly CIA Director John Brennan and Secretary of Defense Ashton Carter ‚Äî but has lost favor recently with former proponents, including Secretary of State John Kerry.
Kerry and other skeptics rightly grasp providing heavy artillery to rebels fighting Russia-backed Syrian government forces to down Russian aircraft ‚Äî which would undoubtedly turn the smoldering U.S. proxy battle into an all-out war with Russia.
Additionally, the Post reports, a growing number of U.S. officials feel the contentious CIA program has only proven to bedevil the ultimate goal of regime change, and, as one unnamed ‚Äòsenior official‚Äô noted, CIA units are ‚Äúnot doing any better on the battlefield, they‚Äôre up against a more formidable adversary, and they‚Äôre increasingly dominated by extremists,‚Äù adding, ‚ÄúWhat has this program become, and how will history record this effort?‚Äù",anonhq.com,1
Taiwan‚Äôs President Accuses China of Renewed Intimidation,"BEIJING  ‚Äî   President Tsai   of Taiwan sharply criticized China‚Äôs leaders on Saturday, saying they had resorted to military and economic threats in order to intimidate the island. ‚ÄúStep by step, Beijing is going back to the old path of dividing, coercing and even threatening and intimidating Taiwan,‚Äù she told journalists in Taipei, the capital, at a   news conference. Tensions between Taiwan and China, which have been rivals since the Communist Revolution of 1949, intensified in December after Ms. Tsai spoke on the phone with the American   Donald J. Trump, breaking a longstanding diplomatic practice. In recent weeks, China has stepped up military activities near Taiwan, sending its sole aircraft carrier through the waters near the island and dispatching military planes in the region. On Monday, Beijing announced that S√£o Tom√© and Pr√≠ncipe, an island nation off the west coast of Africa that was one of Taiwan‚Äôs fewer than two dozen remaining diplomatic allies, had switched its allegiance to the mainland, provoking an outcry in Taiwan. Despite Beijing‚Äôs recent actions, which she said had ‚Äúhurt the feelings‚Äù of the Taiwanese people and destabilized relations, Ms. Tsai vowed to avoid a confrontation. ‚ÄúWe will not bow to pressure, and we will of course not revert to the old path of confrontation,‚Äù she said. Ms. Tsai faces the delicate task of registering discontent with Beijing while also sending a message that Taiwan will exercise restraint. The United States, which sees Taiwan as one of its most reliable allies in Asia and has sold billions of dollars of weapons to the island, has long sought to avoid a conflict between the two sides. But the election of Mr. Trump could complicate matters. He has antagonized Beijing with a series of critical comments. The   has also questioned the One China policy, which has underpinned relations between Washington and Beijing for decades, and criticized China‚Äôs military buildup in the disputed South China Sea. Bonnie S. Glaser, an Asia expert at the Center for Strategic and International Studies in Washington, said Ms. Tsai‚Äôs words might reassure American officials that she would not pursue rash policies in the face of China‚Äôs show of strength. ‚ÄúShe remains calm, rational and patient,‚Äù Ms. Glaser wrote in an email. Still, Ms. Tsai, whose Democratic Progressive Party has traditionally favored independence for Taiwan, could face serious challenges in the coming months. Many people in Taiwan are nervous that Mr. Trump will use the island as a bargaining chip against China. And Ms. Tsai‚Äôs preference for stability in the region may not mesh with Mr. Trump‚Äôs bombastic style. Richard C. Bush, the director of the Center for East Asia Policy Studies at the Brookings Institution in Washington, said Ms. Tsai understood the need to ‚Äúmaintain a balance among relations with China, relations with the United States and domestic politics. ‚Äù Ms. Tsai‚Äôs vision, however, ‚Äúmay not align well with the incoming Trump administration‚Äôs apparent belief that it can pressure China on all fronts more than the Obama administration has,‚Äù he said. Ms. Tsai also sought to quell concerns about planned stopovers in Houston and San Francisco during a visit to Central America scheduled for January. The Chinese Ministry of Foreign Affairs on Thursday called on the United States to block Ms. Tsai from entering the country, warning that such a visit would embolden independence activists in Taiwan. Ms. Tsai described the visit as ‚Äúunofficial,‚Äù saying, ‚ÄúA transit stop is just a transit stop. ‚Äù",New York Times,0
How to form healthy habits in your 20s,"This article is part of a series aimed at helping you navigate life‚Äôs opportunities and challenges. What else should we write about? Contact us: smarterliving@nytimes. com. When you woke up this morning, what did you do first? Did you hop in the shower, check your email or grab a doughnut? What did you say to your roommates on the way out the door? Salad or hamburger for lunch? When you got home, did you put on your sneakers and go for a run, or eat dinner in front of the television? Most of the choices we make each day may feel like the products of   decision making, but they‚Äôre not. They‚Äôre habits. And though each habit means relatively little on its own, over time, the meals we eat, how we spend our evenings, and how often we exercise have enormous impacts. This is particularly true in our 20s, when so many of our habits are still up for grabs. The patterns you establish right now will impact your health, productivity, financial security and happiness for decades. How much money you make, how much time you spend with your friends and family, how well your body functions years from now  ‚Äî   all of these, in many ways, are products of the habits you are building today. (Related: The 8 health habits experts say you need in your 20s) And in the last decade, our understanding of the neurology of habit formation has been transformed. We‚Äôve learned how habits form  ‚Äî   and why they are so hard to break. We now know how to create good habits and change bad ones like never before. At the core of every habit is a neurological loop with three parts: A cue, a routine and a reward. To understand how to create habits  ‚Äî   such as exercise habits  ‚Äî   you must learn to establish the right cues and rewards. In 2002, researchers at New Mexico State University studied 266 individuals, most of whom worked out at least three times a week. They found that many of them had started running or lifting weights almost on a whim, or because they suddenly had free time or wanted to deal with unexpected stresses in their lives. However, the reason they continued exercising  ‚Äî   why it became a habit  ‚Äî   was because of a specific cue and a specific reward. If you want to start running each morning, it‚Äôs essential that you choose a simple cue (like always lacing up your sneakers before breakfast or always going for a run at the same time of day) and a clear reward (like a sense of accomplishment from recording your miles, or the endorphin rush you get from a jog). But countless studies have shown that, at first, the rewards inherent in exercise aren‚Äôt enough. So to teach your brain to associate exercise with a reward, you need to give yourself something you really enjoy  ‚Äî   like a small piece of chocolate  ‚Äî   after your workout. This is counterintuitive, because most people start exercising to lose weight. But the goal here is to train your brain to associate a certain cue (‚ÄúIt‚Äôs 5 o‚Äôclock‚Äù) with a routine (‚ÄúThree miles down! ‚Äù) and a reward (‚ÄúChocolate! ‚Äù). Eventually, your brain will start expecting the reward inherent in exercise (‚ÄúIt‚Äôs 5 o‚Äôclock. Three miles down! Endorphin rush! ‚Äù) and you won‚Äôt need the chocolate anymore. In fact, you won‚Äôt even want it. But until your neurology learns to enjoy those endorphins and the other rewards inherent in exercise, you need to   the process. And then, over time, it will become automatic to lace up your jogging shoes each morning. You won‚Äôt want the chocolate anymore. You‚Äôll just crave the endorphins. The cue, in addition to triggering a routine, will start triggering a craving for the inherent rewards to come. Want more? You might also like: ‚Ä¢ The scientific   workout ‚Ä¢ No time to workout? Try exercising on the job ‚Ä¢ How to pick a health insurance plan",New York Times,0
"N.F.L. Playoffs: Schedule, Matchups and Odds ","When the Green Bay Packers lost to the Washington Redskins in Week 11, dropping to   Aaron Rodgers vowed to ‚Äúrun the table‚Äù in a march to the playoffs. With a   victory over the Detroit Lions on Sunday night, the team fulfilled Rodgers‚Äô promise. Much of the drama of the matchup between division rivals was eliminated earlier in the day when the Redskins lost to the Giants, thus guaranteeing both the Packers and Lions would be playoff teams, but the N. F. C. North bragging rights, and a home game in the first round of the playoffs, were sufficient motivation for Green Bay to push hard enough to secure the team‚Äôs sixth consecutive win and the third consecutive loss for Detroit. Pundits had spent the week deciphering all of the wild scenarios that could play out for positioning among the remaining teams. But when all was said and done, ten of the teams that were in line for a playoff spot remained in the same seeding order. No. 6 Detroit Lions at No. 3 Seattle Seahawks Time: 8:15 p. m. Eastern SATURDAY on NBC The Seahawks‚Äô title hopes took a crushing blow when Earl Thomas was lost for the season with a broken leg. After the injury, the Seahawks went   with the wins coming with major asterisks as they came against the   Rams and 49ers. That collapse paled in comparison to the Lions, who lost their final three games, blowing what had been a large division lead against Green Bay. Line: Seahawks   ( : 43) No. 5 Giants at No. 4 Green Bay Packers Time: 4:40 p. m. Eastern SUNDAY on Fox Thanks to playing in the N. F. C. East, home of the   Dallas Cowboys, the Giants managed to tie Atlanta for the   record in the N. F. C. but got stuck with the No. 5 seed in the playoffs and a road game against a   Packers squad that won its final six games. The good news for the Giants is that superstitious fans will note that the last two times they played the Packers on the road in the playoffs, they not only won the games but went on to win the Super Bowl both times. Line: Packers   ( : 44. 5) Bye weeks: Dallas, Atlanta No. 5 Oakland Raiders at No. 4 Houston Texans Time: 4:35 p. m. Eastern SATURDAY on ESPN Line: Texans   ( : 37) The Texans were the least inspiring of the N. F. L. ‚Äôs division champions and that was complicated further when Tom Savage, whom the team had elevated to starting quarterback after the benching of Brock Osweiler, was forced to leave Week 17‚Äôs loss to Tennessee with a concussion. As bad as that sounds, it may still be enough against a reeling Oakland squad that lost Derek Carr to a broken leg in Week 16, Matt McGloin to a shoulder injury in Week 17, and fell all the way from the No. 2 seed in the A. F. C. to No. 5. It is unclear at this point if McGloin or rookie Connor Cook will start at quarterback against Houston. No. 6 Miami Dolphins at No. 3 Pittsburgh Steelers Time: 1:05 p. m. Eastern SUNDAY on CBS The Dolphins were a contender when the team‚Äôs quarterback, Ryan Tannehill, was lost in Week 14 with injured ligaments in his left knee. Thanks to backup quarterback Matt Moore, and Jay Ajayi, the team‚Äôs   running back, they won two of three games and secured a   berth. But going up against a   offense like Pittsburgh is a tough test for Miami‚Äôs middling defense, even if Tannehill‚Äôs knee heals enough to allow him to return. Line: Steelers  .5 ( : 47. 5) Bye weeks: New England, Kansas City With four teams vying for two N. F. C. playoff spots, all eyes were on the   game on Sunday. A Redskins victory could have caused movement in the seedings, with the Lions and Packers playing a     evening matchup. The Giants, who had already locked up the No. 5 seed and had nothing to gain, threw a wrench in the Redskins‚Äô plans, eliminating their division rivals with a decisive   victory. With the drama essentially taken out of the N. F. C. all of the playoff movement Sunday occurred in the A. F. C. where there was a   at the top of the standings in the A. F. C. West. Just a week after losing Derek Carr, the team‚Äôs quarterback and a legitimate candidate for most valuable player, to a broken leg, the Oakland Raiders were crushed   by the Denver Broncos. That, combined with the Kansas City Chiefs‚Äô   victory over the San Diego Chargers vaulted the Chiefs from a   spot all the way to the No. 2 seed in the A. F. C. which comes with a   bye in the playoffs. The loss for Oakland added to the misery of the Raiders, who have gone from Super Bowl contenders last week to a   team that will play on the road in Houston next week potentially with a   quarterback under center as Carr‚Äôs backup, Matt McGloin, injured his shoulder in the loss to the Broncos. Beyond the switch to the Chiefs as the No.   in the A. F. C. it was business as usual for the teams that will get bye weeks in the playoffs. The New England Patriots secured the No. 1 spot in the A. F. C. with a win over Miami, the Dallas Cowboys were already guaranteed the No. 1 spot in the N. F. C. before their loss to Philadelphia, and the Atlanta Falcons held onto the No. 2 seed in the N. F. C. with a   win over New Orleans. The only remaining chance for a minor   was for the Packers, who led their division by virtue of a tiebreaker, to lose to the Lions, which would have forced them to play on the road in the   round of the playoffs. But the    aspect of the de facto N. F. C. North championship went away when the Redskins lost to the Giants, which eliminated the Redskins from   contention. While most of the races were straight forward, one of the crazier playoff scenarios that had been discussed before the week was the possibility that the Tampa Bay Buccaneers could find themselves in the playoffs. They simply had to shoot the moon by beating the Carolina Panthers, having the Redskins tie the Giants, the Packers lose to the Lions, and should all of that happen they simply required wins by Indianapolis, Dallas, Tennessee and San Francisco to top the Packers in strength of schedule. Tampa Bay took care of their end by beating the Panthers early in the day, but they were eliminated officially when Dallas lost to Philadelphia.",New York Times,0
A Network Telescope perspective of the Conﬁcker outbreak,"",,
"regatio""",Scientific Journal,0,
A Visualization and Modeling Tool for Security Metrics and Measurements Management ,"measurements, because they reduce the effort needed for gathering relevant security evidence and ensure the availability and attainability of it. Savola and Heinonen list, analyze and discuss solutions for the following security-measurability-enhancing mechanisms for software- intensive systems in [1]: flexible communication mechanism, security measurement mirroring data redundancy, auto- recovery on error, multi-point-monitoring, integrity and availability checks, timing framework, measurability support of building blocks of security, use of shared metrics and measurement repositories, reuse of available metrics and measurements relevant to security and secure coding. Visualization is the graphical representation of data or concepts [13]. Vision is the most valuable sense for providing information from computers to humans because humans acquire more information through vision than through all the other senses combined [13]. Card et al. [14] propose six major methods in which visualization can amplify cognition by perception: (i) increasing memory and processing resources by allowing storage of massive amounts of information in a quickly accessible form, (ii) reducing searching by grouping information together, (iii) enhancing recognition of patterns by enhancing patterns, (iv) perceptual inference by making some problems obvious, (v) perceptual monitoring by allowing monitoring of a large number of potential events, and (vi) manipulable medium by allowing exploration of a space of parameters unlike static diagrams. Adequate visualization of security metrics can help us interpret abstract and complex data, that is inherent to security issues, and form a mental image of them. Because research of security metrics is still in its infancy, visualization of security metrics and measurements has not been studied much before, and there is a lack of tools and frameworks supporting it. However, general visualization the management of different kinds of metrics, sometimes also for security metrics. These tools do not well support security metrics meaningfulness goals, lacking support for hierarchical security metrics modeling. tools are used for III. HIERARCHICAL PRESENTATION OF SECURITY METRICS AND MEASUREMENTS AND NEEDS FOR VISUALIZATION Security risks and their impact on the SuI often remain too abstract from the perspective of software-intensive systems development. On the other hand, a lot of detailed security- related information is available yet rarely utilized because their relation to the actual security objectives is not understood. Evidently, a large number of security metrics are needed for sufficient understanding of the security level. The relations between high-level security objectives, specified based on the results of risk analysis, and the detailed level measurements are to understand without hierarchical presentation. difficult Moreover, compliance to standards and best practice specifications often need to be shown. In the following, we discuss decomposition of security objectives and aggregation of security metrics results, and the needs for visualization of security metrics. Security objectives form an important basis for security metrics development. The effectiveness of security controls is determined by continuous risk analysis, carried out in a more frequent cycle compared with business-level risk management activity [3]. At the architectural level, operational risk analysis can be implemented by Architectural Risk Analysis (ARA) [15], which is sometimes referred to as threat modeling [16] or security design analysis. Hierarchical presentation of security metrics is based on the decomposition of security measurement objectives. Savola and Abie [17, 18] developed and analyzed collection of security metrics to measure the correctness and effectiveness of security controls. The aforementioned research [17] introduced an iterative methodology for security metrics development, which has been simplified here: (1) Carry out prioritized ARA of the SuI; (2) Utilize suitable security metrics taxonomies and/or ontologies to further plan the measurement objectives and metrics types; (3) Develop and prioritize security objectives; (4) Identify Basic Measurable Components (BMCs) from the security requirements using a decomposition approach. BMCs are leaf components of the decomposition that clearly manifest a measurable property of the system. Similarly, decompose the system architecture to components; (5) Define measurement architecture and evidence collection. Match the BMCs with the relevant system components with measurable data attainable; (6) Integrate metrics from other sources and select BMCs based on feasibility analysis; and (7) Develop appropriate balanced collection of metrics from the BMCs. The BMCs are identified by security objective decomposition [17, 18], based on the original idea proposed by Wang and Wulf [19]. Figure 1. An example authentication decomposition [19] Figure 1 provides an example of high-level decomposition of authentication objectives. The BMCs are Authentication Identity Uniqueness (AIU), Authentication Identity Structure (AIS), Authentication Identity Integrity (AII), Authentication Mechanism Reliability (AMR) and Authentication Mechanism Integrity (AMI). The decomposition is based on the fact that identity concept and authentication mechanism contribute essentially to the security strength of authentication. A more detailed explanation of the listed BMCs can be found in [17]. The boxes in Figure 1 can be seen as metrics nodes, which are associated with logical expressions on security-relevant parameters. Note that the leaf nodes of Figure 1 are BMCs, not measurements. Metrics and measurement are further refined from the BMCs, and the leaf nodes resulting from that refinement contain raw measurements with security-relevant information. The raw measurements can contain different scale types, e.g., string values of chosen algorithms, counts of successful and unsuccessful authorization attempts, and penetration test results. Special measurement probes can be developed to obtain this kind of information from software systems. A measurement probe is a tool for performing checks on infrastructure objects of a software system in order to provide the required information for the purposes of security metrics. The scale type of security metrics can be nominal, ordinal, interval or ratio. Moreover, confidence values, e.g., in the range from 0 to 1 can be attached to each metric. Security metrics can be aggregated in the form of a weighted sum. In the previous example of authentication, the following formula can be used to aggregate the BMCs in Authentication Strength AS [17]: AS w AII _ _ w AIU AII _ AIU _ w AMR _ _ w AIS AMR AIS _ w _ _ AMI _ AMI , (1) to quote, “where wx is the weighting factor of component x and ‘¯’ denotes normalization and uniform scaling of the component metrics.” The scale types of the leaf metrics need to be normalized. Other ways of aggregation schemes and weightings are also possible, depending on risk prioritization. the The meaningfulness of security metrics and measurements are affected by the aggregation: important details of security- Figure 2. Different colors are used to visualize the results and to offer better meaningfulness of aggregation of security measurement results relevant information are often lost when values of sub- components are merged into one result. Plain aggregation over-simplifies security-related issues. Moreover, practical experience has shown that the aggregated results of security measurements tend to show too optimistic outcomes, or using traffic light terminology, they tend to be green. When designing the visualization solution, it should be remembered that humans can only observe detailed changes in one object at a time, and yet we still look at our surroundings with the impression that we see all the objects and their details simultaneously [20]. The main needs for security metrics visualization can be summarized as follows, based on the above discussion: _ Structured security metrics entities, “building blocks”: The metrics and measurements should be presented by nodes in which it should be possible to associate logical expressions. In measurements, results represented by plain values are sufficient. _ Meaningful metrics relationship modeling: the relationships between security objectives and low-level measurements should be visible. This can be achieved by hierarchical presentation of metrics nodes formed by decomposition of security objectives. _ Alleviation of the metrics aggregation oversimplification challenges by visualization: It should be possible to investigate measurement details without losing the manageability of a large collection of security metrics and measurements. To increase the usability of security metrics, visualization solution should enable to investigate both aggregated and non- aggregated metrics and measurements. _ Measurement probe and security-measurability- enhancing mechanism support: Connection to measurement probes offering raw measurements should be available to the visualization system to support automation of evidence gathering. Moreover, enough for measurability-enhancing mechanisms are needed. support IV. METRICS VISUALIZATION SYSTEM In the following, we briefly present the MVS visualization platform, discuss the security metrics and measurements specification and visualization functionalities of it, and propose how the tool can be used to support security-related decision- making. The MVS is a graphical security metrics modeling environment enabling specification of security metrics and measurements based on Security Metrics Nodes (SMN). It offers an initial solution towards meeting the visualization needs listed above: structured security entity concept, metrics relationship modeling, alleviation of metrics aggregation oversimplification challenges, and measurement probe and security-measurability-enhancing mechanism support. It should be noted that the tool is a research prototype. It can be extended by more advanced graphical features, e.g., the display of different types of diagrams. Note that MVS is a tool for security metrics modeling, not for security modeling in general. Figure 3. Pop-up menus can be opened from each SMN A. Visualization Platform The MVS Visualization Platform consists of the following entities: a hierarchy modeling engine, properties, a logical operations module and a Representational State Transfer (REST) [21] based communication module. These components implement the core functionality of the tool, including saving the current state of the metrics model and saving the measured data. The Visualization Platform supports extensibility well because of the REST communication module and logical operations module implementation. Communication between the MVS and SuI is implemented with the REST interface. Logical operations can be linked with the data gathered from the SuI through REST. A JavaScript engine is used in interpreting logical operations. This allows the user to freely create inference rules, the only restriction being JavaScript syntax. Values come from the results of the sub-component metrics’ operations. Operation can also be a static value or direct measurement from the SuI. B. Security Metrics Presentation: Metrics Models and Nodes The MVS is used to specify, visualize and manage a security metrics model SMM. The basic building block of a metrics model in the MVS is a security metrics node SMN. In an SMM, SMNs form a metrics hierarchy, connected with relationship arrows. All SMNs in the model have the same default property fields: _ _ _ _ _ distinctive name, confidence value of the metric/measurement (range 0…1), operation specification (logical expression), threshold criteria and associated visualization, poll frequency field for automated measurements, and enable/disable flag for operation value evaluation. _ Each SMN can be configured to represent (i) a security metric, (ii) an aggregated security metric, or (iii) a raw security measurement. In the MVS, security metrics can be associated with logical operations using JavaScript scripting language. In addition, it is possible to set thresholds. The outcome is visualized by associating colors to different value ranges of the operation specified in the SMN. An aggregated metric represents more general security-relevant information compared to its sub-component metrics. A raw security measurement SMN is used to input different types of security measurement data the model. Moreover, measurement probes can be associated with it. to Figure 4. Properties of each metrics node can be edited in a window opened from the pop-up menu C. Coloring of Nodes All SMNs can be colored (or left blank). Coloring is important from the metrics management perspective. In security metrics hierarchies, coloring makes it possible to track the status of a large number of metrics in the same view. Using the Card et al.’s [14] terminology, coloring helps especially in reducing the searching by grouping information together and enhancing recognition of patterns by humans. The default coloring scheme of the MVS imitates traffic lights: red stands for insufficient level, yellow for intermediate level, and green for sufficient level. It is possible to add any number of coloring rules for each SMN. Figure 2 shows an example of how coloring can be used to visualize value ranges. The operation in the figure selects the larger of the values of the sub-metrics representing addition and multiplication and outputs label “Addition” or “Multiplication” according to the comparison. Finally, a color is assigned to the SMN according to the operation result using JavaScript based coloring rules. Figure 5. Sub-component SMNs and linking D. Security Metrics Nodes Properties The SMNs can be specified and managed by using a pop- up menu (see Figure 3). Using menu actions, it is possible to add, delete, copy, cut and paste nodes, and create links between different model diagrams. Moreover, it is possible to scale and rotate their graphical box form. A property editing window is opened from the pop-up menu, See Figure 4. (1) The actual security metrics are expressed here in the “Operation” box using JavaScript. The actions associated with the numbers shown in the figure are as follows. (i) Results of the child nodes and measurement value can be used as variables in the operation. (ii) Thresholds for different values of the operation can be set, along with colors for their visualization. Only the default color is compulsory, otherwise any number of color rules can be added. The colors can be selected from a wide color palette. (iii) The thresholds are also expressed using JavaScript. Moreover, if the SMN is used to poll information from a measurement probe, the poll frequency is set from here. E. Saving and Linking SMMs The SMM can be saved to a file. MVS uses XML format; it is also possible to read and edit SMM files with external XML editors, or with web browsers. The SMM can be split into several files using linking, as Figure 5 shows. Linking is very useful when the SMM is built from logically separated sub- models. When using the same sub-model in several SMMs, linking can be used to decrease duplication. A linked SMN obtains its values from the root SMNs of linked SMMs (sub- models). When opening an SMM which has links to other Figure 6. Example of a security metrics hierachical presentation (End User Authentication) with a specific coloring scheme models, the linked models are opened at the same time. Only one instance of the model that is saved to a file can be opened at a time. As Figure 5 displays, values and colors from sub- component SMNs are propagated to the parent SMN. An SMN with a link to another SMM is labeled with italic font. F. Interface to Measurement Probes and Security- Measurability-Enhancing Mechanisms A specific interface for specific measurement probes was implemented to the MVS. The interface implements the connection to specific measurement probes that are part of an adaptive distributed measurement framework described in [22] and [23]. However, the tool can be extended with different kinds of measurement probes. It should be noted that availability and attainability of measurements will evolve over time, e.g., when new probes (or new versions of old probes) become available or existing ones are removed. The changes are managed in the MVS using the leaf SMNs property editing and proper communication solutions between the MVS and the SuI. MVS supports enhancing mechanisms: the following security-measurability- _ a flexible communication mechanism by the REST communication module, _ multi-point-monitoring: easy interconnection of several SMMs, use of shared metrics and measurement repositories: XML file structure enables use of other XML-based metrics files, reuse of available metrics and measurements relevant to security and secure coding: easy interconnection of several SMMs. _ _ G. Security Metrics Model Example Figure 5 depicts a simplified yet realistic SMM for end-user authentication metrics. In the example metrics hierarchy, the focus is on a user name/password pair based authentication mechanism. However, a blank SMN is included for other authentication categories which can be applicable later during the process of analysis. In fact, the SMM supports comparison of the security performance of different authentication categories, provided that relevant sub-component metrics are included in the SMM. Figure 7. Proposed process for security effectiveness assessment Following the decomposition examples of Figure 1, the main branches of authentication decomposition are identity strength and authentication mechanism strength. The metrics in the identity strength branch are based mostly on the reference architecture discussed in the U.S. National Institute of Standards and Technology (NIST) document, the Electronic Authentication Guideline [24]. The ID strength branch contains two sub-branches: ID uniqueness and ID secrecy. The ID uniqueness sub-branch contains sub-metrics for ID registration, ID unforgeability and user-selected password guessing strength, focusing both on detection and prevention. The ID secrecy branch contains sub-metrics for the strength of password scheme in a database, maximum password history and maximum password age. Note that ‘+’ in the SMN denotes that the hierarchy can be further opened, and ‘-’ means that the hierarchy in question is opened. The mechanism strength sub- branches are opened in the figure. Confidence in the metrics or measurements in the SMN is shown in a small box in the upper right-hand corner of each SMN using traffic-light notation. By coloring the different branches and implementing a color aggregation mechanism, it is possible to see the status of larger security-relevant entities and detailed metrics values at the same time. In the aggregation, either positive or pessimistic thinking can be applied: the aggregated color can be chosen to be an “average” color of the sub-values, or the lowest value can be copied upwards. In some cases, being more pessimistic is a better choice, while in others it is not. For example, in the mechanism branch, if there are fatal implementation metrics values, such as, the account and device deactivation not being configured adequately, it is better to copy red values upwards in the metrics hierarchy. H. MVS in Interpretation of Security Effectiveness Evidence Figure 7 proposes a process how to use the MVS in security effectiveness assessment. There are various factors which enable security effectiveness of the target under investigation: security-relevant configuration correctness, correct design, implementation and deployment of security controls and proper security testing activities. Metrics to investigate the different factors should be part of the security metrics hierarchy. Different SMMs can also be generated for different types of factors. Metrics representing the enabling factors can be compared with the security effectiveness evidence, such as results research and development), and from incidents (during the operation and maintenance). Moreover, vulnerability information found from open databases can be considered to be security effectiveness evidence. By using the MVS, it is possible to detect shortcomings in security correctness in a systematic way, compare the metrics results with security effectiveness evidence, and base decision-making on this evidence. For instance, if penetration tests show failures in the authentication mechanism, the types of failures can be associated with the relevant SMNs in the hierarchy, and the configuration, architecture, implementation or deployment of the relevant part of the mechanism can be fixed. from penetration (during testing V. RELATED WORK to increase The related work lacks security metrics visualization tools the meaningfulness of metrics being able low-level connecting high-level security objectives and measurements. However, there are tools and frameworks available for (i) the visualization of security-related data and (ii) the visualization of information in general. In the following, we discuss these categories of contributions. Security visualization is often used in analyzing huge security-related logs or traffic. However, the field is very young. Davix [25] is a collection of 25 different security visualization tools, allowing, e.g., the building of maps from pcap files and map protocol use in real-time across a network. Davix is based on the SLAX Linux operating system [26]. Interactive Quality Visualization (IQVis) [27] tool is designed to visualize the variability of software quality attributes, including security, at run-time. In IQVis, nodes of the model represent infrastructure components, such as devices, sensors and smart agents. PortVis [28] is a tool for visualizing security- related network data. It is useful especially as a port scanning application. SnortView [29] uses Network-based Intrusion Detection Systems (NIDS) logs to present security alert information. Being designed to monitor run-time behavior, these tools can potentially be connected to the MVS, offering input to the raw measurements. Marty [30] widely discusses security visualization approaches and available tools. A lot of research has been conducted in the field of information visualization in general, and a variety of tools and frameworks have been developed to visualize different properties. Software visualization is used to analyze artifacts related to software and its development. Evolve [31] was originally developed to visualize the run-time behavior of Java programs, but can also be used as a standalone tool. Prefuse [32] is a toolkit for interactive information visualization. Streamsight [33] is a visualization tool for large-scale streaming applications. VI. CONCLUSIONS AND FUTURE WORK in software-intensive systems. The We have introduced a modeling and visualization tool called MVS for the management of security metrics and measurements tool increases the meaningfulness of metrics in the contexts of security assurance and risk management by hierarchical metrics modeling, connecting high-level security objectives with detailed measurements. In addition, the tool reduces human visual searching by grouping security level information together and enhances recognition of patterns by metrics value aggregation coloring schemes. The tool can also be utilized for visualization activities other than security measurement, but has been especially developed to alleviate meaningfulness challenges involved in the use of security metrics and measurements. The tool clearly increases usability of security metrics approaches. Development of the MVS tool is still at initial research stage and we intend to enhance it with different types of visualization views facilitating better understanding of different types of security-related evidence, e.g. security effectiveness, correctness, efficiency, or different categories of compliance. Moreover, more alternatives to present the measurement results are planned. ",Scientific Journal,0
Implementation Guidelines for a Harmonised Digital Forensic Investigation Readiness Process Model,"INTRODUCTION for a standardized digital Digital forensics gained importance rapidly over the past years. The need forensic investigation process, including digital forensic investigation readiness, is on the rise due to a raised number of security incidents, raised complexity of digital forensic investigations and everyday advancement of information and communication technology. The fact that societies depend heavily on information technology also contributes to the importance of digital forensics. Dealing with digital evidence requires a standardized and formalized process in order for digital evidence to be accepted in a court of law. By the time of writing this paper, there currently exists no international standard formalising the Digital Forensic Investigation Readiness Process (DFIRP). An effort to standardize the process has, however, started within the International Standardization Organisation (ISO), by the authors [1]. Note that ISO/IEC 27043 is considering the DFIRP as an integral part to the Digital Forensic Investigation Process (DFIP) and, for implementation of ultimately, the DFIRP process should be contained within the DFIP processes, i.e. within one holistic DFIP model. Note that the focus of this paper is only on the implementation of a DFIRP and not on the entire holistic implementation of the DFIP as described in ISO/IEC 27043. Further there is no standardized and harmonised set of implementation guidelines a standardized and comprehensive digital forensic investigation readiness process. The authors define the problem to be addressed in this paper as follows. Due to the fact that there are various digital forensic investigation readiness process models in use across the globe, there currently exists no harmonised model nor there exist implementation guideline for such a process, which can be used as a standardized set of guidelines for any organisation. Providing guidelines for such a process should expedite Digital Forensic Investigations (DFIs). This would primarily be achieved through enabling easier, more efficient and effective planning and preparation for a DFI. Such guidelines would also be a good departure point to encourage the training of inexperienced persons in the field of digital forensic investigation readiness. This section introduces the reader to the subject and states the problem to be addressed. The remainder of the paper is structured as follows. Section II provides background on digital forensic investigation readiness, as same as on legal aspects regarding the digital forensic investigation process. Also, this section explains the basics of the comprehensive harmonised digital forensic investigation model proposed by the Valjarevic and Venter [14, 15], with the focus on digital forensic investigation readiness processes. After that, Section IV presents the implementation guideline for a harmonised digital forensic investigation process model. Section IV concentrates on discussing the proposed guideline and Section VI concludes this paper with indications of future work. II. BACKGROUND The subsections to follow provide background on the following topics. First, background on digital forensics investigation readiness is provided in order to introduce the reader to the basics of the subject. After that, we provide towards interacting with forensic the definition of digital background on the legal aspects regarding the digital forensic investigation readiness processes, in order to show and emphasize the need for a harmonised and standardized process to be followed. A. On Digital Forensics Investigation Readiness The authors first wish to provide a definition of digital forensics as established in previous works of Valjarevic and Venter [14, 15]. Digital forensics is defined as the use of scientifically-derived and proven methods the identification, collection, transportation, storage, analysis, presentation and distribution and/or return and/or destruction of digital evidence derived from digital sources, while obtaining proper authorization for all actions, properly documenting all actions, the physical investigation, preserving the evidence and the chain of evidence, for the purpose of facilitating or furthering the reconstruction of events found to be criminal, or helping to anticipate unauthorized actions shown to be disruptive to planned operations [2]. Now we introduce investigation readiness. Digital forensic readiness is defined as the ability of an organisation to maximize its potential to use digital evidence whilst minimizing the costs of an investigation [3]. What follows is a brief overview of work related to the digital forensic readiness processes. Following works, by different digital forensic experts, scientists and practitioners contain represent past work in digital forensic readiness process itself and give some limited guidance in terms of implementation guidelines. Tan [3] identified factors that affect digital forensic readiness: how logging is done; what is logged; Intrusion Detection Systems (IDSs); digital forensic acquisition; digital evidence handling. Yasinsac and Manzano [4] propose six categories of policies to facilitate digital forensic readiness: retaining information; planning the response; training; accelerating the investigation; preventing anonymous activities; protecting the evidence. Wolfe-Wilson and Wolfe [5] emphasize the need for an organisation to have procedures in place in order to preserve digital evidence in the event that a DFI is needed. Rowlingson [6] defines a number of goals for digital forensic readiness as follows: To gather admissible evidence legally and without interfering with business processes; To gather evidence targeting the potential crimes and disputes that may adversely impact an organisation; To allow an investigation to proceed at a cost in proportion to the incident; To minimize interruption to the business from any investigation; To ensure that evidence makes a positive impact on the outcome of any legal action. Rowlingson also defines key activities in the implementation of digital forensic readiness and this is, in the opinion of the authors, the closest to DFIRP model defined by Valjarevic and Venter [14,15]: Define the business scenarios that require digital evidence; Identify available sources and different types of potential evidence; Determine the evidence collection requirement; Establish a capability for securely gathering legally admissible evidence to meet the requirement; Establish a policy for secure storage and handling of potential evidence; Ensure monitoring is targeted to detect and deter major incidents; Specify circumstances when escalation to a full investigation should be launched; Train staff in incident awareness, so that all those involved understand their role in the digital evidence process and the legal sensitivities of evidence; Document an evidence-based case describing the incident and its impact; Ensure legal review to facilitate action in response to the incident. Since the first Digital Forensic Research Workshop (DFRWS) in 2001 [7], the need for a standard framework for digital forensics has been acknowledged by the information security society. A framework for digital forensics needs to be flexible enough so that it can support future technologies and different types of incidents. Therefore, it needs to be simple and abstract. On the other hand, if it is too simple and abstract then it is difficult to create tool requirements and test procedures for each phase [8]. There are several works presenting digital forensic models, which include readiness as a phase, but, to the best knowledge of the authors, there is no DFIRP model proposed, except the one by Valjarevic and Venter [14, 15]. Carrier and Spafford [9] proposed a digital investigation process model, which has 17 phases, divided in five groups, one group being readiness phases. The group contains two phases: operation readiness phase and infrastructure readiness phase. Mandia, Prosise, and Pepe [10] also proposed a digital investigation process model that includes a readiness phase, known as the pre-incident preparation phase. Beebe and Clark [11] proposed the Hierarchical, Objectives- Based Framework for the Digital Investigations Process, which includes a preparation phase. Beebe and Clark include a preparation phase in their model. This phase encompasses activities to fulfil the aims of digital forensic readiness. The next section provides details on the proposed DFIRP model. B. Legal Aspects In this section the authors provide an overview of the legal aspects pertaining to digital forensics and especially the admissibility of digital evidence in a court of law. This overview is not comprehensive but aims to provide the reader with a sense of the need for a harmonised, and ultimately, a standardized digital forensic investigation process. It should be noted that legal requirements may differ extensively in different jurisdictions across the world. For example, in the United Stated of America cases that include the presentation of digital evidence are treated under rule 702 of the Federal Rules of Evidence, which says: ""If scientific, technical, or other specialized knowledge will assist the trier of fact to understand the evidence or to determine a fact in issue, a witness qualified as an expert by knowledge, skill, experience, training, or education, may testify thereto in the form of an opinion or otherwize."" For application of this rule, the Daubert case (Daubert v. Merel, 1993) is the most important. Other countries have similar guidelines regarding the admissibility of digital evidence. In the United Kingdom, for example, examiners usually follow guidelines issued by the Association of Chief Police Officers (ACPO) for the authentication and integrity of evidence [12, 13]. C. Overview of the digital forensic investigation process classes In this sub-section we wish to introduce the digital forenic investigation process and its classes, for better understanding of the process proposed in [14, 15]. The readiness class of processes deals with pre-incident investigation processes aimed at reaching digital forensic investigation readiness within an organisation. Note that the readiness processes are optional to the rest of the digital forensic investigation processes. The main reason why the readiness processes are optional is due to the fact that these are proactive compared to the rest of the investigation processes, which are reactive in nature. The initialization class of processes deals with the initial commencement of the digital forensic investigation. The processes in this class are concerned with incident detection, first response and planning and preparation of the actual digital forensic investigation. The acquisitive class of processes deals with the physical scene investigation of a case. Processes in this class are concerned with acquisition of digital evidence and include incident scene documentation, digital evidence identification, collection, transportation and storage of digital evidence. The investigative class of processes deals with uncovering the digital evidence. It analyses and interprets digital evidence acquired in order to relate these with actual events and entities (i.e. people and computers). Processes in this class are concerned with digital evidence analyses and interpretation, followed by reporting, presentation and investigation closure. The concurrent class of processes takes place concurrently with all the other processes mentioned above. Concurrent processes are defined as the principles which should be applied throughout the digital forensic investigation process since such concurrent processes are applicable to many other processes within the digital forensic investigation process. Figure 1 shows the classes of digital forensic investigation processes and an overview of their relations. Figure 1: The classes of the comprehensive harmonised digital forensic investigation process model 1) Overview of the readiness processes Figure 2 depicts the readiness processes class (harmonised digital forensic investigation readiness) as described above, refined into process groups as follows. The class of readiness processes consists of three distinctive readiness process groups, being the planning process group, the implementation process group and the assessment process group, as shown on Figure 2. Figure 2: Readiness processes groups The planning processes group includes all readiness processes that are concerned with planning activities, including scenario definition, identification of possible digital evidence sources, planning pre-incident collection, storage and manipulation of data representing possible digital evidence, planning pre- incident analysis of data representing possible digital evidence, planning incident detection, and defining system architecture, as all depicted in Figure 3 [14]. The implementation process group includes the following system architecture, readiness processes: implementing and manipulation of data representing possible digital evidence, implementing pre-incident analyses of data representing possible digital evidence and implementing incident detection, as shown in Figure 3. These processes are concerned with the implementation of the results of the planning processes [14]. implementing pre-incident collection, storage Figure 3: Readiness processes The assessment process group two readiness processes, being the implementation assessment process and the application of assessment results process [14]. The readiness processes are iterative, which implies that, after the last process, one can return to previous readiness processes, as shown in Figure 3. includes III. IMPLEMENTATION GUIDELINES FOR A HARMONISED DIGITAL FORENSIC INVESTIGTION READINESS PROCESS MODEL This section defines guidelines for the implementation of a comprehensive, harmonised digital forensic investigation readiness process, which forms part of the comprehensive and harmonised digital forensic investigation process model. The process has been proposed by Valjarevic and Venter in 2012 [14, 15]. These papers propose a harmonised organisation of the processes while introducing a novel to apply approach in the way some of the processes have been implemented. Guidelines are proposed based on a literature study and detailed analyses of processes proposed within the harmonised digital forensics investigation readiness process [14, 15]. The aim of the guidelines is to enable persons or entities responsible for digital forensic investigation readiness in their organisation the harmonised digital forensics investigation readiness process, for any type of potential digital forensic investigation, while conforming to the basic principles of digital forensics and digital evidence admisibility. The the proposed guidelines for each of the sub processes within the harmonised digital forensics investigation readiness process. In each of the specific subsections the authors first provide a summary of the harmonised digital forensics investigation readiness process itself, after which the guidelines for its implementation are provided. Each of the readiness processes and relevant implementation guidelines are explained in the clauses that follow. following subsections will present A. Scenario definition In this process one should examine all scenarios where digital evidence might be required. The output of this process includes the defined scenarios. These might be scenarios of information security incidents, such as unauthorized use of resources. These can also be scenarios of other events that, as a consequence, require a digital forensic investigation, such as investigating the use of a computer to distribute child pornography. The following implementation guidelines have been identified for this process. It is recommended that a proper risk assessment is performed during this process for each identified scenario, respectively. A risk assessment would enable one to better identify all possible threats, vulnerabilities and related scenarios that would expose particular information assets. Based on the assessed risk from certain threats, vulnerabilities or scenarios, one can, in later processes, better decide on the required measures to achieve investigation readiness within an organisation. This will enable an organisation to take into account the risk level, costs, and benefits of possible measures in a bid to reduce the identified risk. B. Identification of possible digital evidence sources In this process one should identify all possible sources of digital evidence within an organisation. The output of this process is the defined possible sources of digital evidence. The following implementation guidelines have been identified for this process. It is recommended that an organisation makes a complete list of their information resources, including, but not limited to: hardware (computers, servers, smart phones, phones, ip phones, cellular phones, fax machines, network equipment, storage systems, tape libraries, memory mediums etc.), software (operating systems, applications, database management software, drivers, add-ons etc.), data (electronic data in all its forms and formats) and users of specified hardware, software and data. Obtaining the said lists is often also an outcome of a risk assessment as indicated in the previous process. After this, possible sources of digital evidence within these identified information resources should be analysed. This analysis should be performed based on identified possible scenarios when digital evidence might be required. One should map all information resources and all possible risks identified in order to define all possible digital evidence sources. Some of the identified possible sources might not be available as identified during the scenario definition process. For example, if access logs are not introduced within the system, it means that access logs will not be available as a source of data in the case of a digital forensic investigation. In that case, measures should be explored to make the identified source available. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. C. Planning pre-incident collection, storage and manipulation of data representing possible digital evidence In this process one should define activities for pre-incident collection, storage and manipulation of data representing possible digital evidence. The output of this process includes the defined activities for pre-incident collection, storage and manipulation of data representing possible digital evidence. The following implementation guidelines have been identified for this process. One should determine which of identified possible digital evidence artifacts should be collected. Collection, storage and maniputaltion (processing) techniques as well as technology to be used for these activities should be defined, including techniques and technology for eventual destruction of digital evidence. Note that the collection period of potential digital evidence is to be determined by a risk assessment. Also note the collection, storage and manipulation of data have to conform to digital forensic investigation principles in order for digital evidence to be admissible in a court of law. Lastly, note that the retention period of data is to be determined based on three factors: by conducting a risk assessment; previous experience regarding incident detection, data quantities, network capacity and all other matters that could influence cost or efficiency of this process; laws within the particular jurisdiction. During this process, one should also make a list of all needed resources in order to perform needed actions, including human resources and technology resources. A list of human resources needed should include the investigator’s level of education, knowledge and skills. A list of technology resources needed should include the needed hardware and software, with special attention to specialized hardware and software devices for that collection, storage and manipulation of potential digital evidence. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. D. Planning pre-incident analysis of data representing possible digital evidence the digital forensic In this process one should define procedures for pre-incident analysis of data representing possible digital evidence. The input to this process includes the scenarios as defined in the scenario definition process as well as the output from the pre- incident collection process. The input must also include the aims for the readiness processes. The output of this process includes the defined activities for pre-incident analysis of the data that represent possible digital evidence. The aim of this analysis is to detect an incident. Therefore, activities defined in this process must include exact information on how the incident is detected and what behaviour constitutes an incident. As the output of this process is delivered in the form of detected incidents, this links to the input of the incident detection process of investigation processes as listed in Figure 3. The following implementation guidelines have been identified for this process. As the task of data analysis and incident detection is often outside the scope of the functionalities of targeted information systems, it is recommended that this process defines an interface between the readiness processes and a monitoring system, which would analyse data in order to detect incidents. The monitoring system can be any system that is specialised for this purpose. For example, it can be any or more of the following systems: intrusion detection and prevention logging systems, etc. Furthermore, a decision should be made on a solution that will be chosen for a monitoring system, as defined above. In addition, an interface and the data format for data exchange over such an interface should be strictly defined. Special attention should be given to the automation of the interface. As activities of this process are often outside the scope of the functionalities of the target information system, and are performed within an external system, it is of high importance that the appropriate interfaces are defined in detail. For this planning process it is also imperative to make a list of all needed resources in order to perform all needed actions, including human resources and technology resources. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. change-tracking systems, systems, E. Planning incident detection In this process one should define actions to be performed when an incident is detected. The output of this process includes defined actions to be performed once an incident is detected, in particular information to be passed on to the rest of digital forensic investigation process. Information should also include pre-known system inputs, results from all of the readiness class processes as well as data gathered and generated during the implementation process group processes. The following implementation guidelines have been identified for this process. While defining what constitutes an incident is a task of previous process, in this process one should define all information to be collected about an incident and one should also define specific actions to be performed when an incident is detected. Information to be collected might include details on: _ _ _ _ _ _ _ _ hardware, software, data users affected by the incident, description of the threat, description of the vulnerability used, description of exposure, information available about origin, technique and technology used to cause an incident. Furthermore, actions to be performed once an incident is detected depend on the particular system used for incident detection, policies of the organisation and the jurisdiction. Actions can range from neutralising threats using anti-virus software, shutting-down a network connection, or notifying an internal or external department, which will then act on the incident. At this point one should plan for possible needed interactions with the physical investigations at the crime scene (event scene). Also, there should be a clear procedure on which persons and which institutions should be notified during the occurrence of a specific incident. Once again, one should make a list of all needed resources in order to perform the needed actions, including human resources and technology resources. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. this process one should define F. Defining system architecture information system In architecture for the organisation, while taking into account the output results of all previous readiness processes. Input to this process is the results from all previous readiness processes. The input must also include the aims for the readiness processes. The output of this process is the defined system architecture for the organisation. The aim is to customize system architecture to accommodate the accomplishment of the aims of the readiness processes. The following implementation guidelines have been identified for this process. One should consider including all changes that might contribute to achieving a higher level of digital forensic investigation readiness. Examples of this would include measures and solutions for collecting, storing and analyzing possible digital evidence, introducing new sources of possible digital evidence and introducing solutions to improve the organisation’s overall level of information security. One should repeat risk analysis at this point, to weigh possible benefits of changes to system architecture and compare these to needed economical investment to achieve these changes. The repeat of a risk analysis is needed as there are going to be changes to the resources and therefore changes in possible vulnerabilities, threats and exposures. It should be noted that the level of risk analysis required here should not be as exhaustive as the original risk analysis, since many of the issues would have been dealt with during the initial risk analysis. One must make sure that proposed changes do not interfere with functionalities of the target information system and that they do not change or improve current level of information systems security. Proposed changes must detail changes in: _ _ _ _ _ _ _ _ _ _ logical system layout, physical system layout, system network layout, hardware, software, firmware, operating procedures, functionalities, data formats, data flows. During this process one should also make a list of all needed resources in order to perform needed actions, including resources to undertake changes to current system architecture. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. G. Implementing system architecture In this process one should implement the system architecture as defined in the defining system architecture process. The output of this process is the implemented system architecture. Examples of implementing system architecture include the installation of new software, hardware and/or policies which will permit the remainder of the readiness processes to be instantiated across the organisation. The following implementation guidelines have been identified for that all implementations are first tested in a test environment, prior to impelemnting any solution into a production environment information system. All testing results should be documented. All information system users should be timely informed on changes and any impact on the use of the system. At this point one should also make sure that actions to be taken are in line One should make sure this process. system and information the is and storage collection, pre-incident this process with existing organisational policies and jurisdiction. Also, one should make sure that all needed autorizations are in place. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. Implementing manipulation of data representing possible digital evidence In this process one should implement pre-incident collection, storage and manipulation of data representing possible digital evidence, as defined in the planning pre-incident collection, storage and manipulation of data representing possible digital evidence process. The output of the implemented pre-incident collection, storage and manipulation of data representing possible digital evidence. Examples of pre-incident collection, storage and manipulation of data representing possible digital evidence include the implementation of logging software and hardware, with time stamping and digital signature mechanisms in place, or the implementation of customized software to collect the data of importance (i.e. system usage data). The following implementation guidelines have been identified for that all implementations are first tested in a test environment, prior to impelemnting any solution into a production environment information system. All testing results should be documented. As stated for the previous process, at this point one should make sure that actions to be taken are in line with existing organisational policies and jurisdiction and that all needed autorizations are in place. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. this process. One should make sure H. Implementing pre-incident analysis of data representing possible digital evidence In this process one should implement pre-incident analyses of data representing possible digital evidence, as defined in the planning pre-incident analyses of data representing possible digital evidence process. The output of this process is the implemented pre-incident analyses of data representing possible digital evidence. Examples of pre-incident analyses of data representing possible digital evidence include the implementation of change-tracking intrusion detection/prevention software and/or anti-virus software. The following implementation guidelines have been identified for that all implementations are first tested in a test environment, prior to impelemnting any solution into a production environment information system. All testing results should be documented. As this process entails implementation of an external system to the target information system, one must make sure that interface introduced between these two will not influence this process. One should make software, sure functionalities of the target information system. Further interface activities should be closely monitored in order to detect any deviations functionality and performance of the target information system. All activities should be documented as specified during the documentation process, which is a parallel process as shown in Figure 1. from desired is detected via intrusion detection Implementing incident detection I. In this process one should implement the actions defined in the planning incident detection process. The implementation of incident detection depends also on and receives input from the implementing pre-incident analyses of data representing possible digital evidence process, as detection occurs based on the analyses performed. During the implementing incident detection process, detection of an incident occurs according to the rules defined during planning incident detection process. Also, during the implementing incident detection process, one should decide on which data about the incident should be passed on to the rest of digital forensic investigation process. Examples of incident detection can be if change tracking software detects changes in a certain archived log or if an intrusion system. Requirements for an event to be declared an incident requiring digital forensic investigation would depend on policies of organisation and can not be prescribed by this paper. This process represents an interface to the rest of the digital forensic investigation process. This process is an overlap between readiness processes and an investigation itself. The reason for overlap is that the digital forensic investigation can not start until there is an incident dete",,
"ted. Th""",Scientific Journal,0,
Support for Clinton-Style AWB at All-Time Low in U.S.,"Australia Admits Gun Control Failure! Offers Amnesty to ‚ÄúOffenders‚Äù 
Gallup has been asking Americans about their opinion on the assault weapons ban since 1996 ‚Äî two years after President Bill Clinton signed a federal assault weapons ban. That year, 57 percent of those polled said they favored the ban. 
But in 2006, the poll found for the first time that more Americans opposed the ban than favored it, and that opposition has typically increased over the past 10 years. 
What was most interesting about the results, however, was that the sharp uptick in those who opposed the ban came from Democrats, 50 percent of whom said they would not support a ban on assault weapons.   
Less surprisingly, 75 percent of those who identified as Republican said they opposed the ban. 
These numbers are detrimental to the progressive agenda of Obama and Clinton, who have made gun control , particularly a ban on assault weapons, a key issue. 
The truth is that most Americans know that our constitutional right to bear arms is imperative to our continued prosperity as a nation and that an infringement upon that right is dangerous.",conservativetribune.com,1
Fox News Calls for Special Prosecutor to Investigate Hillary Clinton,"By Hrafnkell Haraldsson on Sat, Oct 29th, 2016 at 7:56 am ""If she gets elected on November 8th, this isn't going away. So what needs to happen, special prosecutor? She can't investigate herself as the president-elect""   
President Barack Obama did not have to do anything wrong for Republicans to want to throw him in jail, or to impeach him. Hillary Clinton doesn‚Äôt have to have done anything wrong either, apparently, for Congress to appoint a special prosecutor to take her down once elected ‚Äì because a bunch of emails had nothing to do with her . 
At least, that and completely ignoring the facts, was the theme on Fox News last night, though to be fair, other mainstream media outlets weren‚Äôt much better yesterday. 
Megyn Kelly, fresh from a stint of pretending to be a feminist in a face-off with Newt Gingrich, suggested to Chairman of the House Judiciary Committee Bob Goodlatte (R-WV) that given the FBI revelations of a complete lack of evidence that Hillary Clinton has done anything wrong, ‚Äúwhat needs to happen‚Äù is a special prosecutor. 
MEGYN KELLY : Let me ask you this. If she gets elected on November 8th, this isn‚Äôt going away. So what needs to happen, [a] special prosecutor? She can‚Äôt investigate herself as the president-elect. BOB GOODLATTE : Well, first of all, we have said from the outset that the House Judiciary Committee would follow the truth wherever it leads, and we have done that throughout the summer, asking a number of questions about how this investigation by the FBI was being conducted. We will continue to do that. We don‚Äôt know what‚Äôs going to happen now. We don‚Äôt know what‚Äôs going to happen after the election. So it‚Äôs pure speculation what will happen afterwards. A special prosecutor, remember, is appointed by the Attorney General of the United States. So, you have to decide whether that‚Äôs going to be a fairer investigation than the one being conducted by the FBI under the direction of Director Comey right now. And the Judiciary Committee will certainly stay engaged in this and continue our investigation. 
Democracy is a fragile thing. It is to Goodlatte‚Äôs credit that he declined the call to draw up articles of impeachment in 2014 and that he cautioned Megyn Kelly that a special prosecutor is appointed not by the House but by the Attorney General. However, the GOP-led House has been far too engaged already in undermining our democracy. 
What holds this country together is an idea and an agreement among all parties that it works. Republicans have decided it only works if they‚Äôre in charge, and this has brought us to the point where we threaten to jail our adversaries, appoint special prosecutors if they win, or vote to impeach them outright, as Trump‚Äôs pet snake at CNN, Jeffrey Lord, called for last night : 
Two points, Anderson. One, with 11 days left, this makes Donald Trump‚Äôs argument exactly to the point here, you sum up the argument in terms of her judgment, her paranoia and all of the rest. The second thing, Anderson is, and this is the really disturbing thing, let‚Äôs just say for the sake of the argument she wins the election, this isn‚Äôt going to stop. There will be a move to impeach her the moment her hand comes down from that Bible. This is going to go on and on and on and on and frankly, I mean that‚Äôs something the American people need to consider now. 
It is not difficult to see where this will lead. Hillary Clinton has committed no crimes. Far from being a miscarriage of justice, though it is that as well, this is a miscarriage of reason and sanity. 
This sort of talk is just further evidence that Donald Trump comes from the same vile brew as Paul Ryan and Mitch McConnell and the rest of the party. During the McCarthy era, the House took it upon itself to be both judge and jury. Last night, it was Fox News egging the House on for more of the same. 
This sort of talk is deplorable, it is reprehensible, and it is dangerous ‚Äì and strikes at the very heart of our democracy.",politicususa.com,1
Penn State Fined $2.4 Million For Handling Of Sandusky Case - The Onion - America's Finest News Source,"Nation Puts 2016 Election Into Perspective By Reminding Itself Some Species Of Sea Turtles Get Eaten By Birds Just Seconds After They Hatch WASHINGTON‚ÄîSaying they felt anxious and overwhelmed just days before heading to the polls to decide a historically fraught presidential race, Americans throughout the country reportedly took a moment Thursday to put the 2016 election into perspective by reminding themselves that some species of sea turtles are eaten by birds just seconds after they hatch. Cleveland Indians Worried Team Cursed After Building Franchise On Old Native American Stereotype CLEVELAND‚ÄîHaving watched in horror as their team crumbled after a 3-1 World Series lead, members of the Cleveland Indians expressed concern Thursday that the organization has been cursed for building their franchise on an incredibly old Native American stereotype. Report: Election Day Most Americans‚Äô Only Time In 2016 Being In Same Room With Person Supporting Other Candidate WASHINGTON‚ÄîAccording to a report released Thursday by the Pew Research Center, Election Day 2016 will, for the majority of Americans, mark the only time this year they will occupy the same room as a person who supports a different presidential candidate. Nurse Reminds Elderly Man She‚Äôs Just Down The Hall If He Starts To Die DES PLAINES, IL‚ÄîAssuring him that she‚Äôd be at his side in a jiffy, local nurse Wendy Kaufman reminded an elderly resident at the Briarwood Assisted Living Community that she was just down the hall if he started to die, sources reported Tuesday. ",theonion.com,1
Nat Geo‚Äôs iconic ‚ÄòAfghan Girl‚Äô arrested for false documents in Pakistan,"Nat Geo‚Äôs iconic ‚ÄòAfghan Girl‚Äô arrested for false documents in Pakistan Published time: 26 Oct, 2016 19:10 Get short URL ¬© Tim Chong / Reuters The woman behind National Geographic photographer Steve McCurry‚Äôs famous ‚ÄòAfghan Girl‚Äô portrait has been arrested in Peshawar, Pakistan for possessing falsified documents. 
Pakistan‚Äôs Federal Investigation Agency arrested Sharbat Gula at her home on Wednesday for having a forged Computerised National Identity Card. 
Gula‚Äôs ID cards for both Pakistan and Afghanistan were taken from her. She faces up to 14 years in prison if convicted. 
The arrests coincides with a Pakistani crackdown on undocumented immigrants. URGENT: Britain stops taking child #refugees from Calais ‚ÄòJungle‚Äô at request of French police https://t.co/sI3LYeEZTg pic.twitter.com/Lec9tVMGEu ‚Äî RT (@RT_com) October 24, 2016 
Gula swept to fame after her image appeared on a 1985 cover of National Geographic. It was taken by McCurry at the Nasir Bagh refugee camp near Peshawar in 1984 when she was about 12-years-old. 
Gula‚Äôs striking green eyes has seen her being dubbed the Afghan Mona Lisa, and her photograph is ranked in the top 5 of National Geographic‚Äôs most iconic images. Steve McCurry's famed Afghan Girl on the cover of @NatGeo 's 1984 magazine is on a judicial remand for a fake Pak-ID with the FIA #Peshawar . pic.twitter.com/3szkw1KesG ‚Äî Iftikhar Firdous (@IftikharFirdous) October 26, 2016 
Gula‚Äôs identity was a mystery for 18 years, until McCurry finally tracked her down in 2002, after returning to the region more than 10 times hoping to find her. 
Gula‚Äôs parents were killed in Afghanistan during the Soviet-Afghan war. 
In 1992, Gula left Pakistan and returned to Afghanistan where she married a baker and had three children in a village in Eastern Afghanistan. National Geographic was able to verify Gula was the famous girl by comparing her moles and scar, as well as using iris scanning technology. 
""Her eyes are as haunting now as they were then,"" McCurry said at the time. ""She remembered me, primarily because she had never been photographed before I made the image of her in 1984, or since then."" Mentally ill convict to be executed after court rules schizophrenia 'not a mental disorder' in #Pakistan https://t.co/UZZSogP6QS pic.twitter.com/UqfLREcAKM ‚Äî RT (@RT_com) October 22, 2016 
Last year, Pakistani media reported the National Database and Registration Authority had cancelled the cards of Gula and two men said to be her sons because they had been illegally issued. Three officials are wanted for issuing her card and have been missing since it was first reported. 
Gula applied for an identity card in 2014, using the name Sharbat Bibi. It isn‚Äôt clear why Gula returned to Pakistan after living in Afghanistan. 
Pakistan‚Äôs investigation into fake ID cards has found more than 60,000 cases of non-nationals holding cards. The country is home to 2.5 million Afghan refugees.",rt.com,1
Most Likely Candidates For Trump‚Äôs Cabinet - The Onion - America's Finest News Source,"Hillary Clinton Waiting In Wings Of Stage Since 6 A.M. For DNC Speech PHILADELPHIA‚ÄîSaying she arrived hours before any of the members of the production crew, sources confirmed Thursday that presidential nominee Hillary Clinton has been waiting in the wings of the Wells Fargo Center stage since six o‚Äôclock this morning to deliver her speech at the Democratic National Convention. Depressed, Butter-Covered Tom Vilsack Enters Sixth Day Of Corn Bender After Losing VP Spot WASHINGTON‚ÄîSaying she has grown increasingly concerned about her husband‚Äôs mental and physical well-being since last Friday, Christie Vilsack, the wife of Agriculture Secretary Tom Vilsack, told reporters Thursday that the despondent, butter-covered cabinet member has entered the sixth day of a destructive corn bender after being passed over for the Democratic vice presidential spot. DNC Speech: ‚ÄòI Am Proud To Say I Walked In On Bill And Hillary Having Sex‚Äô A friend of the Clinton family describes a Hillary who America never gets to see: the one he saw having sex. Trump Sick And Tired Of Mainstream Media Always Trying To Put His Words Into Some Sort Of Context NEW YORK‚ÄîEmphasizing that the practice was just more evidence of journalists‚Äô bias against him, Republican presidential nominee Donald Trump stated Thursday that he was sick and tired of the mainstream media always attempting to place his words into some kind of context. Who‚Äôs Speaking At The DNC: Day 4 Here is a guide to the major speakers who will be addressing attendees on the final night of the 2016 Democratic National Convention Bound, Gagged Joaquin Castro Horrified By What His Identical Twin Brother Might Be Doing Out On DNC Floor PHILADELPHIA‚ÄîStruggling to free himself from the tightly wound lengths of rope binding his wrists and ankles together, bruised and gagged Texas congressman Joaquin Castro was reportedly horrified by what his identical twin brother, Secretary of Housing and Urban Development Julian Castro, might be out doing on the floor of the DNC Thursday. Obama: ‚ÄòHillary Will Fight To Protect My Legacy, Even The Truly Detestable Parts‚Äô PHILADELPHIA‚ÄîEmphasizing the former secretary of state‚Äôs competence and tenacity during his Democratic National Convention address Wednesday night, President Barack Obama praised Hillary Clinton as someone who would work tirelessly to defend and advance the legacy he had built, even the ‚Äútruly repugnant parts.‚Äù Tim Kaine Clearly Tuning Out In Middle Of Boring Vice Presidential Acceptance Speech PHILADELPHIA‚ÄîDescribing the look of total disinterest on his face and noting how he kept peering down at his watch as the speech progressed, sources at the Democratic National Convention said that Virginia senator Tim Kaine clearly began tuning out partway through the boring vice presidential acceptance address Wednesday night. Cannon Overshoots Tim Kaine Across Wells Fargo Center PHILADELPHIA‚ÄîNoting that the vice presidential nominee had been launched nearly 100 feet into the air during his entrance into the Democratic National Convention Wednesday night, sources reported that the cannon at the back of the Wells Fargo Center had accidentally overshot Tim Kaine across the arena, sending him crashing to the stage several dozen feet beyond the erected safety net. Biden Regales DNC With Story Of ‚Äô80s Girl Band Vixen Breaking Hard Rock‚Äôs Glass Ceiling PHILADELPHIA‚ÄîDevoting a large portion of his speech to the ‚Äúpioneering, stiffy-inducing‚Äù all-female quartet, Vice President Joe Biden regaled the Democratic National Convention Wednesday night with the rousing story of the metal band Vixen breaking hard rock‚Äôs glass ceiling in the late 1980s. ",theonion.com,1
"Dilbert creator Scott Adams endorses Trump, says Hillary is a bully","Dilbert creator Scott Adams endorses Trump, says Hillary is a bully 
Monday, October 31, 2016 by: Natural News Editors (NaturalNews) I've been trying to figure out what common trait binds Clinton supporters together. As far as I can tell, the most unifying characteristic is a willingness to bully in all its forms.If you have a Trump sign in your lawn, they will steal it.If you have a Trump bumper sticker, they will deface your car.if you speak of Trump at work you could get fired.(Article republished from Dilbert.com )On social media, almost every message I get from a Clinton supporter is a bullying type of message. They insult. They try to shame. They label. And obviously they threaten my livelihood.We know from Project Veritas that Clinton supporters tried to incite violence at Trump rallies. The media downplays it.We also know Clinton's side hired paid trolls to bully online. You don't hear much about that.Yesterday, by no coincidence, Huffington Post, Salon, and Daily Kos all published similar-sounding hit pieces on me, presumably to lower my influence. (That reason, plus jealousy, are the only reasons writers write about other writers.)Joe Biden said he wanted to take Trump behind the bleachers and beat him up. No one on Clinton's side disavowed that call to violence because, I assume, they consider it justified hyperbole.Team Clinton has succeeded in perpetuating one of the greatest evils I have seen in my lifetime. Her side has branded Trump supporters (40%+ of voters) as Nazis, sexists, homophobes, racists, and a few other fighting words. Their argument is built on confirmation bias and persuasion. But facts don't matter because facts never matter in politics. What matters is that Clinton's framing of Trump provides moral cover for any bullying behavior online or in person. No one can be a bad person for opposing Hitler, right?Some Trump supporters online have suggested that people who intend to vote for Trump should wear their Trump hats on election day. That is a dangerous idea, and I strongly discourage it. There would be riots in the streets because we already know the bullies would attack. But on election day, inviting those attacks is an extra-dangerous idea. Violence is bad on any day, but on election day, Republicans are far more likely to unholster in an effort to protect their voting rights. Things will get wet fast.Yes, yes, I realize Trump supporters say bad things about Clinton supporters too. I don't defend the bad apples on either side. I'll just point out that Trump's message is about uniting all Americans under one flag. The Clinton message is that some Americans are good people and the other 40% are some form of deplorables, deserving of shame, vandalism, punishing taxation, and violence. She has literally turned Americans on each other. It is hard for me to imagine a worse thing for a presidential candidate to do.I'll say that again.As far as I can tell, the worst thing a presidential candidate can do is turn Americans against each other. Clinton is doing that, intentionally. Intentionally. As I often say, I don't know who has the best policies. I don't know the best way to fight ISIS and I don't know how to fix healthcare or trade deals. I don't know which tax policies are best to lift the economy. I don't know the best way to handle any of that stuff. (And neither do you.) But I do have a bad reaction to bullies. And I've reached my limit.I hope you have too. Therefore... I endorse Donald Trump for President of the United States because I oppose bullying in all its forms. I don't defend Trump's personal life. Neither Trump nor Clinton are role models for our children. Let's call that a tie, at worst.The bullies are welcome to drown in their own bile while those of us who want a better world do what we've been doing for hundreds of years: Work to make it better while others complain about how we're doing it.Today I put Trump's odds of winning in a landslide back to 98%. Remember, I told you a few weeks ago that Trump couldn't win unless ""something changed.""Something just changed.",naturalnews.com,1
ATE: Anti-malware Technique Evaluator,"",,
" which """,Scientific Journal,0,
Claimed vs Observed Information Disclosure on Social Networking Sites ,"INTRODUCTION Social networking sites like Facebook, Twitter, LinkedIn and the like have given people the ability to communicate and share information with almost anyone, anywhere and at any time. An important aspect of social networking sites is the fact that users can publish status updates, upload photos, audio files and video files for their friends to view. However, users often publish information about themselves that is extremely personal or sensitive [1]. Although privacy settings can be used to protect private information, it is often found that users do not use these privacy settings [1], [2]. For the purpose of this study focus is put on Facebook. Facebook is currently the most popular social networking site and has the largest number of users. As of January 2011, Facebook had more than 600 million active users [3]. This number increased in the same year to 800 million users [4]. This makes Facebook the most popular social networking site and more popular than Google as the most visited site on the Internet [5]. Facebook is the biggest photo site, even bigger than Flickr or Photobucket [6]. Facebook started as a social networking site of Harvard University in 2004 created by Mark Zuckerberg. As with all social networking sites, participation requires a user to create a profile that lists the demographics of the user, a profile photo and requires that a user be above 13 years old. In addition, after users are signed on they can share different information types that can be updated at any time. Contributing to the success of Facebook is the developer platform which was launched in 2007. The developer platform enabled the development of applications that can be used directly or indirectly to interact with the application servers through Facebook [7]. These applications have been labeled to increase the risk of data harvesting [8]. The ease of information storage on the Internet makes it possible for user information to be harvested later. This information may then be used in situations where it can be harmful to the owner [9]. Scholars, the media and many other privacy stakeholders have raised concerns regarding the risks associated with the disclosure of personal information on social networking sites [1], [10–12]. Research on internet users has shown that users claim that privacy is important to them, but do not act likewise [13], [14]. This has also been debated to be the same case on social networking sites users [15], [16], [17] [18]. All these studies focused on the privacy concerns versus observed behavior. This gap between privacy concerns and observed behavior is termed as “privacy paradox” [19]. Other well-known studies in this field include [1], [10], [11], these studies focused on the amount and type of information users disclosed and on the reported privacy concerns or preferences. This current study puts focus on the claimed information disclosure versus observed information disclosure. According to our knowledge this is the first study to focus on the users’ claimed behavior, rather than claimed privacy preferences as previous studies have. This research will add to the existing body of knowledge within privacy studies, predominantly within the rapidly growing area of studying human behavior on social networking sites. This study formulated the following research question: Is there a gap between claimed information disclosure and observed information disclosure on Facebook? The following sections will discuss the methods employed in arriving at the results of this study. The results will be discussed, followed by the discussion of future work and the conclusion. First we start by discussing the participants of the study. II. RESEARCH METHODS The for research methods chosen this study were questionnaires and observations. Questionnaires were used to understand the claimed information disclosure on Facebook. Requests to respond to the questionnaire were made via e-mail to all the recruited participants. Two participants failed to receive the invitation because they provided invalid e-mail addresses. Of the 167 requests made, 100 participants responded to the questionnaire, giving a 61% response rate. To examine the observed information disclosure, observations were made on the users’ public Facebook profiles. A total of 131 participants were observed at a Facebook component-level. We approached this study on a component base. Participants were asked to indicate all the Facebook components they had disclosed on their profiles. During observations all those components were observed to identify the gap for each component. All data collection took place during the month of September 2011. During data collection we adopted the classifications of Facebook components that Nosko, Wood and Molema described in 2010 [20]. They categorized Facebook components into the following three categories: 1. Personal identifiable information: This to hometown, gender, birthday, birth year, e-mail address, address, profile photo, photos, and personal website. This is any information about an individual that can be used to distinguish or trace an individual‘s identity or linkable to an individual [21]. refers 2. Sensitive personal information: This refers to employer information, secondary school, university, current location, mobile number, friends’ list and relationship status. This is any personally revealing or sensitive information that could be used to locate an individual and could be used to threaten or harm the owner. 3. Potentially stigmatizing information: This refers to the wall, religious status, political views, people who inspire me, favorite quotations, music, books, movies, television show, games, sport, activities, interests, gender interested in dating, language and personal description. This is any information that could result in stigmatization within the society. Our methodology differs from the previous studies in the sense that we tested our research question for each Facebook component. III. PARTICIPANTS recruitment 167 participants Participants in this study were freshmen enrolled for IT at Nelson Mandela Metropolitan University for the academic year of 2011. During indicated willingness to participate in the study by signing a participation form. Ethical clearance was obtained before data collection and participation in the study was voluntary. Fig. 1 shows the classification of participants at different stages of the study. The participants that took part in each stage of the study are explained on the relevant sections below. The gray area depicts the probability of participants that filled the questionnaire and were also observed. Possibility is that some participants that completed the questionnaire were not observed and some that did not complete the questionnaire were observed. Table 2 gives the groupings of the participants during observations. The table explains all the information disclosed by each group and the number of participants that were in that group. A. Questionnaire Figure 1. Participants Figure 2. IV. RESULTS Males dominated the sample, with 75% while females registered 25%. A variety of age groups were represented in the sample, ages 18 – 21 dominated the sample with 91%, with ages 22 – 25 and 26 – 30 on 7% and 2% respectively. The majority of the participants (63%) have used Facebook for more than a year and 37% have used Facebook for less than a year. The full results of the claimed information disclosure are given in Table 1. B. Observations Of the 167 participants recruited 35 participants could not be found on Facebook and one participant was deemed invalid as she was already a Facebook friend of the researcher. As we could not determine the behavior regarding specific Facebook components of the 21% that could not be found, they were disregarded for the rest of the observation. However, if they were using a pseudonym they could still be exposing the detail components to relative strangers who might still identify them. Table 1 gives the full results of the claimed information disclosure versus observed information disclosure. The table provides the list of Facebook items examined on the first column. The second and the third columns show the claimed information disclosure and the observed information disclosure respectively. The last column gives the difference between the claimed information disclosure and the observed information disclosure. As an example, for hometown, 92% claimed that they have disclosed it on their profiles, while 61% were observed to have disclosed it on their profiles. Therefore the difference between claimed disclosure and observed disclosure is 31%. The information in Table 1 is represented in three figures according to the difference between the claimed information disclosure and observed information disclosure. TABLE I. CLAIMED BEHAVIOR VERSUS OBSERVED BEHAVIOR TABLE II. CLASSIFICATION OF PARTICIPANTS Personal identifiable information Hometown Gender Birthday Birth year E-mail address Address Profile photo Personal website Employer Secondary school University Current location Mobile number Friends list Relationship status Religious status Political views People who inspire you Favourite quotations Favourite music Favourite books Favourite TV show Favourite games Favourite sport Activities Interests Gender interested in dating Language Personal description Claimed disclosure Observed Difference Status (N = 100) disclosure (N = between behaviors 131) Personal identifiable information Sensitive personal information Potential stigmatizing information Number of participants 92% 99% 90% 68% 74% 26% 100% 13% 61% 91% 19% 11% 5% 2% 97% 2% Sensitive personal information 18% 77% 94% 71% 60% 71% 59% 13% 68% 77% 57% 7% 87% 21% Potential stigmatizing information 61% 42% 31% 51% 76% 55% 74% 52% 53% 58% 68% 55% 72% 29% 23% 10% 15% 16% 59% 39% 56% 35% 19% 41% 37% 39% 25% 27% 31% 8% 71% 57% 69% 24% 3% 11% 5% 9% 17% 14% 53% -16% 38% 38% 32% 16% 35% 17% 16% 18% 17% 34% 17% 31% 16% 47% 2% Invalid & not found Completely open N/A All Somewhat open Hometown, gender, profile photo Somewhat closed Gender, profile photo N/A All Secondary school, university, friends list Secondary school, university, friend’s list N/A All All N/A Completely closed N/A N/A N/A 21% 12% 47% 13% 7% Fig. 2 shows the difference between claimed information disclosure and observed information disclosure for components that had more than 50% difference. All the components in this figure show that users acted more securely than they claimed. The observed information disclosure is less than claimed information disclosure. This is counterintuitive and contrary to popular studies. In the case of reported privacy behavior versus observed privacy behavior, most studies revealed that users act less securely than their privacy preferences. However, the result of this group suggests otherwise. A discussion on the reasons that might cause this is discussed with the results of the study on the discussion section below. Figure 1.Components with more than 50% difference between claimed and observed disclosure Fig. 3 shows the difference between claimed information disclosure and observed information disclosure for components that had a difference between 31% - 50%. All the components in this group show that users acted more securely than they claimed. Again, the result of this group gives a different finding than previous studies. case of reported privacy concern and observed privacy behavior [15–17], there is a gap between claimed information disclosure and observed information disclosure. The gap was found for all Facebook components except for the friends list. All the previous studies have argued that users of social networking sites report that privacy is important to them but do not act according to their privacy preferences. However, in the case of claimed behavior this study shows that users act more securely than what they claim. This is counterintuitive to popular views brought about the forerunner studies in this field. Keep in mind that this study examined a different variable than the previous studies. This counterintuitive finding can be attributed to a number of reasons and these are discussed below. The first reason might be that this study is examining a different variable than the previous studies. The methodology used in this study is not the same as the previous studies in the sense that this study examined the gap for each Facebook component. This study extends that the gap is not only on privacy concerns versus observed privacy, but also found on claimed information disclosure versus observed information disclosure. Research in other fields like organizational behavior research has found that users are bias in self-report. They tend to under-report behaviors deemed researchers or other observers, and they tend to over-report behaviors viewed as appropriate [22]. We do not rule out the possibility of this theory in this study as well. inappropriate by Another reason would be that the users are not really aware of who currently can view what information on their profiles. This maybe because users have not revisited their privacy settings after all the changes Facebook has done on the site’s privacy settings. Therefore, users might be unaware of their disclosure. Research over the years have argued that most users rarely alter default settings [1], [23–25]. This might also be the case also for the participants of this study. Another cause might be that the users might be sharing their information with friends and not with strangers. This study was approached from a stranger point of view. Therefore we could only observe what a normal stranger would see. We would have wanted to examine the gap from a Facebook friend point of view, but the ethical clearance we got from the University only limited us to stand on a stranger point of view. Table 2 shows that only 12% of the participants we were able to observe all possible Facebook components on their profiles. 47% of the participants had somewhat open profiles while the rest yielded little to no information. From all the participants it is interesting to note that 21% of participants were not disclosing information that could easily be associated with by a stranger. It is assumed that the participants maybe using pseudonym names and that maybe another form of privacy protection mechanism. Additionally, both the number of the participants that returned the questionnaire and the participants that were observed are less than the number of participants recruited. This makes it a Figure 2. Components with 31% - 50% difference between claimed and observed disclosure Fig. 4 shows the difference between claimed information disclosure and observed information disclosure for components that had a difference that is less than 30%. The results on this group show that users acted more securely than they claimed with the exception of friends list. The friends list was the only Facebook component where the observed information disclosure scored more than claimed information disclosure. The following section will discuss the results of the study. Figure 3. Components with less than 31% difference between claimed and observed disclosure V. DISCUSSION This study conducted an exploration in which we contrasted self-reported/claimed information disclosure against observed information disclosure of Facebook users. Our preliminary research question was to examine if there is a gap between claimed information disclosure on Facebook. This study discovered that, as it is in the information disclosure and observed possibility that some participants that were observed did not fill the questionnaire and some that filled the questionnaire were not observed. Again, our ethical clearance did not allow us to collect names during the questionnaire stage and we could not verify that the participants we observed are exactly the same as the ones that administered recruited participants filled the questionnaire. Both the reasons for filling the questionnaire and not filling the questionnaire are unknown to the researchers. the questionnaire. Again, not all Lastly, we did not specify to the participants the perspective we were questioning their information disclosure from. The participants could have misunderstood the question to think that we were asking from a Facebook friend’s point of view. The following section concludes the study and also discusses future work. VI. CONCLUSION Social networking sites provide users with a platform to create maintain online social networks and share information. Along with the benefits of social networking sites there are threats, risks and privacy concerns with sharing information with large amounts of people. The privacy implications on these sites are the cause scholars put effort to carefully study these sites. The results of this study contribute to the body of knowledge within the rapidly growing area of social networking sites. There are many reasons for the discrepancy between claimed information disclosure information disclosure. This unpredictable behavior adds to the difficulty that developers of social networking sites have in catering to the privacy needs and concerns of all their users and observed Not only does the study present a counterintuitive result, but it examines a different variable using a different methodology than previous studies. Again, future study will look to compare the individual’s questionnaire results and the observations. We could not do this because the questionnaire was on anonymous basis In evaluating the results of this study, certain limitations should be taken into consideration. Firstly, this only observed the participants from a stranger point of view. Future work will look to observe participants from a Facebook friend point of view to determine if the gap will still be present or decreased. This study could not determine if the participants that filled the questionnaire were the same as the participants observed. Future study will look to observe the same participants as the ones that filled the questionnaire. ",Scientific Journal,0
Dylann Roof Himself Rejects Best Defense Against Execution,"  pages into the   journal found in Dylann S. Roof‚Äôs car  ‚Äî   after the assertions of black inferiority, the lamentations over white powerlessness, the longing for a race war  ‚Äî   comes an incongruous declaration. ‚ÄúI want state that I am morally opposed to psychology,‚Äù wrote the young white supremacist who would murder nine black worshipers at Emanuel A. M. E. Church in Charleston, S. C. in June 2015. ‚ÄúIt is a Jewish invention, and does nothing but invent diseases and tell people they have problems when they dont. ‚Äù Mr. Roof, who plans to represent himself when the penalty phase of his federal capital trial begins on Tuesday, apparently is devoted enough to that proposition (or delusion, as some maintain) to stake his life on it. Although a defense based on his psychological capacity might be his best opportunity to avoid execution, he seems steadfastly committed to preventing any public examination of his mental state or background. ‚ÄúI will not be calling mental health experts or presenting mental health evidence,‚Äù he wrote to Judge Richard M. Gergel of Federal District Court on Dec. 16, a day after a jury took only two hours to find him guilty of 33 counts, including hate crimes resulting in death, obstruction of religion and firearms violations. At a hearing on Wednesday, Mr. Roof told the judge that he planned to make an opening statement but not call witnesses or present evidence on his behalf. The testimony presented by prosecutors during the guilt phase of Mr. Roof‚Äôs trial detailed with gruesome precision how he had plotted and executed the massacre during a Wednesday night Bible study in the church‚Äôs fellowship hall. It was less satisfying in revealing why he had done it. With his choice to sideline his legal team and represent himself, the second phase  ‚Äî   when the same jury of nine whites and three blacks will decide whether to sentence him to death or to life in prison  ‚Äî   may prove little different. Death penalty experts said it was exceedingly rare for capital defendants to represent themselves after allowing lawyers to handle the initial part of a case. Mr. Roof, who also faces a death penalty trial in state court, has not publicly explained his reasoning. But legal filings strongly suggest a split with his   defenders about whether to argue that his rampage resulted from mental illness. Mr. Roof‚Äôs lead lawyer, David I. Bruck, tried repeatedly to plant that notion during the guilt phase, knowing it might be his only chance. Because evidence of mitigating factors is supposed to be reserved for the penalty phase, Judge Gergel allowed him little leeway. In his closing argument, while acknowledging Mr. Roof‚Äôs guilt, Mr. Bruck managed to tell the jury that Mr. Roof subscribed to ‚Äúthe mad idea that he can make things better by massacring the most virtuous, kind and gentle people he could ever have found. ‚Äù Mr. Bruck seeded his speech with words like ‚Äúabnormal,‚Äù ‚Äúirrationality,‚Äù ‚Äúsenselessness,‚Äù ‚Äúdelusional,‚Äù ‚Äúobsession‚Äù and ‚Äúperseveration,‚Äù a psychiatric term referring to the uncontrollable repetition of a particular response. Mr. Bruck, one of the country‚Äôs most experienced death penalty litigators, portrayed his    client as a loner whose most meaningful relationship seemed to be with his cat who staged hundreds of photographs of himself with no sign of friends whose racial hatred was ignited by internet searches and not personal experience who could not pinpoint during his confession to the F. B. I. how many he had killed, how long he had spent at the church or even what month it was who had no escape plan and left suicide notes to his parents. ‚ÄúThere was something in him that made him feel that he had to do it,‚Äù Mr. Bruck said, ‚Äúand that is as much as he knows. ‚Äù After receiving the results of a psychiatric examination in November, Judge Gergel found Mr. Roof competent to stand trial  ‚Äî   meaning that he was capable of understanding the proceedings and assisting in his defense. At Mr. Bruck‚Äôs request, the judge scheduled a second competency hearing for Monday, but he signaled last week that he saw no reason to delay the penalty phase. The judge has repeatedly warned Mr. Roof against representing himself, including immediately after the verdicts, when he called it ‚Äúa bad decision‚Äù and urged him to ‚Äúfully appreciate the implications. ‚Äù The warnings have had no discernible effect on Mr. Roof, who has until Tuesday to reverse his decision to relegate his lawyers to standby counsel. That status allows Mr. Bruck and his team to offer guidance, but not to question witnesses or make objections. Prosecutors plan a procession of grief, perhaps calling dozens of members of victims‚Äô families to testify about the impact of the killings. The prosecutors are also likely to   the considerable evidence of Mr. Roof‚Äôs premeditation and clearly articulated racial intent. Death penalty experts said the absence of mental health evidence to mitigate those aggravating factors could be decisive. ‚ÄúIf the jury views Roof as evil and having made a knowing, intelligent choice to kill these innocent, churchgoing people in order to foment racial hatred, they are much more likely to impose the death penalty than if they believe him to be a young and severely mentally ill person who acted under delusional racist beliefs,‚Äù said Robert Dunham, executive director of the Death Penalty Information Center, a research group. It would take only one holdout on the jury, which consists of 10 women and two men, to spare Mr. Roof from lethal injection. Judge Gergel has ruled that the jurors can be told that prosecutors had rejected Mr. Roof‚Äôs offer, through Mr. Bruck, to plead guilty in exchange for a life sentence. Many in Charleston were relieved by Mr. Roof‚Äôs conviction in light of the mistrial that had been declared 10 days earlier in the state murder prosecution of Michael T. Slager, the white North Charleston policeman whose fatal shooting of a black motorist in 2015 was captured on video. Even those who oppose the death penalty on moral grounds, like the Rev. Joseph A. Darby, a presiding elder for the African Methodist Episcopal Church in Charleston, said it would seem bewildering for Mr. Roof to escape capital punishment. ‚ÄúThat could very well be the end of the death penalty in America, because if there was ever justification for killing anybody, this is the case,‚Äù Mr. Darby said. There is no consensus among members of the victims‚Äô families about Mr. Roof‚Äôs fate. When the Justice Department elected in May to seek the death penalty, it acted against the wishes of many and of the two women he had spared. Five relatives offered Mr. Roof a measure of forgiveness at a remarkable bond hearing two days after the shootings. But by law, those who testify now are prohibited from telling the jury what penalty they think he should receive. ‚ÄúIt‚Äôs going to be extremely emotional, powerful testimony,‚Äù said John H. Blume, a death penalty expert who teaches at Cornell Law School, ‚Äúand that emotion could implicitly and misleadingly indicate to the jury that some of these people want the death penalty when it‚Äôs not the case. ‚Äù If Mr. Roof is sentenced to death, it will be the first time a jury has done so in a prosecution involving the federal hate crimes law, according to experts on capital cases. That statute, which was broadened in 2009, does not carry a potential death sentence, but Mr. Roof was also convicted of other crimes that do. A death sentence most likely would give way to a yearslong series of appeals (in which Mr. Roof could not represent himself). Among the issues could be the composition of the jury, given that Mr. Roof acted rather passively as his own lawyer when it was selected the withholding of evidence on mental health and other mitigating factors and Mr. Roof‚Äôs competence to stand trial and to represent himself. In 2008, the Supreme Court ruled in Indiana v. Edwards that trial judges could insist on legal representation for defendants who are ‚Äúcompetent enough to stand trial but who suffer from severe mental illness to the point where they are not competent to conduct trial proceedings by themselves. ‚Äù Mr. Bruck and his team have argued in court filings that Mr. Roof, a   dropout, ‚Äúhas no right to represent himself in a capital trial, and even less so at the penalty phase. ‚Äù But in the 41 years since the Supreme Court recognized a Sixth Amendment right of   for criminal defendants, in Faretta v. California, the court has never specifically narrowed that holding for death penalty trials, despite their complexity. Some death penalty opponents hope that Mr. Roof‚Äôs defiance will prompt the appellate courts to adopt a more rigorous standard for capital defendants. ‚ÄúWhether or not they‚Äôre legally insane, there‚Äôs certainly something mentally wrong with them,‚Äù said Peter D. Greenspun, a lawyer who was ousted by a defendant, John A. Muhammad, for part of a capital murder trial for the 2002 sniper attacks in the Washington area. He added, ‚ÄúTo have a person like that make this kind of decision, it really calls into question, from a philosophical point of view, whether that person is in a position to understand their civil liberties. ‚Äù Mr. Muhammad, who ultimately reinstated Mr. Greenspun, was sentenced to death in 2003 and executed six years later. ‚ÄúIt‚Äôs something that Roof will likely regret,‚Äù Mr. Greenspun predicted of his choice to represent himself. ‚ÄúAt some point down the road, he‚Äôs going to say, ‚ÄòWhat did I do?‚Äô And there‚Äôs no going back. ‚Äù",New York Times,0
10 Reasons to drink lemon water on an empty stomach,"10 Reasons to drink lemon water on an empty stomach 
Wednesday, November 02, 2016 by: Amy Goodrich Tags: lemon water , detox , digestive health (NaturalNews) Fruit-infused water is extremely trendy right now. Celebrities such as Gwyneth Paltrow, Jennifer Aniston, and Beyonce swear by lemon water first thing in the morning. There are even entire diets based on this fruit. According to some health advocates, drinking lemon water on an empty stomach takes you one step closer to optimal health.But does swapping your morning coffee for this fashionably healthy lifestyle choice offer any significant benefit? Or is this just another health fad? Let's take a closer look at ten incredible reasons why you should start your day with a glass of lemon water. 1. Nutrient powerhouse While low in calories, lemons are loaded with health-promoting essential nutrients, including vitamin C, B-complex vitamins, calcium, iron, magnesium, potassium, and fiber. 2. Boosts immune system Vitamin C plays a major role in our immune system, and lemons are loaded with it. A strong and healthy immune system will help you keep bacterial and viral infections at bay. 3. Digestion aid The citric acid in lemon interacts with enzymes and acids involved in digestion. It stimulates digestion and the secretion of gastric juice. Furthermore, lemon juice loosens toxins in the digestive tract and may help relieve symptoms of indigestion such as bloating, burping, and heartburn. 4. Detoxifies your body Every day, we are subjected to a wide variety of chemicals and toxins present in the air we breathe, the water we drink, and the food we eat. While our body has its own cleansing systems, it is a good idea to put less stress on your already overworked body and give these cleansing mechanisms a boost. Lemon juice stimulates the liver by enhancing its enzyme function, thus helping your body to flush out more waste materials and toxins . 5. Boosts heart and brain health Thanks to its high level of potassium, lemon juice aids in the proper functioning of the nervous system. Furthermore, it has been shown to lower blood pressure which is a major risk factor for a heart attack. 6. Promotes glowing skin Lemons are packed with antioxidants. These little free radical fighting substances help to decrease blemishes and wrinkles for a rejuvenated, radiant skin. When applied to the skin, lemons can also reduce the appearance of scar tissue and age spots. 7. Weight loss aid While lemon juice might not be a weight loss miracle on its own, its high levels of pectin fibers curb the appetite and will help you to eat less at main meals. 8. Fights inflammation Lemon water possesses strong anti-inflammatory properties to soothe a sore throat or inflamed tonsils. Additionally, it removes uric acid from the joints, which is one of the main contributors to chronic pain and inflammation. 9. Boosts energy levels Lemons energize the body and fight feelings of depression and anxiety. So instead of having a coffee in the morning why not try a glass of lemon juice instead? 10. Hydrates your body After a night of hard work, your body is dehydrated and in desperate need of water. Lemon juice provides your body with the much-needed water and electrolytes (potassium, calcium, and magnesium) to rehydrate body tissue and flush out more toxins and waste materials.While lemon water seems to be the perfect drink to prevent or cure a myriad of our daily aches and pains, not all that glitters is gold. Lemon juice can be hard on teeth enamel. Therefore, it's important to dilute it with water and rinse your mouth thoroughly with filtered water after drinking lemon juice. Sources for this article include:",naturalnews.com,1
"Superbugs: Big Pharma to blame for coming death of 700,000 people annually","Superbugs: Big Pharma to blame for coming death of 700,000 people annually 
Sunday, October 30, 2016 by: Ethan A. Huff, staff writer Tags: superbugs , antibiotics abuse , Big Pharma (NaturalNews) As new guidelines set forth by the U.S. Food and Drug Administration (FDA) to curb antibiotic overuse and abuse on factory farms slowly come into effect, the drug industry is apparently setting its greedy sights elsewhere ‚Äì towards countries where restrictions on antibiotic use are looser, or simply where ""anything goes.""Livestock are the single biggest consumers of antibiotic drugs in the U.S. ‚Äì not by choice, of course, but because farmers have long been sold a bill of goods by Big Pharma when it comes to using them to bulk up their cattle more quickly and get them to market as fast as possible. It's a common practice that generates a lot of money in the short-term, but a whole lot of problems in the long-term.The use of antibiotics on farms is arguably the biggest contributor to antibiotic resistance among humans, where drugs that used to treat deadly bugs no longer do. The result is the emergence of so-called ""superbugs"" that are capable of outsmarting even the most potent antibiotics, a phenomenon that's killing people by the hundreds of thousands.Some of the latest reports indicate that the superbug epidemic is claiming the lives of some 700,000 people annually all around the world. And if nothing is done now to stop the drug industry from continuing to poison the well, so to speak, then this number is expected to balloon more than 14-fold to 10 million lives lost by the year 2050.According to the World Health Organization (WHO), there are very few countries in the world with any sort of formidable plan in place to address antibiotic resistance. Even with the U.S. somewhat taking the lead with voluntary FDA suggestions on the matter, none of this will stop drug companies from preying on the developing world that has yet to enact any sort of restrictions on antibiotic use.""They're international companies,"" Gail Hansen, a veterinarian and consultant who works with drug companies, governments and nonprofit organizations, told Bloomberg . ""What happens in the U.S. does certainly make a difference, but it's not the only market they have."" Antibiotic resistance kills 10 times more people than opiate overdoses While it's become common practice for factory animal rearers to exploit antibiotics as a way to fatten their stocks more quickly and supposedly keep them healthy, the use of these drugs for such purposes is disastrous for public health. Comparatively speaking, only 69,000 people die from opiate overdoses globally, which translates to 10 times more people dying from superbugs than from the most deadly pharmaceuticals on the market.Antibiotics are also overused in the realm of human medicine, with some one-third of all prescriptions, according to the U.S. Centers for Disease Control and Prevention (CDC) and the Pew Charitable Trusts, being completely unnecessary. This translates to roughly 47 million prescriptions that aren't even helpful, either because the patients have a viral infection, or don't have an infection at all.The only way anything is going to change is if Big Pharma is barred from selling antibiotics to farmers whose only desire is to rush their animals to market for maximum profits. Doctors must also be reigned in and stopped from indiscriminately handing out antibiotics whenever their patients feel any kind of ailment ‚Äì and again, these measures must have a global reach.""If some of the biggest responsible parties ‚Äì namely the companies making the products ‚Äì are still selling the antibiotics in other countries, it just underscores that this has to be a change that happens across the entire world,"" David Wallinga, a senior health official and physician at the Natural Resources Defense Council (NRDC), told Bloomberg .""And the companies bear a big responsibility for that approach."" Sources for this article include:",naturalnews.com,1
Beware: You could be poisoning yourself by reheating these five foods,"Beware: You could be poisoning yourself by reheating these five foods 
Thursday, October 27, 2016 by: Isabelle Z. Tags: reheating , toxic food , foodborne illness (NaturalNews) Nothing beats a home-cooked meal. Not only does food taste better when you prepare it yourself, but it's also a lot healthier. Unfortunately, this can also be very time-consuming depending on the dish in question, and today's hectic lifestyle means that a lot of us can't cook something special from scratch every day. If you don't want your family to have to resort to toxic fast food on your busy days, what can you do?Making enough home-cooked food to last a few days sounds like a great idea on the surface because you can quickly microwave it, but the truth is that reheating certain foods can actually make you and your family quite ill . Here is a look at some of the foods that you need to be really careful about reheating. Rice According to the NHS, eating reheated rice can cause food poisoning. They explain that the reheating itself is not the culprit; it actually has to do with how the rice was stored prior to reheating. Rice can sometimes contain spores of bacteria known as Bacillus cereus , which can cause food poisoning, and these spores can survive cooking. If rice is then left to sit at room temperature, the spores can grow into bacteria, which will multiply and could produce toxins that can cause vomiting and diarrhea.The NHS says that the likelihood of this happening increases with the amount of time the rice sat outside at room temperature. They advise serving rice right away after it has been cooked or else cooling it as quickly as possible ‚Äì within no more than an hour. They also suggest reheating rice to the point where it is steaming, and avoiding storing it in the refrigerator for longer than one day. Potatoes Potatoes that have sat out at room temperature for several hours can not only lose their nutritional value, but they could also create the perfect conditions for botulism to grow. Uneaten potatoes should go directly to the fridge after cooking to help reduce the chances of this occurring. Chicken When you reheat cold chicken, the structure of its proteins is changed, which can cause digestive upset. If you have leftovers you can't bear to toss, reheat them slowly at a lower temperature on the stove or in the oven to help keep the chicken's proteins intact. It must be steaming all the way through before you can serve it, so be sure to stir it every once in a while. Spinach Spinach is best eaten raw, but it's okay to cook it if you plan to eat all of it immediately afterward. However, when cooked spinach is reheated, the nitrates in it break down into nitrites, turning one of the healthiest foods into something that is potentially toxic and can negatively impact the uptake of oxygen in the blood. Eggs It is not a good idea to repeatedly expose eggs to heat. Reheating them after they've been fried or boiled using high temperatures can cause them to become toxic and upset your digestive tract. Moreover, their protein is destroyed when they are repeatedly exposed to heat. According to the FDA , cooked egg dishes should never be left outside of the fridge for more than two hours; this drops to just one hour if the temperature is higher than 90 degrees F. Warm temperatures between 40 and 140 degrees provide the perfect environment for bacteria to grow.While most of us have reheated these foods and have not noticed any adverse effects, it is important to exercise caution if you plan to continue doing so in order to avoid poisoning yourself and your family or causing digestive issues. Tina Hanes of the USDA Meat and Poultry Hotline suggests making sure your leftovers reach 165 degrees Fahrenheit when you reheat them, and you should use a food thermometer to verify this. Covering the food helps it retain moisture and boosts its chances of heating all the way through, which is vital for avoiding cooler pockets where bacteria can thrive. Finally, keep in mind that microwaves are particularly risky because they do not heat food evenly. Sources include:",naturalnews.com,1
Turning Your Vacation Photos Into Works of Art,"It‚Äôs the season for family travel and photos  ‚Äî   and perhaps enlarging some of those images of snowy landscapes or tropical getaways to decorate your home. There are, of course, the usual print services and methods. You can choose a glossy or matte finish, print a photo on canvas, or make it into a poster with a few clicks online at photo sites like Snapfish and Shutterfly, professional photo shops like Adorama and Mpix, or drugstores and   chains like Walgreens and Costco. But the web is also home to many   printing services, as well as uncommon surfaces on which to enlarge photos for display, be it burlap, wood boards, acrylic or    fabric. Why not try some fresh sites and methods? I recently sent some   quality iPhone vacation photos to a handful of companies that I‚Äôd never used before and had them enlarged to various sizes and printed on different surfaces. I‚Äôve also offered some guidance about bulk digitizing those boxes of old travel photos sitting in your closet or basement so that you can begin the New Year if not with a vacation, then with a   home. Of all the ways to turn photos into wall art, I was most interested in trying engineer prints, named for the large, lightweight prints used by architects. For less than the cost of a couple of movie tickets, you can make huge enlargements. Mind you, it‚Äôs a particular aesthetic, one that‚Äôs most likely to appeal to people who are after an industrial, shabby chic or bohemian look. The paper is thin and the lines of the images are softer than a fine art print. And engineer prints need not be formally framed. People stick them to their walls with washi tape, a crafting tape that comes in innumerable colors and prints or they hang the prints using wood poster rails or skeleton clips. For a while, engineer prints from photos were primarily available in black and white, but now you can find them in color, too. One of the easiest ways to order them online is through Parabo Press, which is run by Photojojo, an online photography gear shop, and Zoomin, a photo printing service in Asia. As with all printing sites, you upload your image, zoom in closer if you like, and then click to buy. The site‚Äôs engineer prints are 4 feet by 3 feet, and cost $20 in black and white, and $25 in color. I sent out two different photos to be made in black and white, and they came out, to my surprise, beautifully. I was impressed that they were able to be enlarged to such a degree and not look blurry. And the paper (while so thin I was worried about accidentally tearing it) lends it an artful, careless look rather than the expected framed print over the couch. Parabo Press is a breeze to use: It‚Äôs clean and easy to read, your options are straightforward, and there are no annoying upsells. The site also offers prints on metal, glass, newsprint and Zines (handmade magazines) calendars photo books and prints from its Risograph machine, which uses   ink and is described by Parabo as having ‚Äúa cult following since its invention in 1980s Japan. ‚Äù A fabric print  ‚Äî   not soft like a bedsheet, more like a place mat made of matte woven fabric  ‚Äî   is another departure from a traditional photo enlargement. Order one from a site such as SnapBox and instead of framing it, you can peel and stick it on your wall. The site‚Äôs fabric posters adhere to (and can be peeled off) smooth surfaces such as untextured walls, glass, ceilings, tile and finished wood surfaces (avoid surfaces like stucco, concrete blocks, brick, unfinished wood, canvas or freshly painted walls). SnapBox offers fabric posters in more than a dozen sizes from 4x4 to 36x54, from less than $2 to about $80. I ordered a 24x36 fabric poster for $34. 99, a discounted price thanks to a holiday coupon  ‚Äî   not cheap (you can buy fine art prints on other sites for less) but you‚Äôre printing on special material. Regardless of the cost, I expected the finished product to look like the sort of cheap thing one might see in a dorm room (it sticks to walls, after all) but I was pleasantly surprised. The fabric was durable and the details in the photo  ‚Äî   crevasses in a glacier onlookers on a bridge  ‚Äî   were nicely defined. SnapBox is a   site with clear instructions and pricing. In addition to fabric posters, it also offers fine art prints, photo books and prints on canvas and pillows. While many places can print photos on hard surfaces such as metal and acrylic, printing on wood boards is less common. The grain shows through your photos, which, thematically speaking, seems to make sense for certain subjects, like nature photos taken at, say, the beach or in a park. But what would something more modern, like a skyscraper or a tower, look like on wood? I decided to give it a try and put an image of Tokyo Tower on an 8x12 board ($65). I sent the photo to PhotoBarn, a family business that makes its products by hand in a ‚Äú ‚Äù in Tennessee. The result was a lovely departure from framed prints and from canvas, which can sometimes make striking photos look like amateur paintings. The wood was smooth and thick, and the image was crisp with a slight sheen  ‚Äî   a perfect complement to the steel of Tokyo tower and the silver and glass of surrounding skyscrapers. For the most part the site is intuitive, though a few too many holiday sale buttons on the home page made for a disorienting start. PhotoBarn will also print your photos on canvas, burlap, and other wood products, like ornaments. I noticed a number of complaints about PhotoBarn on Yelp and the Better Business Bureau website regarding shipping speeds and customer service. I didn‚Äôt have a problem, but if time is of the essence, you may want to check with the company before placing an order. Once you‚Äôve turned the best of your travel photos into art, it‚Äôs time to store the rest. If boxes of prints are taking up closet (and psychic) space, there are plenty of sites online that will scan your old photos (as well as negatives, slides and videos) so you can store them digitally. But there are several things to keep in mind. In general, these sites are a pain to navigate. They‚Äôre cluttered with too much text and fine print, and they offer so many options  ‚Äî   Do you want your photos scanned in order? Do you want both sides of the photo scanned?  ‚Äî   that if you don‚Äôt have a goal in mind before you go in, you can quickly be overwhelmed. Decide ahead of time what exactly you want to scan, how many photos you have and how you might use whatever you scan. Also, note that some of these companies by default send DVDs or CDs of your digital files. Not everyone has a CD or DVD player. If you want a thumb drive instead, be sure to select that option (if it‚Äôs offered) or call the company and see if it will provide one. Be aware, too, that it‚Äôs not unusual for these companies to have long lead times. A number of them digitize your photos in other countries, so it can take weeks to get your images back. For affordable bulk scans, ScanMyPhotos. com is an old standby (you can read David Pogue‚Äôs review on nytimes. com). The company will scan about 1, 800 photos at 300 dpi for $145 at its headquarters in Irvine, Calif. the cost of sending the photo box to you, as well as the shipping of the box to ScanMyPhotos and back to you again is included in the price. That‚Äôs one of the least costly and most uncomplicated deals around. Other companies charge for shipping photo boxes. I asked a photo editor at The Times if 300 dpi is sufficient for scanning and she said that to print photos at larger sizes, a higher dpi is preferable. ScanMyPhotos has such an option: a prepaid box for $259 for the same number of scans at 600 dpi instead of 300 dpi. A thumb drive is an additional $15. 95 a box. To find the best   place to scan photos and film, the Wirecutter, a consumer review site owned by The New York Times, researched 37 different scanning services and tested the top 12 contenders. Memories Renewed took the number one spot. The company, based in Minneapolis, Minn. offered ‚Äúthe best combination of price, quality, and turnaround time of any service we tested,‚Äù Wirecutter said. I was planning to try the service however, according to the Memories Renewed site, demand is so high at the moment that the lead time for most projects is more than two months. Scanning photos of any size up to 8. 5x11 is 60 cents a photo a thumb drive is $10 for 8 GB or $15 for 16 GB. Let‚Äôs say you don‚Äôt want to ship your irreplaceable photos in the mail. Or maybe you‚Äôd rather that strangers not see your photos and home videos. You could buy a scanner and scan your photos yourself, perhaps doing a batch for half an hour each day. Personally, I don‚Äôt want machines around my home collecting dust (and fast becoming outdated). So I decided to try the new PhotoScan app by Google Photos. It‚Äôs free and enables users to scan prints with a smartphone. First things first: These are not   scans. If you have prized photos in need of restoration, then go with a professional. However if, like me, you have a bunch of travel photos  ‚Äî   landscapes, food, monuments  ‚Äî   that you‚Äôre keeping simply because you want to remember where you were when, you may want to consider trying the app instead of giving up some privacy and spending upward of $150. By and large, PhotoScan is simple and quick, with almost no learning curve. If you try it, just make sure to hold your phone level when asked to move it over the image. Remember these words: Don‚Äôt tilt your phone! Most of the scans I made looked as good as the prints in terms of color and clarity. That said, this is unlikely to be your solution if you want   prints or have thousands of photos to scan. Once you get the hang of PhotoScan, using it becomes a repetitive, vaguely   activity. That is, unless the app crashes, which it did several times. But I was still glad for it. Even when it crashed, it took only the tap of a finger to begin again. And you can‚Äôt beat the price.",New York Times,0
Kim Jong-un Says North Korea Is Preparing to Test Long-Range Missile,"SEOUL, South Korea  ‚Äî   North Korea‚Äôs leader, Kim   said on Sunday that his country was making final preparations to conduct its first test of an intercontinental ballistic missile  ‚Äî   a bold statement less than a month before the inauguration of   Donald J. Trump. Although North Korea has conducted five nuclear tests in the last decade and more than 20 ballistic missile tests in 2016 alone, and although it habitually threatens to attack the United States with nuclear weapons, the country has never   an intercontinental ballistic missile, or ICBM. In his annual New Year‚Äôs Day speech, which was broadcast on the North‚Äôs   KCTV on Sunday, Mr. Kim spoke proudly of the strides he said his country had made in its nuclear weapons and ballistic missile programs. He said North Korea would continue to bolster its weapons programs as long as the United States remained hostile and continued its joint military exercises with South Korea. ‚ÄúWe have reached the final stage in preparations to   an intercontinental ballistic rocket,‚Äù he said. Analysts in the region have said Mr. Kim might conduct another weapons test in coming months, taking advantage of leadership changes in the United States and South Korea. Mr. Trump will be sworn in on Jan. 20. In South Korea, President Park   whose powers were suspended in a Parliamentary impeachment on Dec. 9, is waiting for the Constitutional Court to rule on whether she should be formally removed from office or reinstated. If North Korea conducts a    test in coming months, it will test Mr. Trump‚Äôs new administration despite years of increasingly harsh sanctions, North Korea has been advancing toward Mr. Kim‚Äôs professed goal of arming his isolated country with the ability to deliver a nuclear warhead to the United States. Mr. Kim‚Äôs speech on Sunday indicated that North Korea may   a   rocket several times this year to complete its ICBM program, said Cheong   a senior research fellow at the Sejong Institute in South Korea. The first of such tests could come even before Mr. Trump‚Äôs inauguration, Mr. Cheong said. ‚ÄúWe need to take note of the fact that this is the first New Year‚Äôs speech where Kim   mentioned an intercontinental ballistic missile,‚Äù he said. In his speech, Mr. Kim did not comment on Mr. Trump‚Äôs election. Doubt still runs deep that North Korea has mastered all the technology needed to build a reliable ICBM. But analysts in the region said the North‚Äôs launchings of   rockets to put satellites into orbit in recent years showed that the country had cleared some key technological hurdles. After the North‚Äôs satellite launch in February, South Korean defense officials said the Unha rocket used in the launch, if successfully reconfigured as a missile, could fly more than 7, 400 miles with a warhead of 1, 100 to 1, 300 pounds  ‚Äî   far enough to reach most of the United States. North Korea has deployed Rodong ballistic missiles that can reach most of South Korea and Japan, but it has had a spotty record in   the Musudan, its   ballistic missile with a range long enough to reach American military bases in the Pacific, including those on Guam. The North has also claimed a series of successes in testing various ICBM technologies, although its claims cannot be verified and are often disputed by officials and analysts in the region. It has said it could now make nuclear warheads small enough to fit onto a ballistic missile. It also claimed success in testing the   technology that allows a   missile to return to the Earth‚Äôs atmosphere without breaking up. In April, North Korea reported the successful ground test of an engine for an intercontinental ballistic missile. At the time, Mr. Kim said the North ‚Äúcan tip   intercontinental ballistic rockets with more powerful nuclear warheads and keep any cesspool of evils in the Earth, including the U. S. mainland, within our striking range. ‚Äù On Sept. 9, the North conducted its fifth, and most powerful, nuclear test. Mr. Kim later attended another ground test of a new   rocket engine, exhorting his government to prepare for another rocket launch as soon as possible. In November, the United Nations Security Council imposed new  sanctions against the North.",New York Times,0
Losing Trump Tries to Fool Voters with Last Ditch Media Propaganda Scam,"What to do when the October surprise aimed at your opponent flops and your polls are bad, so bad, really bad? 
How about more propaganda! 
The losing Trump campaign‚Äôs latest Hail Mary is to try to fool voters with a last ditch media propaganda push by recruiting volunteers to blitz talk radio with ‚Äútalking points‚Äù and ‚Äúmonitor‚Äù discussions and callers. 
Shared by Talk Show host and Editor-in-chief of Right Wisconsin.com Charlie Sykes: Welp. Trump campaign planning ""talk radio blitz."" Recruiting volunteers to call shows with talking points. https://t.co/eWR5qLKRjR 
‚Äî Real Charlie Sykes (@SykesCharlie) October 31, 2016 
From the Trump campaign‚Äôs call to action: 
Trump Talk Radio Blitz 2016 Help get Donald Trump elected! Please sign up below to volunteer to call into talk radio over the final days of this Presidential campaign and tell Wisconsin‚Äôs voters why you support Donald Trump. We are looking for volunteers to sign up as a Trump Talk Radio Blitz Call Captain and/or a Trump Talk Radio Blitz Caller. 
Thank you for your willingness to help elect Donald Trump and Mike Pence this very important election year! 
What will these call captains do? Oh, ‚Äúmonitor‚Äù other the show and discussions, give talking points ‚Äî you know, propaganda: 
I am willing to help! (check all the apply) I am willing to serve as a Trump Talk Radio Blitz Call Captain, taking responsibility to monitor and recruit other callers for one or more talk radio programs between now and election day 
I am willing to be a Trump Talk Radio Blitz Caller and select one or more 15 minute segments for local talk radio shows to call into and promote Donald Trump‚Äôs campaign 
Call ‚ÄúCaptains‚Äù will be in charge of entire shows to make sure the Trump message gets out and ‚Äúmonitor‚Äù discussions lest anyone bring any facts to the air: 
Call Captains will sign up to be responsible for an entire talk radio program or programs between now and the election to 1) help recruit callers to call in and promote Donald Trump‚Äôs campaign throughout the program for that day, 2) remind Trump Talk Radio Blitz Callers who have sign up of their commitment to call into the show, 3) help ensure your Trump Talk Radio Blitz Callers are prepared to discuss the show‚Äôs topics, 4) monitor the show and the discussions, and 5) report your successes back to the campaign. 
Paid for by the Trump campaign: 
I mean, it‚Äôs working so well for the Trump spokespeople, especially those propaganda artists embedded on a certain cable network. Why not organize non-professionals to give the same talking points and disseminate them in a way that appears ‚Äúorganic‚Äù. LOL, I kid. If you‚Äôve run into these people in a comment section or forums you know they have the campaign message down so exactly as to be a screaming siren of talking points. 
So when you hear screaming tinfoil about fake polls and accusations that Hillary Clinton committed treason even though she was actually cleared, you needn‚Äôt worry that your entire country has gone insane. 
This is the last gasp of a sinking campaign trying to fool the voters with media propaganda dished out by ‚Äúvolunteers‚Äù.",politicususa.com,1
Report: Election Day Most Americans‚Äô Only Time In 2016 Being In Same Room With Person Supporting Other Candidate - The Onion - America's Finest News Source,"Hillary Clinton Waiting In Wings Of Stage Since 6 A.M. For DNC Speech PHILADELPHIA‚ÄîSaying she arrived hours before any of the members of the production crew, sources confirmed Thursday that presidential nominee Hillary Clinton has been waiting in the wings of the Wells Fargo Center stage since six o‚Äôclock this morning to deliver her speech at the Democratic National Convention. Depressed, Butter-Covered Tom Vilsack Enters Sixth Day Of Corn Bender After Losing VP Spot WASHINGTON‚ÄîSaying she has grown increasingly concerned about her husband‚Äôs mental and physical well-being since last Friday, Christie Vilsack, the wife of Agriculture Secretary Tom Vilsack, told reporters Thursday that the despondent, butter-covered cabinet member has entered the sixth day of a destructive corn bender after being passed over for the Democratic vice presidential spot. DNC Speech: ‚ÄòI Am Proud To Say I Walked In On Bill And Hillary Having Sex‚Äô A friend of the Clinton family describes a Hillary who America never gets to see: the one he saw having sex. Trump Sick And Tired Of Mainstream Media Always Trying To Put His Words Into Some Sort Of Context NEW YORK‚ÄîEmphasizing that the practice was just more evidence of journalists‚Äô bias against him, Republican presidential nominee Donald Trump stated Thursday that he was sick and tired of the mainstream media always attempting to place his words into some kind of context. Who‚Äôs Speaking At The DNC: Day 4 Here is a guide to the major speakers who will be addressing attendees on the final night of the 2016 Democratic National Convention Bound, Gagged Joaquin Castro Horrified By What His Identical Twin Brother Might Be Doing Out On DNC Floor PHILADELPHIA‚ÄîStruggling to free himself from the tightly wound lengths of rope binding his wrists and ankles together, bruised and gagged Texas congressman Joaquin Castro was reportedly horrified by what his identical twin brother, Secretary of Housing and Urban Development Julian Castro, might be out doing on the floor of the DNC Thursday. Obama: ‚ÄòHillary Will Fight To Protect My Legacy, Even The Truly Detestable Parts‚Äô PHILADELPHIA‚ÄîEmphasizing the former secretary of state‚Äôs competence and tenacity during his Democratic National Convention address Wednesday night, President Barack Obama praised Hillary Clinton as someone who would work tirelessly to defend and advance the legacy he had built, even the ‚Äútruly repugnant parts.‚Äù Tim Kaine Clearly Tuning Out In Middle Of Boring Vice Presidential Acceptance Speech PHILADELPHIA‚ÄîDescribing the look of total disinterest on his face and noting how he kept peering down at his watch as the speech progressed, sources at the Democratic National Convention said that Virginia senator Tim Kaine clearly began tuning out partway through the boring vice presidential acceptance address Wednesday night. Cannon Overshoots Tim Kaine Across Wells Fargo Center PHILADELPHIA‚ÄîNoting that the vice presidential nominee had been launched nearly 100 feet into the air during his entrance into the Democratic National Convention Wednesday night, sources reported that the cannon at the back of the Wells Fargo Center had accidentally overshot Tim Kaine across the arena, sending him crashing to the stage several dozen feet beyond the erected safety net. Biden Regales DNC With Story Of ‚Äô80s Girl Band Vixen Breaking Hard Rock‚Äôs Glass Ceiling PHILADELPHIA‚ÄîDevoting a large portion of his speech to the ‚Äúpioneering, stiffy-inducing‚Äù all-female quartet, Vice President Joe Biden regaled the Democratic National Convention Wednesday night with the rousing story of the metal band Vixen breaking hard rock‚Äôs glass ceiling in the late 1980s. ",theonion.com,1
Factors Affecting User Experience with Security ,"",,
"onnel. """,Scientific Journal,0,
NFL Warns Clenbuterol Level So High in Beef that Players Fail Drug Tests,"The National Football League (NFL) in the United States has issued a warning to players , asking them to avoid eating beef because of the high levels of Clenbuterol . These high levels have been said to cause players to fail drug tests. Clenbuterol is a muscle-building and weight-loss stimulant. It is a substance banned by the league, and considered a performance enhancing drug. However, recently, many players in the NFL have failed drug tests linked to Clenbuterol. If a player fails this drug test then he is banned for 10 matches.
The majority of beef consumed in the United States is imported from China and Mexico. The NFL said beef from China and Mexico is particularly dangerous to the players.
Clenbuterol has the ability to build muscle, which is why farmers use it, increasing the size of their cattle to increase profits. In Mexico, cattle ranchers are banned from using Clenbuterol as a growth enhancer. However, it is said farmers are not complying with the regulation, leading to widespread abuse of the substance within the nation‚Äôs cattle industry.
The Free Thought Project reports that the NFL‚Äôs independent drug-testing administrator has officially sent a memo to all players in the league, telling them that consuming large quantities of beef while visiting China and Mexico may result in a positive Clenbuterol test. The memo further stated that if players refused to heed the warning and do otherwise, they do so at their own risk.
‚ÄúPlayers are warned to be aware of this issue when traveling to Mexico and China. Please take caution if you decide to consume meat, and understand that you do so at your own risk. Players are responsible for what is in their bodies,‚Äù the memo said.
In 2015, the Texan‚Äôs left tackle, Duane Brown, tested positive for Clenbuterol after a trip to Mexico, during which he ate Mexican beef, sources told ESPN . After a months-long process, Brown was finally cleared in April this year, permitting him to avoid what would have been a 10-match ban.
When the memo was issued, several players in the NFL, including the Arizona Cardinals defensive back, Patrick Peterson, took to Twitter to express his disappointment in the issue.
This can‚Äôt be real life! #SMH #GottaGoVeganOnVacation ü§ï pic.twitter.com/HmWNpi4cAj",anonhq.com,1
Assessing Information Security Culture: A Critical Analysis of Current Approaches,"INTRODUCTION Today’s organizations operate in a global environment. Globalization enables organizations to collaborate and share information resources with one another but also exposes them to many threats both within and from outside of the organization. Organizations therefore need to secure their information resources. largely at Humans are the center of protecting an organization’s information resources through their behavior when interacting with information and information systems. Through the formulation of information security policies, management requires employees to behave in a secure manner and consequently protect information resources. However, employees are known not to adhere to the security policies, leading to security breaches [1,2]. To mitigate the risk posed by the non-adherence of factor”), employees researchers suggest the fostering or development of a culture of information security (information security culture) [3-5]. to security policies (the “human The first step in the development of an information security culture is the assessment of the current state of the culture [6]. According to Schlienger & Teufel [7,8], there is no unique toolset and method for studying information security culture with regards to what to assess and how to assess information security culture. The problem therefore exists that there is no published widely accepted and consolidated approach that indicates how the assessment of information security culture should be done and researchers have called for more research in this area [7-9]. This paper focuses on the analysis and comparison of current approaches to the assessment of information security culture in order to ascertain their suitability specific for use in the culture change process. The remainder of this paper will briefly describe the methodology used (section II); introduce the concept of an information security culture (section III); examine the process for fostering such a culture and discuss the role culture assessment plays in the fostering and management of an information security culture (section IV); critically evaluate and compare current approaches to the assessment of an information security culture (Section V); and finally discuss the outcome of section V (section VI). Section VII concludes this paper by proposing the use of an audit framework for the assessment of information security culture. II. METHODOLOGY This research performs a critical analysis on current (existing) information security culture assessment approaches through a combination of literature reviews and qualitative content analysis techniques. Qualitative content analysis is a “research method for the subjective interpretation of content of text data through the systematic classification process of coding and identifying themes or patterns” [10]. The analysis was conducted according to guidelines provided by Krippendorff [11]. A literature study was conducted and a number of research projects that focus on the assessment of information security culture were identified. The main idea of each article under review was the article’s strengths and weaknesses were analyzed. Qualitative content analysis was done on the identified research based on the following broad thematic areas: identified and _ Did the researchers assess the following levels of information security culture: artifacts, espoused values, shared tacit assumptions and information security knowledge? _ What assessment methods were used to assess which level of information security culture? _ Did the researchers use an integrated approach incorporating all levels of information security culture? The results from the critical analysis, based on these three thematic areas are discussed in section V and VI as the outcome of this paper. III. INFORMATION SECURITY CULTURE Security does not lie only in firewalls, passwords and awareness training but also in a culture that views and thinks correctly about information security issues. A culture of information security needs the organizational culture, to allow them to view and think correctly about information security problems [12]. to be embedded into security culture a is Information subculture of organizational culture [7]. It is mostly described using Schein’s model of organizational culture which is widely accepted in the field of information security [7, 8]. According to Schein [14] organizational culture is “a pattern of shared tacit assumptions that was learned by a group as it solved its problems of external adaptation and internal integration, that has worked well enough to be considered valid and, therefore, to be taught to new members as the correct way to perceive, think, and feel in relation to those problems”. Schein describes culture as existing in three levels: artifacts, espoused values and shared tacit assumptions. This definition however is not specific to information security despite being widely accepted as a general organizational culture definition hence the enhancement by Van Niekerk & Von Solms [15]. Van Niekerk & Von Solms [15] describes information security culture by adapting Schein’s model and adding a fourth level, information security knowledge which supports the other three levels. Information security knowledge is necessary if employees are to behave in a secure manner, as it cannot be assumed that the employees possess such security knowledge. The four levels of information security culture are described below. _ Artifacts: This is the visible layer, referred to as the surface manifestation of culture [13]. Artifacts are the things that are observed in an organization [14] and refer to visible and measurable everyday behavior in the organization include behavior patterns, security handbooks, awareness courses, language and technology. This level though visible is hard to interpret without questioning the “insiders” of the organization. [3]. Artifacts _ Espoused values: This layer is partially visible and unspoken but can shape the behavior of employees [13]. Espoused values represent the “reasons” given by insiders of an organization regarding the observed artifacts [14]. The espoused values of an organization include strategies, goals, philosophies and other documents that describe the values, principles and vision of the organization [14]. _ _ Shared tacit assumptions: Shared tacit assumptions usually form in organizations that have been successful where the success can be attributed to the values, beliefs and assumptions of the founders of the organization [14]. If the organization continues to be successful, the beliefs and values become shared and taken for granted and the organization’s culture [16]. the core of form Information security knowledge: This level supports the other three levels: 1) artifacts (employees need to have adequate information security knowledge of how to carry out their job functions in a secure manner), 2) espoused values (the relevant employees need to have sufficient information security knowledge to know what to incorporate into a policy document for example, to ensure that it adequately addresses the security needs of the organization), and 3) shared tacit assumptions (if an employee’s beliefs conflicts with an espoused value for example, not knowing why a specific control is required, the employee might knowingly disregard the security control). Knowledge could thus help in ensuring compliance [8]. IV. THE ROLE OF ASSESSMENT IN CULTURE CHANGE According to Cameron & Quinn [17] change is a given and organizations are subject to change in order to survive. Change in an organization needs to be properly managed to avoid failure. This failure can sometimes be attributed to the neglect of organizational culture. Organizations therefore need to manage the change process at the cultural level [17]. Schein [16] proposes a structured change management process to facilitate culture change in the organization. Successful culture change needs to start with support from top management [18]. Fig. 1 illustrates a culture change process adapted from Van Niekerk & Von Solms [3] and is described below. _ Step 1: Top management support and commitment Management needs to understand the existing culture (current state), the need for change and the idea of change usually in response to a “business problem”. _ Step 2: Define the specific business problem to the gap between the preferred state This entails assessing the current state of culture, defining the preferred future state of culture and analyzing the current and the preferred state. The steps needed to move from the current state follows a transformative change management process [16]. According to Schein [14] such a transformative process involves three stages: 1) unfreezing (creating motivation for change through disconfirmation and psychological safety), 2) learning new concepts and new meanings for old concepts through imitation of and identification with role models, 3) refreezing (internalizing the new concepts and new meanings). O’Donovan [18] suggests using “culture embedding mechanisms” like modeling, teaching and coaching by leaders to internalize the new way of doing things into everyday practices. _ Step 3: Develop strategic action plan that need Strategic action plans include: Identifying actions and behaviors to be started, stopped or encouraged towards achieving the kind of behavior the desired culture; evaluating that aligns with employees’ readiness for change; educating the employees regarding the culture change to minimize resistance; motivating the employees; generating social support by involving those affected by the change and identifying champions who can influence others and act as role models; providing constant and extensive information to increase employees’ understanding and commitment to minimize confusion and resistance and to prepare them for the effects of the change. _ Step 4: Create a cultural fit Creating a cultural fit through mechanisms like education and training, appraisal and reward systems helps to ensure that the changes last. O’Donovan [18] refers “embedding mechanisms”. such mechanisms as to _ Step 5: Develop and choose a change leader team This is necessary to facilitate the change. The leaders need to be committed, competent and have a common purpose. _ Step 6: Create small wins It is important to identify key action steps that can be executed to generate momentum. Such small actions must be aligned to the desired culture change for motivation of employees. _ Step 7: Identify metrics, measures and milestones It is important to identify measures for success, metrics of the main indicators and milestones to track progress of the change. As the change process progresses it may be necessary to reassess the current state following the results emanating from tracking the change process. Fig. 1 Framework for culture change adapted from Van Niekerk & Von Solms [3] _ Step 8: Feedback and review The organizational environment changes frequently due to internal and external factors and so does the organization’s needs. It therefore becomes necessary to periodically review and refine the entire information security culture management process. The feedback and review can necessitate the reassessment of the current state of information security culture in order to ensure that the organization’s information security culture is continuously managed and the organization’s security needs are met. Assessment plays an important role in the culture change process, as illustrated in the eight-step culture change process. Culture is a complex, extensive and multifaceted concept that needs to be analyzed and assessed at each level in order for it to be understood [14]. The complexity of culture is implicated in the mission, vision, strategy and goals of the organization (external survival issues), and in the relationships amongst people in the organization (internal integration issues). These are also correlated to the broader national cultural assumptions arising from organizations becoming global. incorporating more Culture assessment could be used to solve a business problem, make something more efficient, or to achieve a new strategic goal possibly arising from mergers, joint ventures or partnerships than one culture [14]. According to Schein [14] assessment of culture plays an important role in culture change. It helps an organization to understand terms of strengths and weaknesses and assist to make strategic choices. Consequently, culture assessment enables an organization to solve a problem, to make a change and to learn something new. In order to do its own culture in this, an organization needs to know how the culture or subculture can help or deter it. forces include technology, policies, According to O’Donovan [18], culture change exists in response to internal and external forces in the environment. Internal leadership changes, absenteeism, and rapid staff turnover while external forces include political, economic, environmental and regulatory forces. These forces can necessitate the assessment of culture. For example, a new chief executive officer/chief security officer has a new security vision that could necessitate assessment of the information security culture to align with the new security vision. As explained, one of the reasons for culture assessment is to solve a “business problem”. One such problem could be information security that necessitates culture assessment in order to identify improvements to the security policy. Culture assessment could also arise due to organizations having to comply with regulatory and/or legal requirements and/or standards, for example the health insurance portability and accountability act of 1996 (HIPAA) which deals with the protection of personal healthcare information either stored or in transit within or outside of the organization [12], the Sarbanes-Oxley act of 2002 which requires organizations to independently certify the adequacy of their internal controls [19], and the ISO 27001 which stipulates the requirements for implementing security controls to meet the needs of every organization [20]. the organization’s implementing solutions and addressing According to Martins & Eloff [21] assessment in culture change can also help to ensure that recommendations from previous assessments are implemented. Conducting periodic assessments, the concerns arising from previous assessments can constantly improve information security culture. Culture assessments also assist an organization to identify the current and the desired information security culture, the areas that need the most attention, and improvements needed to achieve the desired information security culture [22]. Another reason for culture assessment is to help an organization understand the behavior of its employees towards information security and to identify key issues for implementation and integration the organization [23]. Assessment of information security culture can also serve as a “wake-up” call for management depending on the results of the assessment and to take decisive action [6]. the information security culture of into Having highlighted the importance of assessment in culture change, it is necessary to discuss approaches that can be used towards the assessment of information security culture. V. CURRENT ASSESSMENT APPROACHES From the literature analysis conducted, a number of research projects were identified that focus on the assessment of information security culture in an organization, namely Maynard, Ruighaver & Chia; Martins & Eloff; Ngo, Zhou, Chonka & Singh; Gerbrasilase & Lessa; Schlienger & Teufel; Finch, Furnell and Dowland; [9,21-25]. Martins and Eloff used an assessment approach consisting of an audit process and including an information security culture questionnaire for the assessment of information security culture in an organization. Ngo, Zhou, Chonka & Singh discussed how the level of information security culture in Australian small and medium enterprises can be assessed. Gerbrasilase & Lessa used a descriptive survey method for the assessment of information security culture in Hawassa referral hospital. Schlienger & Teufel used an information security culture management process incorporating a combination of methods for the assessment and management of information security culture in an organization. Finch, Furnell & Dowland assessed the information security culture by targeting the security attitudes and perceptions of system administrators and end-users to ascertain any disparity between both perspectives. Maynard, Ruighaver & Chia developed a research model for information security culture which can be used to assess the quality of an organization’s information security culture. This paper focuses only on the two assessment approaches that describe a process for the assessment of information security culture in an organization namely Martins & Eloff, Schlienger & Teufel [21,24]. Both assessment approaches will be discussed in this section to determine their suitability specific for use in the culture change process. A. Martins and Eloff’s assessment approach Martins & Eloff [26] argue that employees’ behavior and their interaction with computer systems play a major role in the security of information. They argue that employees need to display an acceptable behavior with regards to information security which should translate to their everyday activity in the organization. in They culture. propose the development of The researchers suggest that organizational behavior needs the desired to be considered organizational incorporating organizational behavior theory in order to instill information security culture. This was done through the development of an information security culture model consisting of three levels: organization, group and individual. They identified certain issues at each level that could promote information security culture and these issues (i.e. policy and procedures, risk analysis at the organization level, management and trust at the group level, awareness and ethical conduct at the individual level) are affected by change agents, namely technology or competition to achieve a certain information security culture. approach attitudes, opinions To determine if information security culture is at an acceptable level in an organization, the researchers propose an assessment approach consisting of an audit process. The employees’ assessment perceptions, towards information security. From the analysis of the information gathered throughout the audit, organizations can address information security culture issues at the three different levels (i.e. organization, group and individual). The audit process consists of four phases: to determine actions aims and _ Phase 1: Development of the assessment instrument instrument was (questionnaire). The assessment validated for reliability and validity [27]. _ Phase 2: A survey process questionnaire organization’s the from phase one the information security culture. The that utilizes to assess researchers propose this assessment to be a continuous process in order to promote information security culture in an organization. _ Phase 3: Analysis of data obtained from phase two that the status of gives a quantitative information security culture in the organization. indication of _ Phase 4: Interpretations and recommendations concerning the analyzed data from phase three. This step provides feedback to the organization regarding their information security culture and to assist the organization in addressing areas of concern. The following paragraphs outline Martins and Eloff’s assessment approach with consideration to the three thematic areas outlined in section II. 1) Did the researchers assess the following levels of information security culture – artifacts, espoused values, shared tacit assumptions and information security knowledge? _ The researchers did not assess artifacts directly. security culture management process involves four stages: diagnosis, planning, implementation and evaluation: _ Diagnosis: This the stage existing analyzes identifies any information security culture and problems. There are to analyzing information security culture. The first is what to analyze (the assessment items) and the second is how to analyze (the assessment methods). The researchers propose a combination of assessment items and methods due to the difficulty in understanding culture. two aspects _ Planning: This stage involves two aspects. The first aspect is strategic planning which involves defining the target or objective for the development of information security culture of which the researchers used the information security policy and defining the market segments like employees, information technology staff and managers for comparison of data. The second aspect is operative planning which involves internal communication in terms of awareness programs, training, education and management buy-in in order to promote the security awareness of employees and managers. _ The researchers did not assess espoused values _ directly. _ The researchers focused on perceptions, attitudes and behavior. It is therefore inferred that they assessed shared tacit assumptions. _ They assessed security knowledge but not comprehensively [27]. 2) What assessment methods were used to assess which level of information security culture? _ No assessment method was used to assess artifacts directly. _ No assessment method was used to assess espoused values directly. _ A questionnaire was used to assess shared tacit assumptions. _ A questionnaire was used to assess security knowledge. 3) Did the researchers use an integrated approach information security levels of incorporating all culture in their assessment? _ The researchers used an auditing approach but did not incorporate all the levels of information security culture in their assessment. B. Schlienger and Teufel’s assessment approach According to the researchers, information security culture needs to be maintained and modified continuously to ensure that it meets the organization’s targets [8]. This continuous process of analysis and change referred to as the information Implementation: This stage involves management commitment, communication with all employees, education and training for all employees, and employee commitment. During this stage, detailed activities, responsibilities, resources, schedules and a budget need to be defined [24]. _ Evaluation: This stage gives information regarding the implemented efficiency and effectiveness of actions. the The researchers used a combination of methods to assess information security culture. They started by analyzing the information security policy in order to understand the official values (espoused values) and the interpretation was validated by interviewing the chief security officer (CSO). Analysis of the security policy also formed part of the assessment of artifacts. The researchers developed a questionnaire to determine the perceptions and attitudes of employees (true values), the perceptions and attitudes of the organization (official values), and what the employee thinks should be the best solution for each question. Unstructured interviews were conducted with the CSO and a technician responsible for security. Firstly, interviews were conducted before analyzing the security policy in order to get an insight of the organization’s information security. Secondly, the interviews were conducted after analyzing the security policy in order to discuss and interpret the identified issues in preparation of the questionnaire. Interviews were also done after analyzing the results of the questionnaire in order to discuss the quality of answers given by respondents. Observation was done as part of assessing artifacts by comparing the respondent’s answers with their behavior. the The following paragraphs outline Schlienger and Teufel’s assessment approach with consideration to the three thematic areas outlined in section II.  1) Did the researchers assess the following levels of information security culture – artifacts, espoused values, shared tacit assumptions and information security knowledge? _ The researchers assessed artifacts, espoused values, shared tacit assumptions but did not assess security knowledge. 2) What assessment methods were used to assess which level of information security culture? _ The researchers used document analysis, interview and observation to assess artifacts. _ To assess espoused values the researchers used document analysis, questionnaire and interviews. _ They used a questionnaire to assess shared tacit assumptions. 3) Did the researchers use an integrated approach information security levels of incorporating all culture in their assessment? _ The researchers used an integrated approach but did not incorporate all four levels of information security culture in their assessment. C. Summary of limitations of the two assessment approaches According to Schein [14] culture should be assessed across all its levels. Neither of the assessment approaches assessed the four levels of information security culture as described in section III. Both assessment approaches used a questionnaire to assess shared tacit assumptions. According to Schein [9, 25] a questionnaire or survey cannot be used to assess culture. This is because the dimensions of culture selected for assessment may not be important in relation to the cultural dynamics of that specific organization. Interviewing only the security personnel, as done by Schlienger & Teufel, may not reveal the problem the organization is trying to solve in its entirety. This will consequently affect what questions to ask and what questions to include in the questionnaire. In addition, the responses to the questionnaire cannot be evaluated to determine reliability and validity especially with the promise of anonymity and privacy, and because the respondents may understand and interpret the questions differently. According to Schein [14] other issues with using a questionnaire to assess culture include: _ Questionnaires do not reveal the interaction and patterning in the cultures and subcultures. _ Only superficial characteristics of the culture will be assessed because survey instruments cannot get at the deeper shared tacit assumptions that define the essence of cultures. _ Due to the tacit nature of cultural assumptions, respondents of questionnaires will not be able to answer the survey questions reliably since there is no certainty as to how the respondents interpret the questions. _ shared Inferring individual responses is very ineffective because individuals perceive questions differently. assumptions from Schlienger & Teufel interviewed the CSO to get an insight of the organization’s information security but this will only provide a perspective of that of the CSO and not a true reflection of the entire organization’s information security. Also by interviewing the security personnel after analyzing the questionnaire results does not give the interpretation for the respondents’ answers but rather what the security personnel thinks the respondents mean. Information security culture should be assessed across all its underlying levels, namely artifacts, espoused values, shared tacit assumptions and information security knowledge. Table 1 summarizes how researchers have assessed information security culture in an organization against the identified the different researchers assessed the four levels of information security culture, and the methods used in the assessment as well as the validity and reliability of the methods. thematic areas, to determine the above if TABLE I. RESEARCH APPROACH, ASSESSMENT ITEMS AND METHODS Assessment item Assessment method Research approach Martins and Schlienger and Document analysis Interview Artifacts Eloff None None Observation None Document analysis Espoused values Questionnaire None No direct assessment Interview None Teufel Analysis of information security policy Interview only with CSO Audit with no formal auditing guidelines or procedure Analysis of information security policy Questioning all level of employees Interview only with CSO Shared tacit assumptions Information security knowledge Questioning all Questioning all Questionnaire level of employees Interview None level of employees Interview only with CSO Questionnaire Few knowledge questions No direct assessment VI. DISCUSSION From section V it is evident that neither of the assessment approaches analyzed has an integrated approach for assessing all the underlying levels of information security culture as described in section III. Schlienger & Teufel [7] focused on the assessment of artifacts, espoused values and shared tacit assumptions but not information security knowledge. Audit-based techniques were used only in the assessment of artifacts. However, no formal auditing guidelines or established auditing framework was used which makes it difficult for the exact same assessment to be replicated in another organization, or by other researchers. Martins & Eloff focused mainly on shared tacit assumptions. Information security knowledge was not comprehensively assessed as acknowledged by the researchers [27]. Martins & Eloff [21] used an auditing approach in their assessment of information security culture but also did not explicitly state that formal auditing guidelines or framework was used. information security knowledge It is also evident from section V that shared tacit assumptions have been addressed extensively by both assessment approaches. However, the assessment of artifacts, espoused values and is currently lacking. It is the assertion of this paper that formal auditing processes, as an integrated approach, can play a role in the assessment of the dimensions of information security culture. Part of the assessment of information security knowledge should also be to assess the level of employee knowledge. This can be included in security related educational programs in an organization. Assessment of information security knowledge should also form part of the audit process to ensure that employees are knowledgeable about information security. This is an important recommendation to ensure responsibility for security violations. VII. CONCLUSION AND FUTURE WORK This paper provided a critical analysis of the current approaches used in the assessment of information security culture. It is evident that the current approaches, though thorough in some aspects, do not present an integrated approach that comprehensively assesses all the levels of information security culture. The current approaches to the assessment of information security culture utilize some form of auditing either as an assessment method or approach. However, none have followed a formal auditing approach utilizing an established audit framework so that the assessment process can be replicated in another organization. The development of such an audit framework for the assessment of information security culture forms part of our future work in this area. ACKNOWLEDGEMENT The financial assistance of National Research Foundation (NRF) towards this research is hereby acknowledged. Opinions expressed and conclusions arrived at, are those of the author and are not necessarily to be attributed to the National Research Foundation. ",Scientific Journal,0
A High-level Architecture  for Efficient Packet Trace Analysis  on  GPU Co-processors,"",,
"ﬁcantly""",Scientific Journal,0,
Comment on There Has To Be a Cheaper Way To Find The Worst People ‚Äî 2016 Elections Will Cost Over $6 Billion by doucyet,"Watch: Aerial Footage Catches Man Stopping at a Drive Thru ‚Äî While Being Chased By Police Home / Be The Change / Government Corruption / There Has To Be a Cheaper Way To Find The Worst People ‚Äî 2016 Elections Will Cost Over $6 Billion There Has To Be a Cheaper Way To Find The Worst People ‚Äî 2016 Elections Will Cost Over $6 Billion Claire Bernish October 30, 2016 2 Comments 
This presidential election cycle has been nothing, if not a controversy-laden shit show of the bizarre ‚Äî and a monumentally pricey one, at that. It‚Äôs now estimated the stupefying cost for the duopoly to provide us with two altogether unappealing candidates ‚Äî possibly the least liked in American electoral history ‚Äî as well as congressional minions, totals no less than $6.6 billion. 
And that‚Äôs a conservative figure. 
Despite nearly a century of anti-marijuana propaganda inundating schools, ads, and politics, in fact, legalizing cannabis has greater approval from the public than either Hillary Clinton or Donald Trump. Yet, the two candidates, their respective political parties, outside groups, and individual donors will, all told, spend around 86.5 million inflation-adjusted dollars more than on the 2012 presidential election. 
There simply must be a cheaper way to find two people no one wants to vote for. 
Hell, Giant Meteor ‚Äî the satirical no-party candidate whose slogan, ‚ÄúJust end it already,‚Äù speaks volumes to weariness ‚Äî has amassed a sizeable following in recent months, as the public apparently abandons all pretense of hope. 
To wit, 70 percent of voters likely to support Hillary Clinton, and a stunning 41 percent of all likely voters, would re-elect President Barack Obama if a third term were legal, Rasmussen reports. 
So, if the public harbors only negligible fondness for Clinton and Trump, who deems them worthy of such colossal sums? 
Unsurprisingly in both cases, it‚Äôs the elite ‚Äî the billionaires and millionaires directly, indirectly, and sometimes shadily channeling their fortunes in hopes their chosen nominee will conform policy favorable to their interests from the helm of the White House. 
OpenSecrets.org reports : 
‚ÄúCandidates and news outlets have decried the outsized influence of a small number of donors throughout the election season. Here‚Äôs a startling statistic: The top 100 families have given about 11.9 percent to the total $5.5 billion raised at this point, compared to 5.6 percent in all of 2012. Their names are familiar ‚Äî Thomas and Kathryn Steyer have given $57.2 million to liberal causes, Sheldon and Miriam Adelson have doled out $47.3 million to GOP-allied forces, and Donald Sussman has so far provided $34.4 million to groups helping Democrats. Expect to see this list shift a bit: Facebook cofounder Dustin Moskovitz pledged an additional $35 million to defeat Donald Trump earlier this month.‚Äù 
Perhaps surprisingly to some, billionaire globalist and avid political influencer, George Soros, ranked only 9th in the list of top donors, contributing ‚Äòjust‚Äô $17,689,038. 
And this election cycle hasn‚Äôt been devoid of grassroots political donors, either, although their chosen ‚Äòvoice of the people‚Äô candidate, Bernie Sanders, has since dropped out and endorsed Hillary Clinton ‚Äî leaving a legacy of heartbroken, if not embittered, young voters scrambling for options ‚Äî and an older, wiser generation hardly able to contain its I told you so‚Äôs . 
OpenSecrets continues: 
‚ÄúThe Obama re-election campaign floored campaign finance experts with his knack for tapping into the small donor pool ‚Äî an impressive 32 percent of the campaign‚Äôs total funds came from people giving $200 or less in 2012. But Bernie Sanders topped that. More than half of his contributions, or 59 percent, came from small donors, totaling $134.6 million, which is about one and half times as much as Clinton and more than twice as much as Trump.‚Äù 
Clearly, the so-called ‚Äòlittle guy‚Äô has nary the clout enjoyed by the privileged, moneyed class ‚Äî and though that might be widely understood ‚Äî the reminder dark horse candidates rarely make it to the big race seems more pronounced than ever in recent years. 
As an Independent, Sanders‚Äô decision to run on the Democratic ticket against the all-powerful Clinton, as a Democratic Socialist, no less, seemed fated to fail from the start ‚Äî after all, groundbreaking though his bid was, a populist David vowing to disassemble the corporate, banking Goliath could never reap the billionaire financing to endure. 
‚ÄúIn a campaign that has broken new ground in lots of ways, there‚Äôs at least one thing we can depend on, and that‚Äôs record-breaking spending on U.S. elections,‚Äù explained Sheila Krumholz, executive director of the Center for Responsive Politics. ‚ÄúWhile this campaign saw the rise of the small donor and a fall in spending reported by groups that hide their donors, overall, important trends hold true: More money is still coming from a tiny set of elite donors. It‚Äôs going to super PACs that are scarcely independent of the campaigns they support. And it‚Äôs targeting competitive races where the vote will be closest and the opportunity to have an impact, greatest.‚Äù 
In effect, the elite doles out its fortunes on elite candidates who will best serve the elite‚Äôs interests ‚Äî but, nah, take it from Hillary: the Russians might rig the election. 
Money is proportionate to influence, it could justifiably be argued, and in an election cycle costing upwards of $7 billion, the overwhelming majority of us pull literally zero weight. 
Finagling the popular vote constitutes little more than a distraction for the political heavyweights, whose insider connections and corporate and industry backers effectively usurped what little import the average American voter maintained when the Supreme Court ruled in their favor with the notorious corporations are people Citizens United decision in 2010. 
Whatever vociferous contention Clinton and Trump might garner from an unusually alert populace this election season, don‚Äôt be fooled ‚Äî the elite ultimately pull the strings ‚Äî the vote is merely an illusory construct of a severely limited choice between two candidates of their choosing. Not ours. 
All told, just remember the words of Mark Twain: 
‚ÄúIf voting made any difference they wouldn‚Äôt let us do it.‚Äù Share",thefreethoughtproject.com,1
FBI Sources: INDICTMENT in Hillary Investigation is ‚ÄòLIKELY‚Äô | Top Right News,"E-mail 
by Jason DeWitt | Top Right News 
Things just went from bad to much worse for Hillary Clinton as the Federal Bureau of Investigation (FBI) expands its investigation of the corrupt Clinton Foundation. 
Fox News Channel‚Äôs Bret Baier reported stunning developments about the Foundation investigation from two sources inside the FBI ‚Äî including that an ‚Äúindictment‚Äù is ‚Äúlikely‚Äù is the case. 
Baier reveals five important new pieces of information in these two short clips. 
The details: 
1. The Clinton Foundation investigation is far more expansive than anybody has reported so far and has been going on for more than a year. 
2. The laptops of Clinton aides Cherryl Mills and Heather Samuelson have not been destroyed, and agents are currently combing through them. The investigation has interviewed several people twice, and plans to interview some for a third time. 
3. Agents have found emails believed to have originated on Hillary Clinton‚Äôs secret server on Anthony Weiner‚Äôs laptop. They say the emails are not duplicates and could potentially be classified in nature. 
4. Sources within the FBI have told Baier that an indictment is ‚Äúlikely‚Äù in the case of pay-for-play at the Clinton Foundation, ‚Äúbarring some obstruction in some way‚Äù from the Justice Department. 
5. FBI sources say with 99% accuracy that Hillary Clinton‚Äôs server has been hacked by at least five foreign intelligence agencies, and that information have been taken from it. 
Transcript: 
BRET BAIER: Breaking news tonight ‚Äî two separate sources with intimate knowledge of the FBI investigations into the Clinton emails and the Clinton Foundation tell Fox the following: 
The investigation looking into possible pay-for-play interaction between Secretary of State Hillary Clinton and the Foundation has been going on for more than a year. Led by the white collar crime division, public corruption branch of the criminal investigative division of the FBI. 
The Clinton Foundation investigation is a, quote, ‚Äúvery high priority.‚Äù Agents have interviewed and reinterviewed multiple people about the Foundation case, and even before the WikiLeaks dumps, agents say they have collected a great deal of evidence. Pressed on that, one sources said, quote, ‚Äúa lot of it,‚Äù and ‚Äúthere is an avalanche of new information coming every day.‚Äù 
Some of it from WikiLeaks, some of it from new emails. The agents are actively and aggressively pursuing this case. They will be going back to interview the same people again, some for the third time. 
As a result of the limited immunity deals to top aides, including Cheryl Mills and Heather Samuelson, the Justice Department had tentatively agreed that the FBI would destroy those laptops after a narrow review. We are told definitively that has not happened. Those devices are currently in the FBI field office here in Washington, D.C. and are being exploited. 
The source points out that any immunity deal is null and void if any subject lied at any point in the investigation. 
Meantime, the classified e-mail investigation is being run by the National Security division of the FBI. They are currently combing through former Democratic Congressman Anthony Wiener‚Äôs laptop and have found e-mails that they believe came from Hillary Clinton‚Äôs server that appear to be new, as in not duplicates. 
Whether they contain classified material or not is not yet known. It will likely be known soon. All of this just as we move inside one week until election day. 
Baier gives more details to Fox News Channel‚Äôs Brit Hume. 
Transcript: 
BRET BAIER: Here‚Äôs the deal: We talked to two separate sources with intimate knowledge of the FBI investigations. One: The Clinton Foundation investigation is far more expansive than anybody has reported so far‚Ä¶ Several offices separately have been doing their own investigations. 
Two: The immunity deal that Cheryl Mills and Heather Samuelson, two top aides to Hillary Clinton, got from the Justice Department in which it was beleived that the laptops they had, after a narrow review for classified materials, were going to be destroyed. We have been told that those have not been destroyed ‚Äî they are at the FBI field office here on Washington and are being exploited. . 
Three: The Clinton Foundation investigation is so expansive, they have interviewed and re-interviewed many people. They described the evidence they have as ‚Äòa lot of it‚Äô and said there is an ‚Äòavalanche coming in every day.‚Äô WikiLeaks and the new emails. 
They are ‚Äúactively and aggressively pursuing this case.‚Äù Remember the Foundation case is about accusations of pay-for-play‚Ä¶ They are taking the new information and some of them are going back to interview people for the third time. As opposed to what has been written about the Clinton Foundation investigation, it is expansive. 
The classified e-mail investigation is being run by the National Security division of the FBI. They are currently combing through Anthony Weiner‚Äôs laptop. They are having some success ‚Äî finding what they believe to be new emaisls, not duplicates, that have been transported through Hillary Clinton‚Äôs server. 
Finally, we learned there is a confidence from these sources that her server had been hacked. And that it was a 99% accuracy that it had been hacked by at least five foreign intelligence agencies, and that things had been taken from that‚Ä¶ 
There has been some angst about Attorney General Loretta Lynch ‚Äî what she has done or not done. She obviously did not impanel, or go to a grand jury at the beginning. They also have a problem, these sources do, with what President Obama said today and back in October of 2015‚Ä¶ 
I pressed again and again on this very issue‚Ä¶ The investigations will continue, there is a lot of evidence. And barring some obstruction in some way, they believe they will continue to likely an indictment. 
Hillary is facing massive consequences for her misdeeds, which voters would do well to consider before electing her president. 
PLEASE SHARE this if you agree Hillary belongs in a prison cell‚Ä¶NOT the Oval Office‚Ä¶ ",toprightnews.com,1
PHOTO: Game Camera Catches Glimpse Of Possible 3-Antlered Buck,"Hillary Camp Caught on Camera Telling Tiny Crowd What to Cheer for 
The archer‚Äôs opportunity finally arrived on Oct. 8, when he went out to the field to hunt for prey . 
‚ÄúWhen the deer came out, there were about 10 deer in the field,‚Äù he said. ‚ÄúI was sitting there and a 4-point came out and walked right underneath me and went in the bean field. About a minute later, (the 12-point buck) came out and walked right underneath my stand. 
Without a moment to lose, Stubblefield fired a shot. 
‚ÄúHe ran about 80 yards and just dropped,‚Äù Stubblefield said. ‚ÄúIt was so dry, when he took off it looked like a smoke trail. He ran right out in the bean field and crashed. It looked like a bomb went off it was so dusty.‚Äù 
R.I.P. to the unnamed buck, though clearly he will not be forgotten anytime soon. 
Please share this story on Facebook and Twitter and let us know what you think about these odd but amazing bucks. What do you think about this? Scroll down to comment below! Advertisement Popular Right Now",conservativetribune.com,1
New Male Birth Control Method Tested - The Onion - America's Finest News Source,"Nation Puts 2016 Election Into Perspective By Reminding Itself Some Species Of Sea Turtles Get Eaten By Birds Just Seconds After They Hatch WASHINGTON‚ÄîSaying they felt anxious and overwhelmed just days before heading to the polls to decide a historically fraught presidential race, Americans throughout the country reportedly took a moment Thursday to put the 2016 election into perspective by reminding themselves that some species of sea turtles are eaten by birds just seconds after they hatch. Cleveland Indians Worried Team Cursed After Building Franchise On Old Native American Stereotype CLEVELAND‚ÄîHaving watched in horror as their team crumbled after a 3-1 World Series lead, members of the Cleveland Indians expressed concern Thursday that the organization has been cursed for building their franchise on an incredibly old Native American stereotype. Report: Election Day Most Americans‚Äô Only Time In 2016 Being In Same Room With Person Supporting Other Candidate WASHINGTON‚ÄîAccording to a report released Thursday by the Pew Research Center, Election Day 2016 will, for the majority of Americans, mark the only time this year they will occupy the same room as a person who supports a different presidential candidate. Nurse Reminds Elderly Man She‚Äôs Just Down The Hall If He Starts To Die DES PLAINES, IL‚ÄîAssuring him that she‚Äôd be at his side in a jiffy, local nurse Wendy Kaufman reminded an elderly resident at the Briarwood Assisted Living Community that she was just down the hall if he started to die, sources reported Tuesday. ",theonion.com,1
FREAK: Classroom Evacuated After Teacher Realizes Unshielded Uranium in Classroom,"Share on Facebook Share Tweet Email Print   
A school in Austria had to be suddenly evacuated after it was accidentally discovered that a rock that had long been on display in a classroom was actually composed of radioactive material. 
According to the U.K. Metro , the radioactive rock in a Salzburg school was actually a chunk of uranium , the same metallic element used to fuel nuclear reactors and produce atomic bomb warheads. 
The discovery came when an anti-nuclear lecturer visited the school to deliver a presentation about the danger of nuclear materials, bringing with him an old-school radium-illuminated watch and a Geiger counter to show students how it measured radioactivity, such as the small amount emanating from the watch.   PHOTOS: Everyday Home Hides Most Incredible 1950s Bomb Shelter We‚Äôve Ever Seen 
However, lecturer Thomas Neff soon realized that he was receiving abnormally high readings on the Geiger counter, so he began moving about the room in an effort to track down the anomaly. 
He eventually came upon a display of various rocks, minerals and fossils, in front of which his Geiger counter veritably ‚Äúexploded‚Äù off the charts, giving a reading of 102,000 counts per minute, about 100 times greater than the radioactivity of the watch, which itself was roughly 20 times greater than that which occurs normally. 
Neff quickly halted his lecture and alerted school authorities, who immediately began evacuating students and faculty while experts were brought in to assess the danger and rectify the situation.   
An alert was also sent out to other schools in the region, where subsequent searches of those locations turned up another 38 radioactive rocks in 11 different schools. 
The uranium rocks have now been gathered in a safe place, though there is no word whether any students or teachers became sick from their exposure to the potentially deadly material . 
Please share on Facebook and Twitter to spread this story about radioactive materials in the classroom.  ",conservativetribune.com,1
Life: Preparing For Change: This Woman Is Scrambling To Experience As Much Human Dignity As Possible Before The Trump Administration Takes Power,"Email 
The results of the 2016 election have left many women frightened for their futures in this country, but 29-year-old Vanessa Carp realizes that times of great upheaval call for decisive action: After realizing how dire the coming years could be for her, this quick-thinking young woman is scrambling to experience as much human dignity as possible before Donald Trump sets foot in the White House. 
Wow. Major props to Vanessa for getting herself prepared! 
With just a few short months before the president-elect comes to power, Vanessa has been taking herself out into the world every single day and soaking up as much human dignity as time permits. Whether she‚Äôs at home, at work, running errands, or even at the gym, Vanessa is seizing every chance to feel like a respected, autonomous human being before it all gets taken away from her on January 20. 
She now basks in the joy of visiting her doctor, unencumbered by the decisions others will soon make on her behalf. She relishes every interaction with her male coworkers while she still gets to feel like their peer. And she loves taking daily jogs around her neighborhood, savoring every last minute of the relative safety she temporarily still feels. 
‚ÄúIt‚Äôs getting down to the wire, but I‚Äôm just focused on hoarding as many normal human experiences as I can,‚Äù noted Vanessa, standing in the middle of her local grocery store and reveling in the feeling of having most of her civil rights intact. ‚ÄúI don‚Äôt know how many moments like these I‚Äôll get to have, so I figure I should stockpile some for the next four to eight years before it‚Äôs too late.‚Äù 
Smart move, Vanessa! 
It‚Äôs unfortunate that women even have to think about things like this in 2016, but we commend Vanessa for taking necessary precautions. Ladies, let‚Äôs all learn a lesson from Vanessa: If you‚Äôd like to experience a little human dignity before Trump is in the Oval Office, now is the time to do it!",clickhole.com,1
"Tyrus Wong, ‚ÄòBambi‚Äô Artist Thwarted by Racial Bias, Dies at 106","When Walt Disney‚Äôs ‚ÄúBambi‚Äù opened in 1942, critics praised its spare, haunting visual style, vastly different from anything Disney had done before. But what they did not know was that the film‚Äôs striking appearance had been created by a Chinese immigrant artist, who took as his inspiration the landscape paintings of the Song dynasty. The extent of his contribution to ‚ÄúBambi,‚Äù which remains a   mark for film animation, would not be widely known for decades. Like the film‚Äôs title character, the artist, Tyrus Wong, weathered irrevocable separation from his mother  ‚Äî   and, in the hope of making a life in America, incarceration, isolation and rigorous interrogation  ‚Äî   all when he was still a child. In the years that followed, he endured poverty, discrimination and chronic lack of recognition, not only for his work at Disney but also for his fine art, before finding acclaim in his 90s. Mr. Wong died on Friday at 106. A Hollywood studio artist, painter, printmaker, calligrapher,   illustrator and, in later years, maker of fantastical kites, he was one of the most celebrated   artists of the 20th century. But because of the marginalization to which   were long subject, he passed much of his career unknown to the general public. Artistic recognition, when Mr. Wong did find it, was all the more noteworthy for the fact that among Chinese immigrant men of his generation, professional prospects were largely limited to menial jobs like houseboy and laundryman. Trained as a painter, Mr. Wong was a leading figure in the Modernist movement that flourished in California between the first and second World Wars. In 1932 and again in 1934, his work was included in group shows at the Art Institute of Chicago that also featured Picasso, Matisse and Paul Klee. As a staff artist for Hollywood studios from the 1930s to the 1960s, he drew storyboards and made vibrant paintings, as detailed as any architectural illustrations, that helped the director envision each scene before it was shot. Over the years his work informed the look of animated pictures for Disney and   films for Warner Brothers and other studios, among them ‚ÄúThe Sands of Iwo Jima‚Äù (1949) ‚ÄúRebel Without a Cause‚Äù (1955) and ‚ÄúThe Wild Bunch‚Äù (1969). But of the dozens of films on which he worked, it was for ‚ÄúBambi‚Äù that Mr. Wong was  ‚Äî   belatedly  ‚Äî   most renowned. ‚ÄúHe was truly involved with every phase of production,‚Äù John Canemaker, an   animator and a historian of animation at New York University, said in an interview for this obituary in March. ‚ÄúHe created an art direction that had really never been seen before in animation. ‚Äù In 2013 and 2014, Mr. Wong was the subject of ‚ÄúWater to Paper, Paint to Sky,‚Äù a major retrospective at the Disney Family Museum in San Francisco. From the museum‚Äôs windows, which overlook San Francisco Bay, he could contemplate Angel Island, where more than nine decades earlier, as a lone    he had sought to gain admission to a country that adamantly did not want him. Wong Gen Yeo (the name is sometimes Romanized Wong Gaing Yoo) was born on Oct. 25, 1910, in a farming village in Guangdong Province. As a young child, he already exhibited a love of drawing and was encouraged by his father. In 1920, seeking better economic prospects, Gen Yeo and his father embarked for the United States, leaving his mother and sister behind. Gen Yeo would never see his mother again. They were obliged to travel under false identities  ‚Äî   a state of affairs known among Chinese immigrants as being a ‚Äúpaper son‚Äù  ‚Äî   in the hope of circumventing the Chinese Exclusion Act of 1882. Signed into law by President Chester A. Arthur, the act, which drastically curtailed the number of Chinese people allowed to enter the country, was among the earliest United States laws to impose severe restrictions on immigration. But in 1906, an unforeseen loophole opened in the form of the San Francisco earthquake and fire. Because a huge number of municipal documents, including birth and immigration records, were destroyed, many newly arrived Chinese capitalized on the loss, maintaining that they had been born in San Francisco before the fire. As United States citizens, they were entitled to bring over their relatives  ‚Äî   or, in the case of Gen Yeo and his father, ‚Äúpaper sons‚Äù posing as relatives. Attuned to the deception, United States immigration officials put Chinese arrivals through a formidable inquisition to ensure they were who they claimed to be. The questions came like gunfire: In which direction does your village face? How many windows are in your house? Where in the house is the rice bin? How wide is your well? How deep? Are there trees in your village? Are there lakes? What shops can you name? The sponsoring relative was interrogated separately, and the answers had to match. For the new arrival, a major mistake, or a series of smaller ones, could mean deportation. To stand a chance of passing, aspirants memorized rigorous dossiers known as coaching papers. The ensuing interrogation was hard enough for adults.    Gen Yeo would undergo it alone. On Dec. 30, 1920, after a month at sea, the Wongs landed at Angel Island Immigration Station. The elder Mr. Wong was traveling as a merchant named Look Get his son as Look Tai Yow. ‚ÄúAngel Island is considered to be the Ellis Island of the West Coast,‚Äù Lisa See, the author of ‚ÄúOn Gold Mountain‚Äù (1995) a nonfiction chronicle of her   family, said in an interview in 2016. However, she continued: ‚ÄúThe goal was really very different than Ellis Island, which was supposed to be so welcoming. Angel Island opened very specifically to keep the Chinese out. ‚Äù Because Mr. Wong‚Äôs father had previously lived in the United States as Look Get, he was able to clear Immigration quickly. But as a new arrival, Gen Yeo was detained on the island for nearly a month, the only child among the immigrants being held there. ‚ÄúI was scared half to death I just cried,‚Äù Mr. Wong recalled in ‚ÄúTyrus,‚Äù an   documentary directed by Pamela Tom, which premiered in 2015. ‚ÄúEvery day is just miserable  ‚Äî   miserable. I hated that place. ‚Äù On Jan. 27, 1921, in the presence of an interpreter and a stenographer, young Gen Yeo, posing as Look Tai Yow, was interrogated by three inspectors. His father had already been questioned. Gen Yeo was well prepared and answered without error. In Sacramento, where he joined his father, a schoolteacher Americanized ‚ÄúTai Yow‚Äù to ‚ÄúTyrus,‚Äù and he was known as Tyrus Wong ever after. Soon afterward, father and son were separated once more, when the elder Mr. Wong moved to Los Angeles to seek work. For reasons that have been lost to time, he could not take his son. Tyrus lived on his own in a Sacramento boardinghouse while attending elementary school. Two years later  ‚Äî   possibly more  ‚Äî   Tyrus traveled to Los Angeles to join his father, who had found work in a gambling den. They lived in a   boardinghouse sandwiched between a butcher shop and a brothel. After school, Tyrus worked as a houseboy for two Pasadena families, earning 50 cents a day. His first art teacher was his father, who trained him nightly in calligraphy by having him dip a brush in water and trace ghostly characters on newspaper: They could not afford ink or drawing paper. When Tyrus was in junior high, a teacher, noting his drawing talent, arranged a summer scholarship to the Otis Art Institute in Los Angeles. By his own account an indifferent student in public school, Tyrus found his calling at the institute, now the Otis College of Art and Design. When his scholarship ended he declined to return to junior high. His father scraped together the $90 tuition  ‚Äî   a small fortune  ‚Äî   to let him stay on as Otis‚Äôs youngest student. He studied there for at least five years, simultaneously working as the school janitor, before graduating in the 1930s. Not long afterward his father died, leaving young Mr. Wong entirely on his own. From 1936 to 1938, Mr. Wong was an artist for the Works Progress Administration, creating paintings for libraries and other public spaces. With friends, including the   artist Benji Okubo, he founded the Oriental Artists‚Äô Group of Los Angeles, which organized exhibitions of members‚Äô work  ‚Äî   an   level of exposure for Asian artists at the time. Mr. Wong, newly married and needing steady work, joined Disney in 1938 as an ‚Äú‚Äù creating the thousands of intermediate drawings that bring animated sequences to life. Asians were then a novelty at Hollywood studios, and Mr. Wong was made keenly aware of the fact, first at Disney and later at Warner Brothers. One   flung a racial epithet at him. Another assumed on sight that he worked in the company cafeteria. Then there was the affront of the  ‚Äôs job itself: Painstaking, repetitive and for Mr. Wong quickly   it is the   work of animation  ‚Äî   ‚Äúa terrible use of his talents as a landscape artist and a painter,‚Äù Mr. Canemaker said. A reprieve came in the late 1930s, when Mr. Wong learned that Disney was adapting ‚ÄúBambi, a Life in the Woods,‚Äù the 1923 novel by the Austrian writer Felix Salten about a fawn whose mother is killed by a hunter. In trying to animate the book, Disney had reached an impasse. The studio had enjoyed great success in 1937 with its animated film ‚ÄúSnow White and the Seven Dwarfs,‚Äù a baroque production in which every detail of the backgrounds  ‚Äî   every petal on every flower, every leaf on every tree  ‚Äî   was meticulously represented. In an attempt to use a similar style for ‚ÄúBambi,‚Äù it found that the ornate backgrounds camouflaged the deer and other forest creatures on which the narrative centered. Mr. Wong spied his chance. ‚ÄúI said, ‚ÄòGee, this is all outdoor scenery,‚Äô‚Äù he recalled in a video interview years afterward, adding: ‚ÄúI said, ‚ÄòGee, I‚Äôm a landscape painter! ‚Äô‚Äù Invoking the exquisite landscape paintings of the Song dynasty (A. D. 960 ‚Äî  1279) he rendered in watercolors and pastels a series of nature scenes that were moody, lyrical and atmospheric  ‚Äî   at once lush and spare  ‚Äî   with backgrounds subtly suggested by a stroke or two of the brush. ‚ÄúWalt Disney went crazy over them,‚Äù said Mr. Canemaker, who wrote about Mr. Wong in his book ‚ÄúBefore the Animation Begins: The Art and Lives of Disney Inspirational Sketch Artists‚Äù (1996). ‚ÄúHe said, ‚ÄòI love this indefinite quality, the mysterious quality of the forest. ‚Äô‚Äù Mr. Wong was unofficially promoted to the rank of inspirational sketch artist. ‚ÄúBut he was more than that,‚Äù Mr. Canemaker explained. ‚ÄúHe was the designer he was the person they went to when they had questions about the color, about how to lay something out. He even influenced the music and the special effects: Just by the look of the drawings, he inspired people. ‚Äù Mr. Wong spent two years painting the illustrations that would inform every aspect of ‚ÄúBambi. ‚Äù Throughout the finished film  ‚Äî   lent a brooding quality by its stark landscapes misty, desaturated palette and figures often seen in silhouette  ‚Äî   his influence is unmistakable. But in 1941, in the wake of a bitter employees‚Äô strike that year, Disney fired Mr. Wong. Though he had chosen not to strike  ‚Äî   he felt the studio had been good to him, Mr. Canemaker said  ‚Äî   he was let go amid the lingering climate of   resentments. On ‚ÄúBambi,‚Äù Mr. Wong‚Äôs name appears, quite far down in the credits, as a mere ‚Äúbackground‚Äù artist. Mr. Wong joined Warner Brothers in 1942, working there  ‚Äî   and lent out on occasion to other studios  ‚Äî   until his retirement in 1968. The indignities he endured were not confined to the studios. Trying to buy a house, he and his wife, the former Ruth Kim, were told that each property they inquired about had just been sold. ‚ÄúThen in a month you‚Äôd go back there and the sign was still there,‚Äù Mr. Wong recalled in ‚ÄúTyrus. ‚Äù After the Japanese attack on Pearl Harbor in December 1941, Mr. Wong, like many   took to wearing a lapel button proclaiming his heritage, lest an angry American beat him up on the street. The war permanently dispersed the fledgling Oriental Artists‚Äô Group. Mr. Wong‚Äôs friend Mr. Okubo was sent, with tens of thousands of other   to an internment camp. ‚ÄúIf World War II hadn‚Äôt happened when it did, I think these artists, even the   artists, would have more of a name than they do today,‚Äù Ms. See said. ‚ÄúAnd that‚Äôs because this little movement that had just barely started was split apart by the war. ‚Äù Mr. Wong, who became a United States citizen in 1946, also designed Christmas cards for Hallmark and painted elegant   designs on dinnerware, now sought after by collectors. A longtime resident of Sunland, Calif. he became, in retirement, a renowned kitemaker, designing, building and hand coloring astonishing, airworthy creations  ‚Äî   butterflies, swallows, whole flocks of owls, centipedes more than 100 feet long  ‚Äî   that streaked the Southern California sky like paint on blue canvas. During the last 15 years of Ruth Wong‚Äôs life, when she was ill with dementia, Mr. Wong forsook his work to care for her. After her death in 1995, he slowly began making art again. In 2001, in formal recognition of his influence on ‚ÄúBambi,‚Äù Mr. Wong was named a Disney Legend. The honor  ‚Äî   whose previous recipients include Fred MacMurray, Julie Andrews and Annette Funicello  ‚Äî   is bestowed by the Walt Disney Company for outstanding contributions. In 2003, a retrospective of his work, curated in part by Ms. See, was the inaugural exhibition at the Chinese American Museum in Los Angeles. The Disney Family Museum‚Äôs retrospective, ‚ÄúWater to Paper, Paint to Sky,‚Äù traveled in 2015 to the Museum of Chinese in America, in Lower Manhattan. Mr. Wong‚Äôs death, at his home in Sunland, was confirmed by the filmmaker Ms. Tom. His survivors include three daughters, Kay Fong,   Wong and Kim Wong and two grandchildren. When his daughters were small, Mr. Wong encouraged them to make art, as his father had encouraged him. Yet he would not let them have coloring books. The reason was simple: He did not want his children constrained, he said, by lines laid down by others.",New York Times,0
Suicide Bombing in Baghdad Kills at Least 36,"BAGHDAD  ‚Äî   A suicide bomber detonated a pickup truck loaded with explosives on Monday in a busy Baghdad market, killing at least 36 people hours after President Fran√ßois Hollande of France arrived in the Iraqi capital. The Islamic State later claimed responsibility for the attack. The bomb went off in a produce market that was packed with day laborers, a police officer said, adding that another 52 people were wounded. During a news conference with Mr. Hollande, Haider   Iraq‚Äôs prime minister, said the suicide bomber had pretended to be a man seeking to hire day laborers. Once the workers gathered around, he detonated the vehicle. The Islamic State, also known as ISIS or ISIL, claimed the attack in a statement circulated on a website that is often used by the group. It was the third such attack in three days in or near Baghdad, underscoring the lingering threat posed by the extremist group despite a string of setbacks for it elsewhere in the country over the past year, including in and around the northern city of Mosul. The attack took place in Sadr City, a vast Shiite district in eastern Baghdad that has been repeatedly targeted by Sunni extremists since the 2003   invasion. Militiamen loyal to the Shiite cleric Moktada   were seen evacuating bodies in their trucks before ambulances arrived. Bodies were scattered across the bloody pavement alongside fruit, vegetables and laborers‚Äô shovels and axes. A minibus filled with dead passengers was on fire. Asaad Hashim, 28, an owner of a nearby cellphone store, described how the laborers had pushed and shoved around the bomber‚Äôs vehicle, trying to get hired. ‚ÄúThen a big boom came, sending them up into the air,‚Äù said Mr. Hashim, who suffered shrapnel wounds to his right hand. He blamed ‚Äúthe most ineffective security forces in the world‚Äù for failing to prevent the attack. An angry crowd cursed the government, even after a representative of Mr. Sadr tried to calm them. Late last month, the Iraqi authorities started removing some of the security checkpoints in Baghdad in a bid to ease traffic for the capital‚Äôs six million residents. ‚ÄúWe have no idea who will kill at any moment and who‚Äôs supposed to protect us,‚Äù said Ali Abbas, a    father of four who was hurled over his vegetable stand by the blast. ‚ÄúIf the securities forces can‚Äôt protect us, then allow us to do the job. ‚Äù Several smaller bombings elsewhere in the city on Monday killed at least 20 civilians and wounded at least 70, according to medics and police officials. All officials spoke on the condition of anonymity because they were not authorized to speak to reporters. The United States State Department condemned the attacks. Separately, the American military announced on Monday the death of a coalition service member in Iraq in a ‚Äú  incident,‚Äù without providing further details. Mr. Hollande met with Mr. Abadi and President Fuad Masum, and later traveled to the   northern Kurdish region to meet with French troops and local officials. He pledged to help displaced Iraqis return to Mosul, where Iraqi forces are waging a large offensive against the Islamic State. France is part of the   coalition formed in 2014 to fight the Islamic State after the extremist group seized large areas in Iraq and neighboring Syria. France has suffered multiple attacks claimed by the extremist group.",New York Times,0
"Sick With a Cold, Queen Elizabeth Misses New Year‚Äôs Service","LONDON  ‚Äî   Queen Elizabeth II, who has been battling a cold for more than a week, missed a New Year‚Äôs Day church service at her country estate in Sandringham, Buckingham Palace said on Sunday. A week earlier, the queen, who is 90, missed a Christmas Day church service, for the first time since 1988, because of the illness. ‚ÄúThe Queen does not yet feel ready to attend church as she is still recuperating from a heavy cold,‚Äù the palace said in a statement. The queen‚Äôs husband, Prince Philip, who had also been ill, was well enough to attend both services, in the church at Sandringham, which is in Norfolk, on the east coast of England. The queen, who ascended to the throne in 1952, became the world‚Äôs   monarch following the death of King Bhumibol Adulyadej of Thailand in October. She is also Britain‚Äôs   monarch, having last year surpassed Queen Victoria‚Äôs   reign. Her mother lived until the age of 101.",New York Times,0
Rift Between Officers and Residents as Killings Persist in South Bronx,"After the bullet shells get counted, the blood dries and the votive candles burn out, people peer down from   windows and see crime scenes gone cold: a band of yellow police tape blowing in the breeze. The South Bronx, just across the Harlem River from Manhattan and once shorthand for urban dysfunction, still suffers violence at levels long ago slashed in many other parts of New York City. And yet the city‚Äôs efforts to fight it remain splintered, underfunded and burdened by scandal. In the 40th Precinct, at the southern tip of the Bronx, as in other poor, minority neighborhoods across the country, people long hounded for   infractions are crying out for more protection against grievous injury or death. By September, four of every five shootings in the precinct this year were unsolved. Out of the city‚Äôs 77 precincts, the 40th has the highest murder rate but the fewest detectives per violent crime, reflecting disparities in staffing that hit hardest in some neighborhoods outside Manhattan, according to a New York Times analysis of Police Department data. Investigators in the precinct are saddled with twice the number of cases the department recommends, even as their bosses are called to Police Headquarters to answer for the sharpest crime rise in the city this year. And across the Bronx, investigative resources are squeezed. It has the highest   rate of the city‚Äôs five boroughs but the thinnest detective staffing. Nine of the 14   precinct detective squads for violent crime in the city are there. The borough‚Äôs robbery squad is smaller than Manhattan‚Äôs, even though the Bronx has had 1, 300 more cases this year. And its homicide squad has one detective for every four murders, compared with one detective for roughly every two murders in Upper Manhattan and more than one detective per murder in Lower Manhattan. In   lobbies and   family apartments, outside methadone clinics and art studios, people take note of the inequity. They hear police commanders explain that they lack the resources to place a floodlight on a dangerous block or to post officers at a   corner. They watch witnesses cower behind   doors, more fearful of a gunman‚Äôs crew than confident in the Police Department‚Äôs ability to protect them. So though people see a lot, they rarely testify. And in the South Bronx, as in so many predominantly black and Hispanic neighborhoods like it in the United States, the contract between the police and the community is in tatters. Some people have stories of crime reports that were ignored, or 911 calls that went unanswered for hours. Others tell of a 911 call for help ending in the caller‚Äôs arrest, or of a minor charge leading to 12 hours in a fetid holding cell. This is the paradox of policing in the 40th Precinct. Its neighborhoods have historically been prime targets for aggressive tactics, like    that are designed to ward off disorder. But precinct detectives there have less time than anywhere else in the city to answer for the blood spilled in violent crimes. Gola White, who was beside her daughter when she was shot and killed in a playground this summer, four years after her son was gunned down in the same housing project, ticked off the public safety resources that she said were scant in Bronx neighborhoods like hers: security cameras, lights, locks, investigating police officers. ‚ÄúHere, we have nothing,‚Äù she said. When it comes to ‚Äú  families,‚Äù she said, the authorities ‚Äúdon‚Äôt really care as much. That‚Äôs how I feel. ‚Äù The Times has been documenting the murders logged this year in the 40th Precinct, one of a handful of neighborhoods where deadly violence remains a problem in an era of   crime in New York City. The homicides  ‚Äî   14 in the precinct this year, up from nine in 2015  ‚Äî   strain detectives, and when they go unsolved, as half of them have this year, some look to take the law into their own hands. From hundreds of conversations with grieving relatives and friends, witnesses and police officers, the social forces that flare into murder in a place like the 40th Precinct become clearer: merciless gang codes, mental illness, drugs and long memories of feuds that simmered out of officers‚Äô view. The reasons some murders will never be solved also emerge: paralyzing fear of retribution, victims carrying secrets to their graves and relentless casework that forces detectives to move on in hopes that a break will come later. Frustrations build on all sides. Detectives‚Äô phones rarely ring with tips, and officers grow embittered with witnesses who will not cooperate. In the meantime, a victim‚Äôs friends conduct their own investigations, and talk of grabbing a stash gun from a wheel well or a mother‚Äôs apartment when they find their suspect. In the chasm between the police and the community, gangs and gun violence flourish. Parents try to protect their families from drug crews‚Äô threats, and officers work to overcome the residue of years of mistrust and understaffing in communities where they still go racing from one 911 call to the next. The streets around St. Mary‚Äôs Park were the scene of two fatal shootings logged in the 40th Precinct this year. Both are unsolved. James Fernandez heard talk of the murders through the door of his   apartment on East 146th Street in a     the Betances Houses. He lived at the end of a long hallway strewn with hypodermic needles, empty dope bags and discarded Hennessy bottles. A   young men who spoke of being in a subset of the Bloods gang had made it their drug market, slinging marijuana and cocaine to regulars, flashing firearms and blowing smoke into the Fernandez apartment. When Mr. Fernandez, 40, asked the young men to move, they answered by busting up his car. This kind of crime, an anachronism in much of New York, still rattles the 40th Precinct, even though murders there have fallen to 14 this year from 83 in 1991. It has more major felony crimes per resident than any other residential district in the city. It is also one of the poorest communities in the country, and many young men find their way into underground markets. Mr. Fernandez was not one to shrink from the threats. When he was growing up on the Lower East Side, he rode his bicycle around to the customers of the drug dealers he worked for and collected payments in a backpack. After leaving that life, he got a tech maintenance job and, three years ago, moved into the Betances Houses with his wife and daughter, now 11. He had two choices to get help with the drug crew: call the police for help and risk being labeled a snitch, or call his old Lower East Side bosses for muscle and risk violence. He chose the police. Again and again, he walked into a local substation, Police Service Area 7, and asked for protection. His daughter was using an inhaler to relieve coughs from the marijuana smoke. Mr. Fernandez and his wife got terrible headaches. ‚ÄúThere‚Äôs a lot of killers here, and we are going to kill you,‚Äù a sergeant‚Äôs police report quoted a    telling Mr. Fernandez in August 2015. A second report filed the same day said a    warned him, ‚ÄúI‚Äôm going to shoot through your window. ‚Äù Mr. Fernandez told the police both the teenagers‚Äô names, which appear in the reports, and then went home. He said one of their friends had seen him walk into the substation, and they tried to intimidate him out of filing another report. Three days later, the same    propped his bike on their door, ‚Äúthen said if I was to open the door and say something, they would body slam me,‚Äù Mr. Fernandez‚Äôs wife, Maria Fernandez, wrote on slips of paper she used to document the hallway ruckus and the inadequate police response. The boys made comments about how easy a target she was and about how they would have to ‚Äúslap‚Äù her if she opened the door while they made a drug sale, and they threatened to beat the Fernandez family because ‚Äúthey are the ones snitching,‚Äù her notes say. But another   complaint at the substation, 10 days after the first, brought no relief. A week later, feeling desperate, Ms. Fernandez tried calling: first to the substation, at 8:50 p. m. when one of the boys blew weed smoke at her door and made a   threat to attack her, and then to 911 at 10:36 p. m. The police never came, she wrote in her notes. She tried the 40th Precinct station house next, but officers at the desk left her standing in the public waiting area for a   she said, making her fear being seen again. Officers put her in worse danger some months later, she said, when they came to her door and announced in front of the teenagers that they were there on a complaint about drug activity. Mr. Fernandez started doing the work that he said the police had failed to do. He wired a camera into his peephole to record the drugs and guns. The footage hark back to the New York of the 1980s, still very much present to some of the precinct‚Äôs residents. Around 6:30 each morning, Sgt. Michael J. LoPuzzo walks through the tall wooden doors of the 40th Precinct station house. The cases that land on his metal desk  ‚Äî   dead bodies with no known cause, strip club brawls, shooting victims hobbling into the hospital themselves  ‚Äî   bring resistance at every turn, reminding him of an earlier era in the city‚Äôs   campaign. ‚ÄúI haven‚Äôt got one single phone call that‚Äôs putting me in the right direction here,‚Äù said Sergeant LoPuzzo, the head of the precinct‚Äôs detective squad, one day this summer as he worked on an answer to an email inquiry from a murder victim‚Äôs aunt about why the killer had not been caught. ‚ÄúAnd people just don‚Äôt understand that. ‚Äù Often it is detectives who most feel the effects of people turning on the police. Witnesses shout them away from their doors just so neighbors know they refuse to talk. Of the 184 people who were shot and wounded in the Bronx through early September, more than a third  ‚Äî   66 victims  ‚Äî   refused to cooperate. Over the same period in the 40th Precinct, squad detectives closed three of 17 nonfatal shootings, and 72 of 343 robbery cases. Part of the resistance stems from   preventive policing tactics, like    that were a hallmark of the     style under former Mayor Michael R. Bloomberg and his police commissioner, Raymond W. Kelly. Near the height of the    strategy, in 2012, the 40th Precinct had the   stops in the city, the   stops in which officers used force and the most frisks. Of 18, 276 stops that year, 15, 521 were of people who had done nothing criminal. The precinct was also one of the   areas that the department flooded with its newest officers. At roll calls, they were pressured to generate numbers: write tickets and make arrests. They had no choice but to give a summons to a young man playing in a park after dark, even if the officers had done the same growing up in the same neighborhood. ‚ÄúI need to bring something in today to justify my existence,‚Äù Officer Argenis Rosado, who joined the precinct in 2010, said in an interview at the station house. ‚ÄúSo now you‚Äôre in a small area, and day after day you‚Äôre hammering the same community. Of course that community‚Äôs eventually going to turn on you. ‚Äù The pressure warped the way officers and residents saw each other. Rookies had to ignore why someone might be drinking outside or sitting on a stoop. ‚ÄúSome of the cops that came out at that time probably viewed the community differently, too,‚Äù said Hector Espada, a   veteran of the precinct. ‚ÄúNot because they wanted to, but because they had to. Because some way or somehow, you can‚Äôt give someone a $115 summons and feel like you guys could still have a civil conversation after that. ‚Äù Morale wilted in the aged station house on Alexander Avenue, in Mott Haven. Officers felt pressure to downgrade crime complaints to make them appear less serious. Several said in interviews that they had overlooked crime reports from immigrants because they were seen as unlikely to complain, and watched supervisors badger victims into repeating their stories in hopes that they would drop their complaints. The practice of downgrading complaints resulted in the disciplining of 19 officers in the precinct last year, one in a string of scandals that has left officers there feeling overscrutinized for problems that also existed elsewhere. Four commanders in the precinct were sent packing in five years, one of them after officers were found to be ‚Äúticket fixing,‚Äù or forgiving parking tickets for friends, and another after he was recorded giving guidance on whom to stop and frisk: black boys and men, ages 14 to 21. Some officers fled to other commands. Others became reluctant to take assignments in proactive policing units, like   that put them in   situations on the street. ‚ÄúWhenever I walked through the doors of the precinct, to me, it seemed like a black cloud,‚Äù said Russell Lewis, a    of the 40th. ‚ÄúIt was like a heaviness. When you walked in, all you wanted to do was do your 8 hours 35 minutes and go home, because you didn‚Äôt want to get caught up in anything. ‚Äù The precinct covers only about two square miles, but the more than a dozen housing projects there mean that it overflows with people. Methadone clinics draw addicts from around the city.   lofts on the southern edge of the precinct presage a wave of gentrification. Even as the Police Department has hired 1, 300 more officers for neighborhood policing and counterterrorism, officers in the 40th Precinct said they could still rush to 25 911 calls during a shift  ‚Äî   a number unchanged from what the new police commissioner, James P. O‚ÄôNeill, said he was handling in a similar South Bronx precinct 15 years ago. Several dozen calls at a time can be waiting for a response. Residents know that if you want the police for a domestic problem, it helps to hint that there is a weapon. Last year, the precinct drew the   number of civilian complaints for officer misconduct in the city, and the most lawsuits stemming from police actions. The precinct is trying to improve morale under a new commanding officer, Deputy Inspector Brian Hennessy. A cadre of what the department calls neighborhood coordination officers has been on patrol since last January, part of a citywide effort under Mr. O‚ÄôNeill and Mayor Bill de Blasio to bring back the beat cop, unencumbered by chasing every last 911 call, who can listen to people‚Äôs concerns and help with investigations. The precinct has made among the most gun arrests in the city, and officers said they now had more discretion to resolve encounters without a summons or an arrest. At one corner near a school, on Courtlandt Avenue and East 151st Street, that has long spawned complaints about gunfire and fights, Inspector Hennessy and some of his officers painted over graffiti and swept up drug paraphernalia this summer. People said it was the first answer to their complaints in years. But the inspector acknowledged that the residue of   policing lingers. ‚ÄúThat perception really sticks,‚Äù he said. The workload in the 40th Precinct is startling and reveals a gap in how detective squads are equipped to answer violent crime in Manhattan compared with the Bronx, Brooklyn and Queens. Three of the precinct‚Äôs 16 detectives are carrying more than 400 cases each this year, and many others have loads in the high 300s, even though the department advises 150 in violent precincts. When they are assigned a homicide, they typically have four days to investigate before dealing with other cases. Quieter precincts can give detectives a month with little distraction to investigate a murder. Detectives in the 40th Precinct have each handled an average of 79 violent felonies this year through    ‚Äî   murders, rapes, felony assaults and robberies. By contrast, a detective in the precinct on the southern end of Staten Island carries nine such cases a detective in the precinct patrolling Union Square and Gramercy Park handles 16 and a detective in the precinct for most of Washington Heights handles 32, the citywide median. Last year, the 40th was the    for violent crime, with 65 cases per detective. In the Bronx as a whole, a precinct detective has carried an average of 58 violent felonies this year, compared with 27 in Manhattan, 37 in Brooklyn, 38 in Queens and 25 on Staten Island. Rape cases and robbery patterns are later sent to more specialized units, but precinct detectives do extensive initial work to interview victims, write reports and process evidence. Precincts in much of Manhattan, which are whiter and wealthier than the South Bronx, often have more property felonies, like stolen laptops or credit cards, and the police say those can be complex. But even accounting for those crimes, the 40th Precinct has some of the heaviest caseloads of overall crime per detective in the city. Michael Palladino, the head of the Detectives‚Äô Endowment Association and a former Bronx officer, said staffing disparities affected the department‚Äôs efforts to build trust in communities like the South Bronx. Witnesses make a calculation, he said: ‚ÄúIf I cooperate with the detectives, there‚Äôs so much work, there‚Äôs so few of them there, they won‚Äôt even get the chance to protect me, or they‚Äôll be there too late when the retaliation comes. ‚Äù Sergeant LoPuzzo, who turned down a more prestigious post to stay in the 40th Precinct, said that his squad worked tirelessly to handle cases with the people he had, and that while every squad wanted more detectives, staffing needs for counterterrorism units and task forces had created new deployment challenges across the department. ‚ÄúWe fight with the army we have, not the army we wish we had,‚Äù he said. Details of how the Police Department assigns its 36, 000 officers are closely held and constantly in flux, and the public has minimal information on how personnel are allocated. Presented with The Times‚Äôs analysis of confidential staffing data, the department‚Äôs chief of detectives, Robert K. Boyce, vowed to send more detectives to the 40th Precinct and said the department would reassess its deployment more broadly in troubled precincts. He said a recent decision to bring gang, narcotics and vice detectives under his command made it easier to shift personnel. Chief Boyce said the burdens on detectives went beyond felony crimes to include   and   cases. And he noted the support that precinct squads got from centralized units focusing on robberies, gangs or grand larcenies, for example. Major crime keeps pounding the 40th Precinct, at rates that in 2015 were only a tenth of a percent lower than in 2001, even as citywide crime dropped by more than a third over the same period. But the precinct‚Äôs detective squad shrank by about eight investigators during those years, according to staffing data obtained from the City Council through a Freedom of Information Law request. The squad covering Union Square and Gramercy Park, where crime dropped by a third over that period, grew by about 11 investigators. (The 40th Precinct was given an additional detective and four   investigators this summer, when it was already missing three detectives for illness or other reasons.) Retired detectives are skeptical that community relations alone can drive down crime in the city‚Äôs last ‚Äú‚Äù the busiest precincts. Rather, they say, the Police Department should be dedicating more resources to providing the same sort of robust investigative response that seems standard in Manhattan. ‚ÄúAny crime in Manhattan has to be solved,‚Äù said Howard Landesberg, who was a 40th Precinct detective in the late 1980s. ‚ÄúThe outer boroughs are, like, forgotten. ‚Äù Retired detectives said that understaffing made it harder to solve crimes in the Bronx, Brooklyn and Queens, where the higher prevalence of gang and drug killings already saddled investigators with cases in which people were not inclined to cooperate. Through   detectives had closed 67 percent of homicides in Manhattan and 76 percent of those in Staten Island this year, compared with 54 percent of those in the Bronx, 42 percent of those in Queens and 31 percent of those in Brooklyn. Of last year‚Äôs homicides, detectives cleared 71 percent in Manhattan, 63 percent in the Bronx, 62 percent in Queens, 57 percent in Staten Island and 31 percent in Brooklyn. ‚ÄúIt‚Äôs the culture of the Police Department that they worry about Manhattan,‚Äù said Joseph L. Giacalone, a former sergeant in the Bronx Cold Case Squad, in part ‚Äúbecause that‚Äôs where the money is. ‚Äù He added: ‚ÄúWhen de Blasio came in, he talked about the tale of two cities. And then he‚Äôs done the complete opposite of what he said. It‚Äôs just business as usual. ‚Äù The Bronx‚Äôs struggles extend into prosecutions. In each of the last five years, prosecutors in the Bronx have declined to prosecute violent felony cases more than anywhere else in the city. And the rate of conviction in the Bronx is routinely the lowest in the city as well, but has ticked up this year to surpass Brooklyn‚Äôs rate through November as Bronx prosecutors work to streamline cases. Some cases have become even more difficult to win because of the   problem in the 40th Precinct, which has allowed defense lawyers to attack the credibility of officers who were implicated, said Patrice O‚ÄôShaughnessy, a spokeswoman for the Bronx District Attorney‚Äôs office. The district attorney, Darcel D. Clark, elected in 2015, said in a statement, ‚ÄúI was a judge here in the Bronx, and I heard from jurors that they can‚Äôt be impartial because they don‚Äôt trust the police. ‚Äù Against that tide of mistrust, Sergeant LoPuzzo‚Äôs detectives work 36 hours straight on some fresh cases. They buy Chinese takeout with their own money for a murder suspect. They carry surveillance videos home in hopes that their personal computers may enhance them better than a squad computer. They buy an urn for a homeless mother who has her murdered son‚Äôs ashes in a box. In the months after a killing, they can seem like the only people in this glittering city who are paying attention to the 40th Precinct‚Äôs homicide victims. Newly fatherless children go back to school without a therapist‚Äôs help. Victims‚Äô families wander confused through a courthouse and nearly miss an appearance. Newspapers largely ignore killings of people with criminal pasts, pushing them down the priority lists of the   chiefs at Police Headquarters. In a stuffy   squad room, the detectives of the 40th Precinct grapple with an inheritance of government neglect. They meet mothers who believe their sons might never have been murdered had a city guidance counselor listened to pleas to help them stay enrolled, or had a city housing worker fixed the locks or lights on a building. And the detectives work alongside a vicious system on the streets for punishing police cooperators. Young men scan court paperwork in prison, looking for the names of people who turned on them. One murder victim in the precinct this year was cast out of his crew after he avoided being arrested with them in a gang takedown some believed he was cooperating. A longtime 40th Precinct detective, Jeff Meenagh, said a witness in a homicide case was going to testify until he went back to his neighborhood and was told that anyone who testified would ‚Äúget what you deserve. ‚Äù The allies Sergeant LoPuzzo makes are friendly only for so long. He helped clear a woman‚Äôs son of a robbery charge by locating surveillance video that proved he was not the robber. The mother started calling with tips under a code name  ‚Äî   about a gun under a car, for example. But she always refused to testify. And she cut ties this year after Sergeant LoPuzzo arrested her son in the stabbing of two people and her   in a shooting. New York City owns East 146th Street and the   buildings on each side. But James Fernandez, in the Betances Houses, said the reality on the ground was different: The drug boss ran the block. By October, Mr. Fernandez was increasingly afraid  ‚Äî   and fed up. Mr. Fernandez and his wife went so far as to give officers keys to the building door, so they could get in whenever they wanted, showed them the videos and offered them   access to his camera so they could see what was happening in the hallway. A couple of officers said they needed a supervisor‚Äôs permission to do more. Others answered that the young men were only making threats. Officers occasionally stopped outside their building, causing the young men to scatter, but did not come inside, Mr. Fernandez said. The menacing worsened. Mr. Fernandez‚Äôs daughter was harassed as she arrived home from school. She grew more and more distressed, and her parents had her start seeing a therapist. Mr. Fernandez made several complaints at the office of the borough president, Ruben Diaz Jr. and visited a victim‚Äôs advocate in the district attorney‚Äôs office. On Oct. 20, 2015, he sent an online note to the police commissioner‚Äôs office. ‚ÄúWe went to all proper channels for help,‚Äù the note said. ‚ÄúBoth precincts failed us, except 2 officers who helped us, but their hands are tied. No one else to turn to. I have months of video of multiple crimes taking place and we are in extreme danger. ‚Äù ‚Äú40th and PSA 7 won‚Äôt do anything,‚Äù he wrote, referring to the local substation. ‚ÄúPlease we need to speak to some one with authority. ‚Äù The local substation commander, Deputy Inspector Jerry O‚ÄôSullivan, and the Bronx narcotics unit were alerted to the complaints. But Mr. Fernandez said he never heard from them. So he relied on his own street instincts to protect his family. He made pleas to a man he thought was employing the dealers in the hallway. The activity quieted briefly, but it returned after the young men rented a room in a woman‚Äôs apartment upstairs. Mr. Fernandez approached a different man who he learned was the boss of the operation. The man agreed to ask the dealers to calm down. He even hired a drug customer to sweep the hallway, Mr. Fernandez said. But two weeks later, the dealing and the harassment resumed. So he went to his old Lower East Side bosses, who hired men to trail his wife and daughter on their way out of the building and make sure they made it safely to school. At other times they sat outside the Betances Houses. He also bought two bulletproof vests, for about $700 each. He could not find one small enough for his daughter. ‚ÄúI have no faith in the City of New York, I have no faith in the police, I have no faith in the politicians,‚Äù Mr. Fernandez said. ‚ÄúThe only thing I know for sure: God, if we‚Äôre in a situation again, I will be left to defend my family. ‚Äù Paying such close attention to what was happening in the hallway, Mr. Fernandez said he learned some details about two recent homicides that the 40th Precinct was investigating. But because his calls for help were going nowhere, he said he decided not to put himself in greater risk by talking: He would not tell the police what he had learned. ‚ÄúI‚Äôm bending over backward, and nobody‚Äôs not even doing anything,‚Äù he said. ‚ÄúWhy am I going to help you, if you ain‚Äôt going to help me?‚Äù By last January, a new neighborhood coordination officer was working with residents of the Betances Houses, and ended up with the most arrests in his housing command, Inspector O‚ÄôSullivan said. Chief Boyce said that the silos in which gang and narcotics detectives used to work made responding to complaints more difficult, but that the recent restructuring would remove those obstacles. ‚ÄúNo one should live like Mr. Fernandez lived, with people dealing drugs outside of his apartment,‚Äù he said. Mr. Fernandez‚Äôs complaints did not spur any arrests, but two men from the hallway were caught separately this year in shootings. One of them, whom Mr. Fernandez named in a police report, was charged this summer with hitting an officer with a metal folding chair and firing three gunshots into a crowd, court papers say. He is being held on Rikers Island on an attempted murder charge. That was too late for Mr. Fernandez. By May, he had moved his family away.",New York Times,0
Life: Chaos: This Coffee Shop Is Also An Art Gallery,"Citizens of the four corners of the world, take heed, and behold what horror descends upon us. An unholy paradox has revealed itself. Within its walls, utter chaos reigns: This coffee shop is also an art gallery. 
Insanity. There can be no godly reason for such an abomination. 
Lo, for here the oceans shall fall out of sight and paintings shall be with coffee and coffee shall be with pastries. The coming and going of a restroom key affixed to a wooden ladle marks the only indication that time passes as the mind sits frozen, unable to comprehend. 
How can it be so? What vile architect of insanity can have devised this place? 
A Lumineers album plays in its entirety while patrons settle in to a collective nonchalance that suggests either total defeat or a complicity in this wickedness. They offer nothing, seated at their wooden tables, typing on MacBooks and drinking coffees as paintings leer from behind them. Elsewhere, a father turns to son, offering him a coffee as they admire a painting of Nighthawks reimagined with Coen brothers characters. 
Truly, this is bedlam. 
Above the lattes, art looms. A lone merchant stands behind a concrete slab where two tip jars enjoin visitors to choose between Sex Pistols or The Descendents, but for what are we tipping? 
Hot, caffeinated broth sloshes above a chalkboard that favors a soup of the day and Wi-Fi password. A small sticker that says ‚Äú$350‚Äù marks an oil depiction of a sunset, but on another wall, a sunrise on canvas bears no marking at all. Could the latter be mere decoration? And if everything has a price in this mayhem, could it be that the paintings in the bathroom are also for sale? 
All sense has fled this place. The mind knows when it has confronted something that simply cannot be. 
To comprehend such a chaotic nightmare is futile. The boundary between Earth and the netherworld is not separated by gates after all, but a door. And on that door, a sign that says ‚ÄúOpen daily from 6 a.m. to midnight,‚Äù bidding all enter and willingly fall to madness.",clickhole.com,1
Pollsters Admit They Underestimated Voters‚Äô Adrenal Glands - The Onion - America's Finest News Source,"Man Wearing ‚ÄòJewmerica‚Äô T-Shirt Never Dreamed He‚Äôd See This Day SAND SPRINGS, OK‚ÄîFeeling a mixture of intense pride and abject disbelief after news networks called the 2016 presidential election in favor of Donald Trump, local man Terry Williams, who is currently wearing a T-shirt adorned with the word ‚ÄúJewmerica,‚Äù told reporters late Tuesday night that he never dreamed he‚Äôd see this day during his lifetime. Nation Throws Off Tyrannical Yoke Of Moderate Respect For Women WASHINGTON‚ÄîPolitical experts are hailing Donald Trump‚Äôs historic presidential victory early Wednesday as a resounding declaration that the nation is finally ready to cast off the tyrannical yoke of moderate respect for women that has suffocated the citizens of this country for generations. Nation Elects First Black-Hearted President WASHINGTON‚ÄîShattering a barrier long thought unbreakable in the United States, Donald Trump, the 70-year-old billionaire real estate mogul from New York, became the first black-hearted man in history to win the American presidency, in the early hours of Wednesday morning. Nation‚Äôs Optimists Need To Shut The Fuck Up Right Now WASHINGTON‚ÄîSaying their rosy attitude about the state of the election was not helping anything given what is currently transpiring, sources confirmed Tuesday night that the nation‚Äôs optimists need to seriously shut the fuck up as soon as humanly fucking possible. Anderson Cooper Informs Viewers CNN Just Minutes Away From First Significant Piece Of Information Of Day NEW YORK‚ÄîRoughly two hours into the network‚Äôs live nine-hour-long ‚ÄúElection Night In America‚Äù programming block, CNN anchor Anderson Cooper informed viewers Tuesday evening he is only moments away from delivering the first piece of genuinely significant information of the day. ",theonion.com,1
Challenges to Digital Forensics: A Survey of Researchers & Practitioners Attitudes and Opinions ,"aware and reduce the forensic opportunity (i.e. The operating systems that forensically wipe files upon deletion). The format of the survey was kept simple yet effective so that there is an optimum balance between the time spent on the survey, the efforts required to complete it, while giving enough opportunity for the participants to express their views in the most succinct and reasonable manner possible. To achieve this, a number of trial runs of the survey were undertaken by local researchers in the domain acting as participants. From the feedback obtained, the survey was further refined and optimised. The use of a Likert-scale was utilised extensively in order to maximise the information provided through a quantitative approach. It was felt, whilst a qualitative-based research methodology would provide a richer set of results, fewer participants would be willing to take part. As such a quantitative approach was selected, with the option (in numerous places) for the participant to provide further information. The survey was published online, and the researcher shortlisted potential participants from the identified stakeholder total, 128 communities who were emailed directly. In invitations were sent out for completion amongst the international community (a number of which were to groups rather than individuals). A total of 42 (split between 19 academic researchers and 23 practitioners) completed the survey and were subsequently analysed. Whilst this was not an overly large respondent level, given the highly targeted nature of the survey, it was considered not an unreasonable sample size from which to proceed to analyse. III. SURVEY RESULTS The survey was divided into four sections: demographics, the current forensic capabilities, the future challenges and legislative concerns. The results presented initially are based upon the response from the complete population; however, where appropriate, further analysis based upon the stakeholder perspectives of researchers and practitioners is presented. A. Demographics An analysis of the demographics illustrates the respondents are on the whole well educated with a significant level of experience – both in terms of years within the domain and holding relevant professional qualifications. Of the 19 researcher responses all have a postgraduate degree, as did 48% of the practitioners. Notably however, 65% of practitioners also had one or more relevant professional qualifications – as would be expected due to the nature of the role. Furthermore, as illustrated in Figure 1, 55% of the respondents have 3 or more years experience. This demonstrates the respondent population is a relevant and experienced mix of academic researchers and practitioners appropriate to provide informed opinions. Figure 1. Respondent Experience within the Domain B. The Role of Digital Forensics and Current Tool this section was felt about digital Capabilities to explore how The purpose of respondents currently forensics and specifically with regards to what the current limitations were and how well tools are able to provide the necessary functionality. An initial question asked respondents how important their background and expertise, it was expected that the response would be favorable towards it being important; however, it was asked to explore their opinion within the wider context and later responses. The respondents overwhelming (98%) ranked it 4 or 5 out 5. they felt digital forensics was. Given to provide a comparison to The respondents were also asked to check what they feel were key limitations. The list being derived from an examination of literature and with the additional option of permitting the respondent to add any missing limitations. Overall, the volume of data to be analyzed (74%) and the time taken (67%) are considered the key limitations, with the remaining factors obtaining less than 50% of the response. Further analysis based upon the stakeholders perspectives (i.e. researcher and practitioner), as illustrated in Figure 2, show that proportionally researchers feel there are a greater number of limitations than their practitioner counterparts. Whilst they agree with the volume of data, more researchers felt the time taken to undertake investigations was a limiting factor. Another significant difference of opinion appears with the automation of forensic analyses – with double the proportion of researchers (63%) identifying it as a key limitation over the practitioners (30%). The only category where a greater proportion of practitioners felt there was a limitation was the legal aspects. The reasons for this are unclear; however, it is likely practitioners have to contend with the legislative aspects in reality on a more regular basis than researchers and are thus subject to the difficulties. Figure 2. Principal Limitations to Investigations Figure 3. Current Tool Capability in releasing more interestingly, 7 of Further examining the opinions of the stakeholders, participants were asked if restricting or limiting investigations to cases of high importance could be a viable approach as it would assist investigative resources. Overall, only 9 of the 42 responses felt this would be a useful policy; those responses were from practitioners. There are clear reasons why such a policy would be difficult to invoke in practice – how would one decide what was important until after the investigation? Furthermore, relying upon triage tools or automation alone could introduce an unacceptable level of error. However, it is clear that the volume of data and time taken results in investigations is becoming increasingly and prohibitively expensive. With the explosion of data and technologies, at what point does the traditional model of digital forensics become infeasible. An analysis of current tool capabilities reveals an interesting perspective. Overall, the consensus is that tools reside in the middle of spectrum (neither too few or fit for purpose) with a slight skew towards being more capable than not (as illustrated in Figure 3). However, exploring capability based upon the forensic category (e.g. computer, network, embedded and mobile) suggests an order to the capability, with computer forensic tools being largely fit for purpose and subsequent categories declining in capability, in the order of mobile, network and finally embedded. This trend arguably matches the market maturity, with computer and mobile-based tools being far more widespread. It is notably that few feel any of the tools are completely fit for purpose. A further examination based upon stakeholder perspectives reveals the same trend. This is somewhat contradictory, as it might be expected that researchers (who are specifically tasked with developing solutions that current tools do not have the capability for and are therefore potentially more aware of the missing capabilities) would have resulted in the selection of poorer current capabilities. However, that was not the case. impact upon software that can have an concerned with were malicious Participants were asked a series of questions regarding how concerned they were with respect to a range of technologies forensic investigations. As illustrated in Figure 4, on average all of the technologies were skewed towards being concerned, with over 60% of respondents selecting 4 or 5 out of 5 in 5 out of the 7 categories. The two categories that that respondents appeared less and steganography. It is unclear as to the reasons for this but perhaps they are at opposite sides of the same spectrum – investigators have been working with malicious software since the inception of cybercrime, and therefore are comfortable with the nature of steganography, its rather undetectable nature, it is not perceived to a problem because it (appears) not to exist. Arguably both encryption and steganography are a form of anti-forensics; however, given their significance, it was deemed important to extract them and have them considered in their own right. technology. Conversely, given the Figure 4. Technologies that Cause Concern Upon asking respondents to rank the technologies, a slight difference results within the stakeholder perspectives. As illustrated in Table 1, researchers feel that cloud computing is the single highest priority, whereas researchers feel it is third after anti-forensics and encryption. Whilst both stakeholders agree encryption is a key issue, they differ on the remaining priority – with the pace of technological change. researchers suggesting Table 1. Ranked Technologies that Cause Concern 1 Priority 2 3 Encryption = Pace of Technology Encryption Cloud Computing Researchers Practitioners Cloud Computing Anti- Forensics Anti- Forensics Cloud Computing Encryption Overall To provide a better insight into the experiences of the respondent population (as this will likely impact upon their responses), participants were asked on how frequently they had undertaken investigations involving more advanced data hiding techniques. Of those to whom the question was applicable (i.e. had undertaken investigations) 66% responded that they had infrequently, with only 24% stating frequently. None had undertaken them on a very frequent basis. Whilst digital forensic challenges are well understood and documented, this finding suggests the impact of many of them has not yet manifested itself practically. This would suggest that whilst literature highlights the many technological issues that exist and the possible opportunities open to criminals, they are not currently utilizing them. Furthermore, the resultant issues that investigators focus upon are therefore those that have an immediate impact upon them. Finally, respondents were asked how they felt towards the use of open source utilities and tools. Significantly, 69% of respondents consider open source tools as an important or very important aspect of tool development. Given the rapid pace of technology, it will become increasingly difficult to rely upon commercial vendors to provide all the necessary solutions, particularly in a timely manner. C. The Future Challenges A key aspect of the survey was to focus upon the (perceived) future challenges to digital forensics, with a view of enabling researchers to priorities them. Respondents were asked a couple of preliminary questions in this regard – whether they felt the field of forensics would be more or less challenging, whether investigations would take more or less time and whether investigations would be more sophisticated. Overwhelmingly, 93% of participants felt investigations would be more challenging the future and more sophisticated, with 67% also of the mind it would take more time. Given the current challenges identified in the previous section, this presents a rather bleak outlook. Interestingly, 19% of participants did feel investigations in the future would take less time. Unfortunately, a subsequent question did not follow this up to understand why they felt this. Arguably however, given the increases in cases, increases in technology, increases in the volume of data and increases in the sophistication of analysis required, given current technology capabilities this will have to result in an increase in the resources required. in Exploring exactly where respondents felt increases would result gave rise to a set of interesting results – as illustrated in Figure 5. All participants, independent of their role, felt that investigators would experience an increase in mobile-based cases. However, with the remaining categories, a greater proportion of researchers than practitioners felt there would be increases. The largest difference between stakeholders opinions was with regards to the role embedded forensics will have – with 59% of researchers envisaging an increase in demand but is accompanied with only 30% of practitioners. this Figure 5. Increase in Forensic Investigations Participants were also asked to rank the future challenges. Many of the categories were a duplicate from the current challenges, in addition to a number of broader categories such as tool capability, visualization and social networks. An analysis of Figure 6 reveals a similar result to the earlier challenge question – with cloud computing, anti-forensics and encryption remaining high priorities. This is interesting from the perspective that participants not only feel these are current issues but that no tools or solutions exist (or arguably are on the horizon) to solve them in the future. This raises further questions are to why this is the case? In reality a mix of technological barriers and legislative impediments are likely reasons. Further down the list and challenges such as social networks, forensic analyses and network forensics come through as key priorities. Figure 6.The Relative Ranking of Future Challenges An analysis based upon stakeholder perspectives (as illustrated in Figure 7) show a similar outlook with respect to the key priorities such as cloud computing and encryption. However, they do differ with respect to a number of other priorities. Researchers feel anti-forensics, steganography and visualization to be less significant than practitioners. Instead, they feel social networking, digital forensic tools, forensic analyses and legislation as more significant priorities. Figure 7. The Relative Stakeholder Ranking of Future Challenges Given the challenges that exist, a number of questions were posed to participants with a view of examining their opinion towards approaches or technologies that could have an impact upon the challenges. Initially, they were asked if relevant having evidence/artifacts was (83%), respondents felt this was a essential (with an average Likert score of 4.2). identify and extract important. Overwhelming the ability to Figure 8. Importance of Identify Relevant Artifacts A further question then asked whether criminal profiling could be an approach to identifying relevant artifacts. Theoretically, profiling offers the investigator the opportunity to more easily identify and extract relevant evidence. As illustrated in Figure 9, 85% of participants thought this was an important (or very important) area (average Likert score of 4). Figure 9. The Role/Application of Criminal Profiling in Digital Forensics Two questions were then focused upon approaches that aid the aforementioned approaches. Overall, participants were very positive towards the use of such technologies. They were asked if they felt automation was an important factor in the future. Automation has the potential to remove a considerable volume of manual tasks. As illustrated in Figure 10, 58% of respondents felt it was important. Figure 10. The Role and Importance Automation can have within Digital Forensics The second question, specifically asked them whether artificial intelligence (AI) approaches could have a role in assisting investigators. Overall, the response was more mixed with an average Likert score of 3.7 - suggesting a slight skew towards using the approach. Whilst the use of AI has been successfully applied in a range of fields, including aspects of forensics, it will inevitably introduce errors which themselves would require verification by an investigator. Finally, respondents were asked about the relationship between the researcher and practitioner communities – specifically whether improved communication and collaboration. Overwhelming, 93% of respondents felt the domain would benefit (Likert score of 4.6). As illustrated in Figure 11, the question resulted in the highest proportion selecting 5 out of 5 on the scale. it would benefit from  Figure 11. Improved Communication & Collaboration Between Stakeholders An analysis of stakeholder responses to these questions revealed a close similarity, with no significant difference in opinions. This felt a little at odd to what would have been expected. Given these challenges are well known, it would have been expected that researchers would be more actively looking to such technologies to reduce the problem. D. Legislative Aspects The last section sought to explore how legislative aspects have an impact upon digital forensics. Asked whether they felt legislation was an inhibitor or enabler, participants were fairly even in their consideration – with an average Likert score of 3.3. As illustrated in Figure 12, this suggests a significant divide in the impact that legislation has. Whilst it is encouraging there is a very mild skew towards being an enabler, a 25% of respondents feel it is specifically an inhibiting factor. Figure 12. The Extent to Which Legislation is an Enabler or Inhibitor Investigating the issue further, respondents were asked to what extent they felt legalization involving crossing borders was the issue. As illustrated in Figure 13, respondents did feel legalization involving different jurisdictions was a concern (74% and a average Likert score of 4). Figure 13. The Extent to Which Cross Border Legislation is a Barrier is required to The final question sought to examine to what extent current tools are capable of considering the legislative requirements. For example, limiting search and automated analysis to specific and permitted data areas but not others. This in countries where warrants provide permissions to analyze specific data areas and not others. For example, the USA has strong privacy laws that restrict investigators in what they analyze based upon what the suspect criminal activity is. Interestingly, respondents in general felt existing tools were capable of achieving this (as illustrated in Figure 14). However, to the author’s best knowledge, none of the commercial tools are specifically equipped with take relevant national legislation into consideration. For example, data carving analyses will extract data from the complete image – not defined areas. It also requires the investigator to specifically understand the remit of the investigation and the nature of relevant legislation. the capability Figure 14. Forensic Tools Capable of Considering Legislation IV. DISCUSSION The survey has raised a number of considerations for the forensic community. Fundamentally, it has highlighted the importance and prioritization of key challenges. The domain of anti-forensics, including encryption and steganography, remain a significant current and future challenge for investigators. Unfortunately, with people becoming more mindful of information security, the further introduction of security within operating systems (e.g. full disk encryption within Microsoft and Apple Mac), the promotion of anti- to locate forensic technologies and wiping of media, the ability for investigators relevant evidence will become increasingly challenging. As suggested by [4], forensic tools will have to become less focused upon identifying evidence that is criminal in its own right and become more intelligent in identifying artifacts that highlight misuse. For example, analyses will need to be developed that are capable of picking up the application of anti-forensic techniques. Whilst not evidence of the crime itself, it will provide invaluable intelligence to the examiner to how to best proceed. trace evidence to There was an expectation when designing the survey tool, that given the wide range of challenges that exist within the domain, the responses to questions such as the current tool capabilities and current technology concerns would have revealed a higher proportion with the feeling that current approaches are not meeting the requirements. However, this was not the case. Without further information, it is difficult to suggest the reason for this; however, there does appear to be a disparity between the challenges and the impact they actually have in practice. For example, upon asking participants, what proportion of cases involved advanced data-hiding only 7 of the 42 respondents had on a frequent basis. If challenges are not manifesting themselves in practical cases, it is logically to assume these will have less of an impact. this issue resulted reducing The respondents have reaffirmed that the volume of data and the time taken to undertake an investigation are key limiting factors. Further exploring techniques and technologies for potentially favorably suggesting future research should focus upon approaches such as criminal profiling and automation. In particular, [7] has presented an interesting application of criminal profiling to digital forensics. A key enabling technology in many other fields, AI, has provided approaches to data mine large volumes of data, have strong pattern associative capabilities and result in detection performances that outperform many other approaches [8-10]. There is some obvious concern over the applicability of such a technology within forensics; however, with careful implementation and configuration, AI techniques could become useful tools. It is likely such tools were reside initially within triage, with investigators still utilized to confirm their findings. However, as they advance and trust and reliability is established, AI-based approaches would be useful in dealing with standard cases (e.g. those that do not deploy advanced data hiding techniques). The need for better communication and collaboration between researcher and practitioner communities is imperative for the creation of effective solutions particularly against the increasing range of challenges. Moreover, the nature of these challenges is incorporating a wider range of technology-based knowledge – forensics is no longer focused merely hard drive analysis but increasingly needing to understand complex distributed systems and innovative applications such as cloud computing and social networking. Researchers have the necessary knowledge but often lack the practitioner insight into problem. Interesting, comparing the previous work by [4] with this research, almost a decade on, and some key differences and similarities appear. Whilst education/training was not an option provided in the list of current concerns, notably, it was not added in by any of the participants. Furthermore, an analysis of the respondents suggests a well educated and trained population. This suggests that training and education is an issue the industry has managed to contend with. However, issues of technology, encryption and tools highlighted in 2004 remain current and future challenges almost a decade on. The findings have also highlighted significantly the increasing attention being given to developing areas of technology. Cloud computing and social networking were ranked in the top 5 future concerns. In one respect, the reason for this might be due to the changing location of the artifacts – moving away from the tried and tested hard drive onto systems that are complex, proprietary and not well understood by the forensics community [11]. However, these technologies also represent a second potentially more disconcerting trend – the behavior of Increasingly individuals are no longer tied to a single computer, but instead have a multitude of devices and technologies with which they can access information. This will only increase and forensic investigators, procedures and tools are not designed for such examinations. is changing. individuals V. CONCLUSIONS & FUTURE WORK the Exploring The paper has presented a study that has both confirmed a number of challenges within digital forensics and also provided an understanding over their relative priority. Interestingly, amongst the usual suspects of anti-forensics and encryption, cloud computing and social networking have been identified as key future challenges. issues from the differing stakeholder perspectives of the research and practitioner did reveal some interesting aspects – with researchers focusing upon the new challenges and practitioners remaining focused upon issues that still have a direct impact. With the exception of cloud computing – which both parties agreed was the single top priority, practitioners highlighted for both the current concerns and future challenges, anti-forensics and encryption in the top three issues. However, for researchers, social networking and tool capability were amongst the top priority future challenges. Future work must focus upon developing effective approaches to solving these challenges and provide a robust research environment that is proactively developing forensic- based solutions before practitioners identify them as issues. Future work must focus upon the technology horizon rather than merely “fire fighting” the issues. Consideration to forensic capabilities of evolving and new technologies needs to take place, so that a forensic capability exists at the outset of a new technology rather than years after the technology has been adopted (and misused!). ",Scientific Journal,0
Farm owner arrested for protesting Dakota Access Pipeline's theft of her land,"Farm owner arrested for protesting Dakota Access Pipeline's theft of her land 
  Vicki Batts Tags: eminent domain , Dakota Access Pipeline , civil liberties (NaturalNews) There have been countless stories across the web that have documented those struggling to fight against the Dakota Access Pipeline. Native Americans, farmers, ranchers, landowners and environmental activists have joined together to oppose the pipeline and bring the project to a grinding halt.Just recently, 127 Native American activists were arrested at a construction site near the Standing Rock Sioux reservation in North Dakota. Despite the corporate media's attempts to suppress information surrounding the controversial pipeline, concerns about the Dakota Access project have become increasingly visible.However, one issue has remained in the dark: the government's abuse of eminent domain.Eminent domain is what gives the government the right to seize private, law-abiding citizens' properties for public use. Normally this means a road, a school or another public structure that will be used by the citizens of a town or which in some way serves them. However, the government can also seize private land if they can prove that it will provide some sort of ""public good."" This loophole ‚Äì and yes, it is a loophole ‚Äì has allowed our government to steal land from taxpayers for years, and then give it to a private citizen or company. Such is the case with the Dakota Access Pipeline.This pipeline is not a public project; it is being conducted by a private company that will profit off their endeavors. And yet, in spite of all the land that will be destroyed and the people whose lives will be disrupted by this project, the government has still seen fit to steal land and give it to the pipeline's creators under the guise of ""public good.""In Iowa, a major conflict between farmers and the government has broken out. You see, the state government is trying to take their land, and consequently, their livelihoods, to make way for the pipeline .The state's decision to hand over land owned by farmers to the Dakota Access LLC has been met with quite a lot of opposition ‚Äì as it should have. Just last week, Calhoun County farmer Cyndy Coppola was arrested on her own property for protesting the government's usurpation of her own land .Apparently, they expected farmers to idly sit back and watch them steal what they worked so hard for.Cyndy Coppola's arrest has been ignored by the mainstream media, probably because it doesn't align with their narrative that Big Government knows best. An injurious move at best, her arrest highlights everything wrong with the way our government is being run.The theft of privately owned land simply cannot be ignored any longer ‚Äì it is unconstitutional for the government to give a tax-paying citizen's land to a multi-billion dollar energy company so that they can make even more money. And it won't just be the oil and gas companies that profit off this disgusting display of government; dozens of big banks and investment firms will also be taking home a piece of the pie. Food and Water Watch reported in September than 17 financial institutions loaned Dakota Access LLC a whopping $2.5 billion to launch their project. They noted that banks have also given a substantial number of resources to the Energy Transfer Family of companies, such as billion-dollar credit lines and revolving credits. In total, a staggering 38 banks have given this corporation over $10 billion in loans and credits.How many of those banks do you think have ties to our government? Goldman Sachs is on the list, as are several other infamous industry names.If you think the pipeline is being constructed to better our country, you're wrong. It's being constructed to make money for a group of elitist corporations ‚Äì and they plan on taking down anyone and anything that stands in their way. Sources:",naturalnews.com,1
Life: Career Goals FTW: 6 Tips For Asking The Man Whose Hand Is Currently On Your Knee For A Raise,"Email His hand might be on your leg, but right now, you‚Äôre in the driver‚Äôs seat! 
1. Phrase your request carefully: It‚Äôs absolutely imperative that when you ask the man who currently has his hand on your knee for a raise, you are respectful and conscious. Nail down every word in your request beforehand, so that when the time comes, the man with his hand on your knee is inspired, but not threatened. 
2. Anticipate your talking points: Ask yourself: Is your employer data-conscious? Is your parent company in a good financial position to give out raises right now? Are five sweaty fingers simply resting on your knee, or are they moving around, toying playfully with the fabric on your pants? As you‚Äôre sitting across from the man controlling those fingers, make sure you‚Äôre constantly asking yourself these types of questions, because if you do, you‚Äôre guaranteed to step up your negotiation skills tenfold! 
3. Assume a stance that is confident but also gently pushes his hand off your knee: Whether you‚Äôre crossing your legs to subtly shake his hand off your knee or swiveling your chair to guide his palm onto the armrest of your chair, try to use your body to signal confidence, rather than looking fidgety. After all, visual cues are just as important as verbal ones. So when you sit down in his office and notice the hand on your leg or rubbing your upper arm, look like the valuable employee you truly know you are! 
4. Make sure to ask for an exact salary before the man‚Äôs fingers start moving up your leg: It might seem daunting at first to directly ask for money, but if you don‚Äôt come with an exact number in mind, the man with his hand on your knee will end up confused, with little direction information about how much of a bump to give you. Calculate an appropriate new salary so you can bargain, but when you do, avoid mentioning the salaries of any other coworkers whose legs he has also touched, because this is about you, not them! 
5. Don‚Äôt mention the hand: A real negotiator stays on target and doesn‚Äôt let day-to-day work conversations creep into business. Mentioning or even looking at the hand is a surefire way to upset the man with his hand on your knee and bring the meeting to a close without getting a raise. Keep your message laser-focused! 
6. Be prepared for a no: Look, you might not get a raise this time, but don‚Äôt worry, it‚Äôs not personal! That‚Äôs just business, and sometimes you have to ask once, twice, or even three more times before the man with his hand on your knee finally gives you a raise. And if he doesn‚Äôt, who knows? Might be time for a new job!",clickhole.com,1
Digital Forensic Readiness in the Cloud,"I. INTRODUCTION The primary objectives of this research can be summarised as follows: Shortening the DFI process by quickening the acquisition of data through the use of a remote and central log server. The secondary objectives include the synchronization of timestamps and the ﬁltering and indexing of log evidence on the server to quickly identify where the logs came from. Digital Forensic Science is deﬁned by Palmer[1], as “the use of scientiﬁcally derived and proven methods toward the preser- vation, collection, validation, identiﬁcation, analysis, interpre- tation, documentation and presentation of digital evidence derived from digital sources for the purpose of facilitating or furthering the reconstruction of events found to be criminal, or helping to anticipate unauthorized actions shown to be disruptive to planned operations.” Normally a DFI follows a search and seizure approach. This entails seizing suspected devices for acquisition and analysis of data. Acquisitions is normally done by making a bit-by- bit copy of the storage medium of a device. However in a cloud environment this is not a simple task to perform. The cloud holds some challenges for digital forensic investigators that is discussed by Barbara[2]. One such challenge discussed by Birk[3] includes gaining access to the physical hardware running the cloud instance as the physical location of devices is often unknown, making search and isolation of devices difﬁcult impossible. The time taken to complete a Digital Forensic Investigation(DFI) has the potential to be one of the biggest challenges of the investigation due to if not time-line constraints and deadlines, and could determine the success of the investigation. This poses the challenge to the investigators to attempt to complete the investigation or at least part of it as fast as possible. The cloud service providers also has full control over the sources of evidence and also the companies assets, making investigations by corporate security teams difﬁcult if not impossible. Barbara concludes that the challenge for digital forensic examiners, with regard to cloud computing environments, is to determine the who, what, when, where, how, and why of cloud-based criminal activity[2]. The cloud holds further challenges such as data and pro- cess provenance. The history of digital objects, such as who accessed a speciﬁc object and when it was accessed are stored using meta-data that is referred to as data and process provenance. This can be crucial information in a DFI, and can be a question that remains unanswered if no supporting logs are made available. The traditional model for a DFI was not designed with is worth investigating the cloud in mind and therefore it whether this traditional model can be optimized or improved to better suit cloud computing environments. Shortening the evidence acquisition phase already improves on the problem of time which is a major concern as Tan has shown[4]. Hence the research question that this paper addresses is as follows: How can we use digital forensic readiness to help to shorten the digital forensic investigation phase in cloud computing environments? The remainder of this paper is structured as follows. Section 2 discusses the history and background leading up to this re- search being undertaken. Section 3 discusses the requirements and implementation of a prototype solution. Section 4 is a discussion section and section 5 concludes with the results obtained from this work. II. BACKGROUND The background section discuss the history of both digital forensics and cloud computing and how it came into existence. As the Internet grew over the years, the number of crimes committed using digital devices grew as well. In response to this growing number of digital crimes, digital forensics also referred to as computer forensics emerged[1]. A. Digital Forensics Murphy deﬁnes Digital Forensics as the application of science to the identiﬁcation, collection, analysis and exami- nation of digital evidence, whilst preserving the integrity of the information and maintaining a strict chain of custody for the evidence[5]. The goal of computer forensics according to Yasinsac, and Manzano[6], is to provide sufﬁcient evidence to allow the prosecutor to successfully prosecute the perpetrator. The DFI process involves ﬁve steps or phases according to Cohen[7]. These steps are identiﬁcation, collection, trans- portation, storage, examination and presentation. Each of these steps are discussed next. The goal of the identiﬁcation process is, to identify relevant evidence so that it can be collected and processed. Only evidence deemed to be legal and useful in building a case should be collected for analysis. This is referred to as the proportionality rule. The proportionality rule also states that only upon good cause can there be discovery of computerized data[8]. This is to protect clients from unreasonable searches. Next the acquisition of evidence is undertaken. The col- lected evidence has to be transported and stored in a secure location. The integrity of the evidence as well as that of the chain of custody must be maintained at all times in order for the evidence to be admissible and acceptable. The process of examination involves Analysis, Interpretation, Attribution and Reconstruction. Evidence can be produced by many sequences of events. When trace evidence is associated with a speciﬁc program or event, this evidence can be analysed in a set. The analysis phase will focus on identifying trace evidence associated with sets of traces. Evidence may suggest that a certain event occurred but in fact the evidence could have been produced by malicious code. Hence it is very important during the interpretation of evidence to establish, if the user in question is actually the one who committed the crime. This is done by identifying anchor events during the attri- bution phase. Anchor events are events that can draw clear lines between the physical world and the digital world. Non- repudiation is the characteristic that draws the line between physical and digital world. When it can be proved that a certain event took place and it cannot be claimed that it did not take place. Sometimes a higher level of certainty is required than what is given from analysis of logs and records. Then an examiner will perform reconstruction. The goal is to reconstruct the sequence of events to determine the state at which the system was at the time of the incident and to verify evidence. At the conclusion of an investigation, the evidence has to be presented to a judicial body. This can be done in the form of a report or testimony to judges or juries. Such a report should be as simple as possible to ensure that the jury are informed and can make an informed decision. Technical jargon must be avoided at all cost. Traditionally in computer forensics, an investigator pulls the plug on the machine, after which the disk or discs are imaged for analysis at a later stage[9]. Isolating a device is often necessary in order to perform an investigation as discussed by Delport[10], however isolation becomes a challenge in the cloud. It is not necessarily possible to pull the plug in a cloud computing environment. Even identifying suspected physical computers becomes much harder when considering the virtualization of resources in the cloud. computers sus- pected to be involved in a crime can not simply be seized for investigation therefore the post event driven model of investigating is inadequate in cloud computing environments. Hence, traditional computer forensics fails in cloud computing. B. Digital Forensics in the Cloud Digital forensics in the cloud requires a proactive approach; being prepared for expected litigations or disputes in advance. The proactive approach taken in this research is DFR. This research will attempt to apply digital forensic readiness to a cloud instance as part of the solution to improve on the traditional approach of a DFI and address the challenge posed by isolation in the cloud. DFR is deﬁned by Tan[4] as the ability of an organization to maximise its potential to use digital evidence while mini- mizing the cost of an investigation. An organization has the ability to actively collect data that could be required during a DFI such as log ﬁles, emails, network trafﬁc records and other digital data that could be considered potential evidence should an incident occur. Implementing digital forensic readiness in a cloud environment can be achieved by setting a standard to which all of the organisations that use the speciﬁc cloud en- vironment have to conform to. Some researchers have already started looking at DFR such as the work of Rowlingson[11]. Rowlingson proposes ten-steps an organization should im- plement to comply to digital forensic readiness. Three of these steps will be addressed in the proposed solution, Step 2 - Identify available sources and different types of potential evidence. Step 4 - Establish a capability for securely gathering legally admissible evidence to meet the requirement. Step 5 - Establish a policy for secure storage and handling of potential evidence. These steps are applicable and can be implemented in a cloud environment. Baggili[12] identiﬁes a few challenges that digital forensic investigator are faced with in a cloud environment, we will address two of these challenges in our proposed solution: 1. Jurisdictional issues - this relates to who has the jurisdiction to investigate an international incident in a cloud environment 2. Decreased control over data and decreased access to forensic data from a client side. Supporting logs is important in the cloud as data and process provenance could be crucial in a DFI[13]. This data provides the investigators with meta-data regarding the history of digital objects. Tan[4] states that centralized logging is the key to efﬁcient forensic strategies. Logging data to a system other than itself better maintains the integrity of the data. This is exactly how we propose to do logging in our solution to preserve the integrity of the stored logs which could be used as potential evidence and quicken the acquisition phase by allowing easier access to the data. The reviewed literature in this section has shown that conducting a DFI in the cloud can be a very complex task and comes with a host of challenges. The process can be simpliﬁed by the application of digital forensic readiness in the cloud. However to apply DFR successfully to cloud environments require the collection of best evidence in such a manner that the integrity of the evidence is maintained throughout the process of collecting, transporting and storing of evidence. Based on the information provided in this section the author can compile a list of requirements for the proposed model. This will be looked at in the next section. III. MODEL FOR DIGITAL FORENSIC READINESS IN CLOUD COMPUTING ENVIRONMENTS From statistics collected by StatOwl[14] the Windows Op- erating System is the most widely used operating system on the market at the time of this writing, accounting for 84.69%. Therefore the focus of the development of the prototype for centralized logging is solely targeted towards the Windows Operating System. The subsections that follow is laid out as follows. Section 3.1 gives a brief overview of the requirements for the modal. Section 3.2 gives a overview of the environment in which the model will be implemented. Section 3.3 discuss the implemen- tation. A. Model Requirements Based on the literature review we have identiﬁed the follow- ing basic requirements that a DFR on the cloud should have. Communication Channel, Encryption, Compression, Authen- tication of log data and proof of integrity, Authenticating the client and server, and Timestamping. The communication channel is required to move data be- tween the client side and the server side software. Encryption is used to secure the communication to ensure it is not tampered with during transfer. Compression is used to package all of the backup log ﬁles into a single ﬁle to make transfer easier while compressing the data. The authentication of log data and proof of integrity proves that the data has not been tampered with, authenticating the client and server is required to ensure the log data came from the correct machine and was sent to the correct server. Timestamping is done to sync the clocks of the machines to avoid confusion during the presentation phase. The requirements of the two applications with regards to the DFI process is laid out below. Identiﬁcation - Determined pre-development Collection - Client application Transportation - Client and Server Application Storage - Server Application Examination - Independent task B. Proposed Model Figure 3.2.1 shows a brief layout of the cloud environment on which centralized logging will be applied. Figure 3.2.1 - Layout in an attempt The proposed solution implements remote and centralized to improve the integrity of stored logging, evidence. It also overcomes the jurisdictional issues that investigating teams could be faced with. Each organisation has a virtual machine sharing data and application with other organisations. Each virtual machine contains a local digital forensic module, this is the client module responsible for forwarding log evidence to a central server. The Windows Event Logs of each virtual machine in the hybrid cloud is backed up using the Win32 API, compressed and forwarded to a remote and central log server, where it is indexed and stored in a digital forensic ready manner. C. Implementing the proposed model The proposed model is implemented in a proof of concept prototype. The implementation of this prototype is discussed in this section. Both the client and server applications consist of a windows service. The subsections is discussed in the order of execution that they are required in the implementation. 3.3.1 Collection of log data Collection of event log data is accomplished by making use of the Advanced Windows 32 Base API system calls to OpenEventLog, BackupEventLog and CloseEventLog. 3.3.2 Compression Once all the log data has been collected and the meta-data computed the data is compiled into a single archive ﬁle, this archive is then encrypted and transmitted to the server. The implementation of this model achieved a compression rate of up to 87% compressing the original logs from 27.6 Mb to 3.7 Mb. 3.3.3 Communication Channel Because all communication occurring in the proposed model will be on a one-to-one bases between the client and the server the TCP protocol was selected as the communication channel. It is a connection orientated protocol which ensures better reliability than a connectionless connection such as UDP. This is an insecure connection therefore the data sent over the connection needs to be encrypted to maintain the conﬁdentiality and integrity of the data. 3.3.4 Encryption The primary purpose of encryption is to maintain the integrity and conﬁdentiality of the data. It also ensure that the data being transmitted is not tampered with during the transmission phase. In the implementation of this prototype the use of the AES-256 encryption scheme was selected. However AES is a symmetric encryption scheme that requires the client to provide the server with the key to decrypt the dataﬁle. To exchange this key the RSA asymmetric encryption scheme is used. In this research the AES and RSA algorithms was selected because it is the most popular schemes that guarantees conﬁdentiality and authenticity of data over an unsecured connection[15]. The RSA algorithm has large computational overhead due to the size of its key. Therefore the large data ﬁles are encrypted using AES which is a faster algorithm[16], and only the aes-key is encrypted with the RSA algorithm. Each time a client sends data to the server, an AES key is encrypted with the servers public key and sent to the server. 3.3.5 Authentication of log data and proof of integrity Authentication of the Windows Event Logs is accomplished by the computer-id ﬁeld in the event log itself. It is thus also a requirement to ensure the integrity of the log data is maintained so that this identiﬁcation can not be changed without being detected. A cryptographic hash function is to accomplish this goal as hash functions were designed speciﬁcally to protect the authenticity of information[17]. 3.3.6 Authenticating the client and server It is required that all evidence logged on the central log server be authenticated as original data. The Windows Event Logs may include a ﬁeld indicating the computer name where the data came from, but what if an attacker clones a system before attempting an attack and thereafter replaces the log evidence indicating the events that took place during the attack with clean logs from the cloned system? Therefore it is required that any client sending data to the server be authenticated at the server before storing any log evidence. To accomplish this the client and server communicates a difﬁe hellman shared secret. The client application signs the Difﬁe-Hellman shared secret using the RSA sign and verify functions. The signature is sent with the encrypted zip ﬁle to the server. The server veriﬁes the signature against the hash of its own shared secret to authenticate the client. 3.3.7 Authentication of log data and proof of integrity Each log ﬁle is hashed using the SHA-256 hash algorithm. The hash codes is saved in the meta-data.xml ﬁle with the names of each log. However this poses a problem, if the hash code is saved in plain-text anyone who changes the log-ﬁle can simply create a new hash of the ﬁle and overwrite the original in the meta data ﬁle. To overcome this problem the original hash of the log ﬁle is used as an encryption key to encrypt a salt value and the resulting cipher-text is then saved to the meta data ﬁle. The Validate.exe application veriﬁes the integrity of the log by computing the hash of the log and encrypting the same salt value. The integrity of the log is then veriﬁed by matching the corresponding cipher-text against the cipher-text provided in the meta data ﬁle. If a single byte in a log ﬁle is changed it does not go undetected. Figure 3.2.2 - Validation Figure 3.2.2 above shows the process of validating the log evidence. In this ﬁgure the program has detected that the HardwareEvents log has been changed since the collection time. 3.3.8 Timestamping Determining the time at which events took place is quite simple, the event logs contain a ﬁeld that logs the timestamp at which an event took place. This value of the logged ﬁeld however is determined by the date-time of the computer, set by the user. This presents a problem, the times on all the machines may not be synchronized. To overcome the problem of out-of-sync timestamps from different systems Tan[4] suggest the Network Time Protocol, RFC-5905[18], as the most efﬁcient protocol to achieving time synchronization on IP based systems, and we will use the same protocol in this research. There exists a policy in the windows platform disallowing the users from changing the data and time on the machine. In order for the synchronization of timestamps to be successful the policy must be implemented on the client machines. IV. DISCUSSION The primary objectives of this research can be summarised as follows: Shortening the DFI process by quickening the acquisition of data through the use of a remote and central log server. And in doing so improving the integrity of the log evidence[4]. The synchronization of timestamps and the ﬁltering and indexing of log evidence on the server has the goal of quickly identify where the logs came from and is a secondary objective. This model was designed after conducting a literature study to identify the gaps in existing solutions and based on this study the prototype was developed. The prototype captures log evidence and transport it to a remote and central log would allow investigators the opportunity to investigate attacks in the same way as live-forensics. Further more the acquisition of evidence data from the Windows Registry and Slack Space can be investigated. ",Scientific Journal,0
"As Second Avenue Subway Opens, a Train Delay Ends in (Happy) Tears","Finally. The Second Avenue subway opened in New York City on Sunday, with thousands of riders flooding into its polished stations to witness a piece of history nearly a century in the making. They descended beneath the streets of the Upper East Side of Manhattan to board Q trains bound for Coney Island in Brooklyn. They cheered. Their eyes filled with tears. They snapped selfies in front of colorful mosaics lining the walls of the stations. It was the first day of 2017, and it felt like a new day for a city that for so long struggled to build this sorely needed subway line. In a rare display of unbridled optimism from hardened New Yorkers, they arrived with huge grins and wide eyes, taking in the bells and whistles at three new stations. ‚ÄúI was very choked up,‚Äù Betsy Morris, 70, said as she rode the first train to leave the 96th Street station, at noon. ‚ÄúHow do you explain something that you never thought would happen? It‚Äôs going to change the way everybody lives as far as commuting goes. ‚Äù It was a major moment for New York‚Äôs sprawling transit system after decades of failed efforts to bring the line to one of the few corners of Manhattan the subway did not reach. The opening of the first segment of the line  ‚Äî   an extension of the Q train to 96th Street  ‚Äî   promises to lighten the crush of passengers on the Nos. 4, 5 and 6 trains along Lexington Avenue, the nation‚Äôs most overcrowded subway line, which had been the only line on the Upper East Side of Manhattan. When the stations opened shortly before noon, they were quickly filled with giddy riders both young and old, and strollers, suitcases and dogs  ‚Äî   all familiar sights across the system. But for all the excitement, the line, with just three new stops, is much more modest than the ambitious route running the length of Manhattan that was once envisioned. It serves a relatively affluent and not very diverse part of the city, which has more than eight and a half million people and many   and minority residents who live far from a subway line. With the subway reaching its highest ridership levels since 1948, much of the aging system is plagued by crowding and delays, even as subway and bus fares are expected to rise again in March. Still, there was reason to cheer. The opening of a new subway line is a rare occasion in the United States and comes at a time of mounting concern about the deteriorating state of the nation‚Äôs infrastructure, from its roadways and bridges to its public transit systems. Few new subway stations have opened in recent years, even as expansive subway networks have sprouted in Asia, and most American cities never built any in the first place. The major subways in the Northeast  ‚Äî   in New York, Washington and Boston  ‚Äî   are grappling with old equipment and funding shortfalls, with Washington experiencing a near meltdown over safety problems. With mounting bills for basic maintenance, these subways have largely failed to grow. So the arrival of the   Second Avenue subway, which was first proposed in the 1920s, was a notable achievement for the   Metropolitan Transportation Authority, which runs the city‚Äôs vast network of subways, buses and commuter railroads. The first phase of the project took nearly a decade to build and cost about $4. 4 billion. With the opening, the map of the city‚Äôs loved and loathed subway adds three new stations, bringing the total to 472  ‚Äî   the most of any subway in the world. A station that opened at Hudson Yards on Manhattan‚Äôs Far West Side in 2015 was the city‚Äôs first new station in a  . On Sunday, New Yorkers were mesmerized by the artwork adorning the walls. At the 72nd Street stop, Sumana Harihareswara stopped to gaze at a mosaic of a woman of South Asian descent dressed in a burgundy sari, looking at her cellphone. Ms. Harihareswara was overcome with emotion. ‚ÄúI don‚Äôt think I‚Äôve ever come across subway art before that makes me feel so seen,‚Äù she said through tears. ‚ÄúThis woman could be my aunt she could be my cousin. ‚Äù She and a stranger exchanged a knowing glance. ‚ÄúRepresentation matters,‚Äù they agreed. Ms. Harihareswara, a longtime transit enthusiast from Astoria, Queens, said she was struck by the diversity portrayed in the mosaics, including a mural of a gay couple holding hands. ‚ÄúThere is no feeling quite like seeing yourself cemented into the infrastructure of New York,‚Äù Ms. Harihareswara said. After decades of aborted efforts to build the Second Avenue line, and at least three groundbreakings in the 1970s, construction on the current segment began in 2007. The line was originally projected to open in 2013, but subway officials pushed the deadline to the end of 2016 many years ago. Gov. Andrew M. Cuomo, a Democrat who effectively controls the authority, pressed officials to meet the December 2016 deadline even as concerns grew that the subway would not be ready in time. Still, the agency made the deadline  ‚Äî   just barely  ‚Äî   with a lavish inaugural ride on New Year‚Äôs Eve for a collection of dignitaries that culminated in a midnight toast. Although many New Yorkers believe the city runs the subways, it is actually the governor who appoints the authority‚Äôs chairman and holds considerable sway over the agency. Mr. Cuomo has capitalized on the Second Avenue opening to raise his national profile, overshadowing his frequent nemesis Mayor Bill de Blasio, a Democrat who attended the inaugural ride on Saturday but was not given a speaking slot. Despite general good will over the opening, some transit advocates expressed concerns over the high cost of the project and questioned whether officials would move aggressively to extend the line to 125th Street in East Harlem as planned. On Sunday morning, Mr. Cuomo arrived at the 96th Street station with the authority‚Äôs chairman, Thomas F. Prendergast, to join the first trip for regular riders, who cheered as the train pulled out of the station. Then Mr. Cuomo‚Äôs voice came over the loudspeaker. ‚ÄúRest assured: I‚Äôm not driving the train,‚Äù he joked. The first day of service was smooth, although there were a few hiccups. Around 3 p. m. there were delays on the Q line because of a train with mechanical problems at the City Hall station. About an hour earlier, the elevator at the new 86th Street stop had begun to malfunction, stranding passengers above and below ground. Strollers were wheeled onto steep escalators. Parents became upset. Jill Tallmer, 62, and her mother, Margot Tallmer, 91, contemplated visiting another day. ‚ÄúWe‚Äôve been waiting for 10 years, or more, to ride,‚Äù the younger Ms. Tallmer said while standing with her mother, a lifelong New Yorker who is in a wheelchair. ‚ÄúHopefully, it‚Äôs almost ready for us. ‚Äù It was not, and they left after a few minutes. At the 72nd Street station, George Braith, a jazz saxophonist, was being mobbed by an eager pack of veritable paparazzi. The reason for his newfound celebrity: His likeness is featured in a mosaic there. ‚ÄúWould you look at that guy?‚Äù Mr. Braith, 77, said. ‚ÄúPretty handsome fellow if you ask me. ‚Äù He is one of several local celebrities portrayed in the artwork, including chef Daniel Boulud. In Mr. Braith‚Äôs mosaic, he is clad in a slick red blazer and carrying his signature Braithophone, alto and soprano saxophones melded into one. Taking the instrument from his suitcase, he obliged the crowd with a brief tune. ‚ÄúAre you famous?‚Äù a   asked, seeing the hubbub. ‚ÄúIn the jazz world,‚Äù Mr. Braith replied. The man shook his head and said, ‚ÄúWell, you‚Äôre immortalized as far as I‚Äôm concerned. ‚Äù Another   celebrant, Ian Ma, 15, lives in Sheepshead Bay, a waterfront neighborhood in southern Brooklyn that is nowhere near the new subway line. But he has been enchanted by trains since he started rolling toy models on the floor as a child, he said, and he cajoled his parents into giving him a ride. ‚ÄúI feel like I‚Äôve been waiting for this train my whole life,‚Äù he said, seemingly speaking for many others.",New York Times,0
"Modi‚Äôs Cash Ban Brings Pain, but Corruption-Weary India Grits Its Teeth","MUMBAI, India  ‚Äî   It was a bold and risky gamble by Prime Minister Narendra Modi of India that quickly seemed to backfire. The announcement of a ban on the largest currency bills circulating in India, which came into full effect at midnight Friday, the last day for depositing the old notes at banks, set off cash shortages that have hit the country‚Äôs most vulnerable people hard and prompted worries about the economy. But despite those concerns, as well as doubts about whether the currency ban will reduce corruption as it is designed to do, for the moment, at least, Mr. Modi‚Äôs bet appears to be paying off in the public arena. Even as the poorest Indians have struggled, many have continued to voice support for the prime minister‚Äôs initiative to target the vast amounts of untaxed money, known as ‚Äúblack money,‚Äù flowing through the country‚Äôs economy, in hopes that it will combat an endemic culture of corruption. Mr. Modi, analysts say, has successfully tapped into deep frustration with the corruption that pervades almost every public interface with government. ‚ÄúEven though the cash ban has produced enormous hardship for me and my family, I support what Mr. Modi is doing for our country,‚Äù said Hem Raj Chechi, 39, a taxi driver in New Delhi, the capital, who said business had been down 50 percent since the ban was announced. Mr. Chechi has not been able to pay his children‚Äôs school fees or send money back to his village to support them for nearly two months. But, he said, ‚ÄúWe need to fight black money, even though it is hurting little people like me. ‚Äù Mr. Modi came to power as a disruptive force pledging to overturn the status quo in New Delhi, bring jobs and fight corruption. Indians have repeatedly taken to the streets in recent years to demand an end to corruption, widely seen as being most detrimental to the poor and powerless. Declaring war on corruption, Mr. Modi announced on Nov. 8 that 500 and 1, 000 rupee bills, worth about $8 and $15, would be banned the next morning. With the currency ban, Mr. Modi has managed to convince many disaffected Indians that he is on their side. He has also used his powerful skills as a communicator to persuade people like Mr. Chechi that the pain stemming from the ban is for the   good. That frustration with a political and business elite viewed by many as corrupt is what drove many Americans to vote for   Donald J. Trump last year, said Eswar S. Prasad, an economics professor at Cornell University who is a native of India. ‚ÄúTrump made the case that only he could effect change by blowing up the system,‚Äù Mr. Prasad said in an interview. ‚ÄúModi, in the same way, did have a persuasive narrative that small changes at the margins can‚Äôt tackle   problems like corruption. We needed big and painful changes, really disruptive ones. ‚Äù Mr. Modi appeared on television on New Year‚Äôs Eve to tell Indians he understood their pain and urge them to bear with him in the aim of creating a better nation. He compared his cash ban to the freedom struggle led by Mohandas K. Gandhi, a battle of good versus evil. ‚ÄúToday Mahatma Gandhi is not among us, but the path that was truth that he showed us is still most appropriate,‚Äù Mr. Modi said, using the honorific for Gandhi. ‚ÄúAs we begin the centenary year of the Satyagraha,‚Äù he said, referring to Gandhi‚Äôs nonviolent resistance movement, ‚Äúlet us recall the Mahatma and resolve to follow his message of truth and goodness. ‚Äù Mr. Modi was presiding over one of the   major economies in the world when he announced the ban on the rupee notes, which made up 86 percent of the money in circulation. ‚ÄúIt‚Äôs a little bit crazy,‚Äù said Geng Xiao, a professor of finance and public policy at the University of Hong Kong. ‚ÄúWhen I first read the news, I couldn‚Äôt even figure out if it was true. ‚Äù The government said that people depositing large amounts of old currency  ‚Äî   more than 250, 000 rupees, or about $3, 670, could be asked to prove that they had paid taxes on it. Some saw vast amounts of untaxed holdings suddenly rendered worthless. Whether Mr. Modi‚Äôs   move will actually reduce corruption is a matter of debate. Some economists believe it could pave the way for other measures intended to discourage bribery or restructure the economy. But others say the currency ban is unlikely to result in a significant reduction in corruption, even as it has inconvenienced hundreds of millions of people who have struggled to get enough cash to meet their daily needs while the government tries to print enough new notes to replace the banned ones. And a country short of cash has been unable to spend it, which is likely to reduce economic growth in the short term, economists say. People living on the edge of poverty have been hit hard, their diets and livelihoods severely affected. Many in India see Mr. Modi, who has also had success persuading Parliament to clear the way for a unified    tax to make it easier to ship and sell things across state lines, as living up to his promise to be a reformer of India‚Äôs ossified and bureaucratic economy. But in the process, he has placed the Indian economy and his political future at risk. Many economists believe that the Indian economy will take at least a   hit because of a dire shortage of cash, and that the future impact is uncertain. If the effects are prolonged, the public support for Modi could wane. ‚ÄúIf this move damages the economy, he‚Äôs in for a rough ride,‚Äù said Harsh Pant, the head of strategic studies at the Observer Research Foundation, a New Delhi think tank. Mr. Xiao said that in a system in which the informal sector is substantial, the economy could be expected to experience a shock when so much cash is suddenly taken out. ‚ÄúYou need offsetting stimulus policies to keep the economy growing,‚Äù he said. Mr. Modi was elected by an overwhelming majority in 2014, defeating the government, led by the Indian National Congress party, that had ruled for most of the country‚Äôs   life, on the promise of bringing development and jobs and reducing corruption. ‚ÄúIn 2014, he had presented himself as the big disrupter,‚Äù Mr. Pant said. ‚ÄúHe was the precursor to what‚Äôs happening in the West. ‚Äù Mr. Modi was then the chief minister of the state of Gujarat, where he had a reputation for tearing away the country‚Äôs red tape so businesses could set up shop and expand. But during his first two years after winning national election, Mr. Modi, whose party did not control the upper house of Parliament, struggled to achieve a significant economic overhaul. Mr. Modi changed the political narrative in 2016 as he successfully fought to get Parliament to clear the way for the simplified    tax. He cast the opposition as ‚Äúpeople who don‚Äôt want change, who don‚Äôt want reform,‚Äù Mr. Pant said, forcing them in August to support the changes. But the challenges facing Mr. Modi remain considerable. Seven weeks after the cash ban began to be put in place, the currency shortage remains acute, leading to a sharp drop in demand for services and earnings, many providers said. Nagender Tiwari, 42, a rickshaw driver in East Delhi, said he was earning only 60 percent of the 1, 000 rupees, or about $15, that he used to take home daily before the cash ban. As a result, his family, which includes two children in 11th grade, has reduced its consumption of fish and meat. They have been unable to pay the rent on their home, he said. He said he was skeptical about whether the ban was reducing corruption, noting that he continued to be stopped by traffic police officers who extorted bribes. ‚ÄúSo if bribery is not stopped, how can black money be stopped?‚Äù he asked. Raj Kumar Bindal, 65, a paper trader in New Delhi, said sales that plummeted to nearly nothing in the days after the cash ban had returned to about half of what they were before. ‚ÄúWe can‚Äôt shift to a cashless mode overnight,‚Äù he said. Surjit Bhalla, a New   economic adviser for the Observatory Group in New York, said he believed Mr. Modi was likely to enact several other major changes in the coming months, possibly including a move to a simplified personal income tax. India needs to reduce the incentives for taxpayers to cheat, Mr. Bhalla said. In the United States, for every $100 collected in income tax, an estimated $20 owed is not paid, he added. In India, for that same $100 in taxes paid, $200 more is owed, he said. Collectively, the cash ban and other anticorruption initiatives have the potential to transform India, Mr. Pant and others said. But doing so depends on Mr. Modi‚Äôs continuing to command the political narrative of the country, as he has so successfully done since instituting the cash ban. ‚ÄúSo far, he has taken control of the narrative and the people are with him,‚Äù Mr. Pant said. ‚ÄúHe thinks he can continue to do it, but we really don‚Äôt know. There are so many unknowns. ‚Äù",New York Times,0
Failed weapons systems cost Pentagon $58 billion over two decades,"Failed weapons systems cost Pentagon $58 billion over two decades Published time: 26 Oct, 2016 19:47 Get short URL The Pentagon building in Washington, DC. ¬© AFP The Pentagon loves to throw good money after bad ‚Äí to the tune of nearly $60 billion on failed big-ticket weapons systems over the last two decades, according to a new internal Department of Defense review. 
From the Army‚Äôs Future Combat Systems (FCS) that focused on fighting the last war to its RAH-66 Comanche stealth helicopters that never quite got off the ground, between 1997 and October 2016, the Pentagon invested $58 billion on weapons technology it never received. That doesn‚Äôt include the boondoggle that is the F-35 jet , which was finally declared ‚Äúready for combat‚Äù at the beginning of August. 
The FCS ($20 billion) and the Comanche ($9.8 billion) are just two of 23 major weapons programs that were canceled before they were finished, and together the two Army projects made up more than 50 percent of the ‚Äúsunk costs‚Äù outlined in the Pentagon‚Äôs annual internal acquisitions performance review. The 224-page report by Undersecretary of Defense for Acquisition, Technology and Logistics Frank Kendall was published earlier this week. Read more 5 costly Pentagon projects of dubious merit 
The report noted how much money was spent on each canceled program, how far along in the process they were before they were killed, and if any of the technology was rolled up into new programs. For example, although the FCS was canceled, parts of it ‚Äí including many of the manned ground vehicles and the Intelligent Munitions System ‚Äí were swept up into a current program called the Army Brigade Combat Team Modernization Program. 
Most of the programs were killed before they blew through their budgets, but eight of them spent all the money allotted to them before the Pentagon canceled them, the report found. 
The Government Accountability Office, a Congressional watchdog, conducted an audit of Pentagon spending in 2011 and found $70 billion in waste, the New York Times reported at the time . Much of the overspending happened because the DOD started building weapons systems before the designs were fully tested, the auditors said. 
With acquisitions overruns long being a thorn in the side of the Pentagon‚Äôs budget, in March the Air Force enlisted IBM‚Äôs Jeopardy! -winning cognitive computer , Watson. Two contractors are currently working to create programs that would enable Watson to navigate the 1,897-page Federal Acquisition Regulation, helping potential government vendors actually bid for military contracts. The project is expected to become operational by 2018. 
Another way the Pentagon has sought to cut down wasted spending is through the latest update to its acquisitions program, called ‚ÄòBetter Buying Power 3.0‚Äô, which was announced in April 2015 . The program was designed to have ‚Äúa stronger emphasis on innovation, technical excellence, and the quality of our products,‚Äù Kendall wrote in a memo ordering the program's implementation. It calls on the military-industrial complex to make projects more affordable in terms of funding, schedule and manpower throughout the entire lifespan of their products. It will also reward contractors for successful expense management, and ask them to eliminate unproductive processes and unnecessary bureaucracy. 
Of course, holding contractors accountable for their failures when it comes to major cost overruns or weapons systems that don‚Äôt work is easier said than done. And it doesn‚Äôt help when someone at the Pentagon thinks it‚Äôs a good idea to spend money on bomb-sniffing elephants .",rt.com,1
"Mar-a-Lago, the Future Winter White House and Home of the Calmer Trump","WEST PALM BEACH, Fla.  ‚Äî   When   Donald J. Trump rang in the new year this weekend, he did it in   opulence, joined by the actor Sylvester Stallone, the gossip page fixture Fabio and a crowd of wealthy developers reveling under the swaying palm trees at Mr. Trump‚Äôs    resort in Palm Beach. President George W. Bush had his ranch in Crawford, Tex. His father had a compound in Kennebunkport, Me. President Obama has taken frequent vacations in Hawaii, staying at a private home. But Mr. Trump‚Äôs   private club in Florida, where he has spent the past two weeks away from his home in New York City, is likely to eclipse them all as the 45th president‚Äôs winter White House. And that was always the intention of Marjorie Meriweather Post, the cereal heiress and the property‚Äôs original owner, who left    to the federal government when she died in 1973, hoping it would serve as a home for presidents. But the government had no interest in her plan, and Mr. Trump later bought the property for less than $10 million, turning it into a club where membership costs six figures. Mr. Trump‚Äôs arrival was greeted with sneers by the Palm Beach elite, and he opened up   ‚Äôs membership to Jews and   who had been excluded from other   establishments. He was also the first club owner on the island to admit an openly gay couple. Since Mr. Trump‚Äôs victory in November,    has been stuffed with guests attracted by an amenity unique to this club: the chance to rub shoulders with the next president. ‚ÄúIt‚Äôs like going to Disneyland and knowing Mickey Mouse will be there all day long,‚Äù said Jeff Greene, a developer and unsuccessful Democratic candidate for the Senate from Florida in 2010, who is a    member and was a Hillary Clinton supporter. Instead of hosting major corporate executives and potential cabinet secretaries for interviews inside a boxy transition office at Trump Tower in Midtown Manhattan, Mr. Trump has been seated at an ornately designed couch, upholstered in pale fabric laced with gold, beneath a chandelier hanging from the ceiling, a scene resembling a mansion in ‚ÄúSunset Boulevard‚Äù or ‚ÄúCitizen Kane,‚Äù two of Mr. Trump‚Äôs favorite movies. At night, the couches are moved out and tables are added to accommodate the evening cocktail crowd, among whom Mr. Trump moves from one table to the next, the most powerful greeter in the world. At the annual New Year‚Äôs Eve party on Saturday night, a   white menu included ‚ÄúMr. Trump‚Äôs wedge salad,‚Äù a wild mushroom and Swiss chard ravioli and a ‚Äúbreakfast buffet. ‚Äù Those in attendance drifted in under a    striped awning, the men dressed in tuxedos, the women in ball gowns, many with their hair swept high. Guests stepped onto a red carpet as they entered the club and wandered over to a poolside cocktail party. Mr. Trump later delivered remarks, according to a guest, who said he thanked his family and the club members for their support over the years. Howie Carr, a conservative radio host who was supportive of Mr. Trump, roamed the crowd, with Mr. Carr posting on Twitter that his daughter asked Mr. Trump if she could be an intern in the White House. Mr. Trump‚Äôs two adult sons, Eric and Donald Jr. posed for photographs. Joe Scarborough and Mika Brzezinski from MSNBC‚Äôs ‚ÄúMorning Joe‚Äù were also there. Like most aspects of Mr. Trump‚Äôs business interests, the party generated controversy as tickets to it were made available to club members and guests for a little more than $500. Mr. Trump‚Äôs aides rejected the questions. Mr. Trump returned to New York on Sunday night. But the club will remain an escape for him. His contentious Twitter posts belie his relative calm when he is at    compared with when he is isolated inside Trump Tower. Mr. Trump‚Äôs combative public persona  ‚Äî   often on display during his campaign  ‚Äî   mostly dissolves behind the   walls of his castle. ‚Äú   is an environment he can control,‚Äù said the historian Douglas Brinkley, who last week attended a    lunch with a longtime club member, Chris Ruddy, the chief executive of Newsmax Media. ‚ÄúI watched him hold court  ‚Äî   he was so comfortable in his own skin, and so relaxed. ‚Äù Mr. Ruddy has hosted Mr. Carr and Laura Ingraham, another conservative radio host who supported Mr. Trump, at the club and has introduced Mr. Trump to a range of news media figures, politicians and donors. He described the   as ‚Äúseeking the   Donald Trump: totally at ease, very positive, very gregarious. ‚Äù Mr. Trump appears to feed off contact with the people at the club. Over the Thanksgiving holiday, he queried dinner guests about whether he should appoint Rudolph W. Giuliani or Mitt Romney as his secretary of state (he ended up picking neither). During this trip, he has heaped praise on his ultimate choice for the job, Rex W. Tillerson, the head of Exxon Mobil. (Mr. Trump has called him ‚ÄúMr. Exxon. ‚Äù) He talks about the work he has done to find a solution for the problems at the Department of Veteran Affairs, which included a recent meeting with a number of executives at   . Mr. Trump told a New York Times reporter that he intended to make Brian Burns, the businessman son of a confidante of Joseph P. Kennedy, the ambassador to Ireland. Isaac Perlmutter, the reclusive head of Marvel Entertainment, is a    member who helped Mr. Trump put that meeting together. Mr. Trump has also held   with Robert K. Kraft, the owner of the New England Patriots and a club member, and hosted prominent figures like Carlos Slim, the billionaire who is Mexico‚Äôs richest man. Mr. Trump his wife, Melania and their    son, Barron, inhabit a residential area of the club. His adult children and their families usually stay in nearby cabanas on the property. Mr. Trump frequently dines on the patio, a central point of action, where at night a singer plays with a small band, sometimes belting out requests from Mr. Trump and other guests. (‚ÄúMy Way,‚Äù a song popularized by Frank Sinatra, was one recent choice.) A violinist sometimes moves among tables, plucking tunes like the theme from ‚ÄúFiddler on the Roof. ‚Äù Mr. Trump has given the cadre of White House reporters who now cover him some access to the club, but grudgingly so  ‚Äî   he once again eluded the reporters covering him on Saturday, slipping away without any warning to play golf at another of his clubs nearby in Jupiter. And outside the confines of    old grievances flare up. On the golf course, Mr. Trump spotted Harry Hurt, a biographer who wrote critically of Mr. Trump years ago, preparing to play a round with David H. Koch, a billionaire conservative donor. Mr. Trump ordered club officials to remove Mr. Hurt from the property, according to a Facebook post by Mr. Hurt. Over the years, Mr. Trump has also been perpetually at loggerheads with Palm Beach officials. He has filed lawsuits attempting to keep noisy planes from flying over    and there have been disputes over the height of his oversize flagpole on the grounds. With its owner‚Äôs coming new job, the club has had some changes. Guests now go through an elaborate security screen to gain access to the main entrance. Secret Service agents are now sprinkled throughout the property, at night blending into the shrubbery along the grounds. Robin Bernstein, a club member for nearly 25 years, said that some club members might express frustration, but that most thought it was important ‚Äúthat we keep Donald and his family safe. ‚Äù Attendees seem to see a benefit so far in having the   around, and expect it will continue. ‚ÄúThe loser in this game is Camp David,‚Äù said Mr. Brinkley, referring to the longtime presidential retreat in Maryland. ‚ÄúOnce you‚Äôre at    and it‚Äôs so opulent and   the idea of suddenly inserting yourself into Camp David‚Äôs Maryland mountains environment seems unlikely. ‚Äù",New York Times,0
EPIC: New Yorker‚Äôs ‚ÄúHillary Themed‚Äù Haunted House Has Libs Absolutely Furious,"Before viewing the video, below, please be warned that its contents are truly disturbing. If you have children, have them avert their eyes so they aren‚Äôt forever scarred by the images of Clinton that decorate this person‚Äôs yard. 
This house in Bellmore!!! 
Posted by Brian Mc Kibbin on Thursday, October 20, 2016 
The video of the house, posted on Facebook by Brian McKibbin, has gone viral, garnering over 2.5 million views in less than a week. Advertisement - story continues below 
The house was decorated with signs calling Clinton a ‚Äútraitor,‚Äù‚Äúliar‚Äù and ‚Äúmurderer.‚Äù The house also featured several large displays encouraging people to vote for Republican nominee Donald Trump. 
It was unclear whether the person behind the decorations decided to do this for Halloween or just because he really doesn‚Äôt like Clinton . In any case, it really is an awesome display that works perfectly for both Halloween and the upcoming election. 
While this house might be scary, the real nightmare would be Clinton actually being elected. That‚Äôs why you need to get out and vote. If your state has early voting, go vote right away. 
We can laugh about this house‚Äôs anti-Hillary display all we want, but if she wins on Nov. 8, we won‚Äôt be laughing anymore. Advertisement - story continues below",conservativetribune.com,1
"Gianno Caldwell Claims Hillary Only Cares About Black Vote, Not Black Lives","BREAKING: Trump Set to Break 50-Year-Old Record With Black Voters 
‚ÄúBlack lives don‚Äôt matter to Hillary Clinton. Black votes matter to Hillary Clinton,‚Äù he said. 
In his speeches, Trump has promised black voters across America that he will actually work to improve their lives if he is elected. Trump often asks blacks, rhetorically, ‚ÄúWhat do you have to lose?‚Äù when asking for their vote. Advertisement - story continues below 
America‚Äôs inner cities are a mess, and poverty and crime are decimating black families across America. Decades of Democrat leadership in major cities have done nothing to improve the lives of millions of black Americans. 
What Trump is offering is an alternative ‚Äî a chance at a better future. All Clinton wants to do is promise more hope and change and then spend four years breaking her promises . 
Democrat solutions to inner city problems don‚Äôt work. Republicans may not be perfect, but they believe that every American can succeed if given the opportunity to do so. 
Trump wants to give every American a chance to succeed. He wants to rebuild the inner cities, bring back jobs and increase education so black Americans everywhere can escape the cycle of poverty and flourish. Advertisement - story continues below",conservativetribune.com,1
AT&T sold access to customer data to law enforcement ‚Äì report,"AT&T sold access to customer data to law enforcement ‚Äì report Published time: 26 Oct, 2016 18:10 Get short URL ¬© Shannon Stapleton / Reuters New documents show telecommunications giant AT&T sold customer data to local law enforcement departments for a record-making $100k to $1 million last year. The company is currently seeking a $85 billion acquisition deal for Time Warner. 
Documents show the telecommunications giant was not only working with US Drug Enforcement Agency but routinely sold customer data to local police departments who were investigating a range of crimes from murder to Medicaid fraud. The documents were first reported by The Daily Beast. AT&T provides the leads, then investigators just happen to find the exact same evidence through police work: https://t.co/d01Uc4iWYm ‚Äî The Daily Beast (@thedailybeast) October 25, 2016 
Under its secretive program called Hemisphere, AT&T could search trillions of call records and analyze cellular data to determine where a target is located, with whom they speak and potentially why. 
The documents also show the company only required an administrative subpoena, a lower-level legal document which ‚Äì unlike a search warrant ‚Äì does not require authorization from a judge. Police departments paid anywhere from $100,000 to $1 million a year for access to the Hemisphere program. Under the agreement, police were prohibited from disclosing use of the program to the public or even in court. 
‚ÄúLike other communications companies, if a government agency seeks customer call records through a subpoena, court order or other mandatory legal process, we are required by law to provide this non-content information, such as the phone numbers and the date and time of call,‚Äù AT&T told the Beast in a statement. Must read: AT&T spying on citizens to make millions mining data for police & govt surveillance w/o a warrant https://t.co/VDSqCwOqL7 pic.twitter.com/NWe1tjJC7W ‚Äî Anna Massoglia (@annalecta) October 25, 2016 
Because of the ban on disclosing the existence of the Hemisphere program, law enforcement agencies were covering their bases by later seeking a court order for a wiretap, or trailing a suspect to gather evidence equivalent to what they had acquired through Hemisphere. 
Unlike other telecommunications providers, AT&T stores metadata ‚Äì call time, duration, location data ‚Äì of its customers going back to 2008. AT&T was one of the first companies to be exposed for its role in government surveillance when it shared its customer‚Äôs metadata with the NSA. 
Mark Klein, a former AT&T technician and whistleblower, revealed details about the NSA installing network hardware at a San Francisco, California site known at Room 641A to monitor, capture and process American telecommunications, beginning in 2002. 
‚ÄúI knew this wasn‚Äôt legal because the NSA is not supposed to do domestic spying,‚Äù Klein told RT in an interview in 2015. He said the engineering documents showed ‚Äúthey were tapping into the main data flow on the internet and sending that data down to the secret room.‚Äù 
‚ÄúI knew that was totally illegal. The apparatus itself did not provide for any kind of selection it was just a vacuum cleaner sweep of everything. That violates the Fourth Amendment right there which requires warrants for specific information,‚Äù Klein said. 
AT&T was also known to have a ‚Äúpartnership‚Äù ‚Äì through its Hemisphere program ‚Äì with the DEA for the purpose of counter-narcotics operations. 
The revelations come as AT&T‚Äôs proposed $85 billion acquisition of Time Warner has surveillance critics and privacy advocates alarmed. 
Jeffrey Chester, executive director of the Center for Digital Democracy, told the Daily Beast he opposed the merger because it would allow AT&T ""to use Time Warner‚Äôs content as bait to invade every aspect of the lives and habits of its nearly 30 million wired customers of its broadband and television services ‚Äì many through its DirecTV subsidiary ‚Äì and its more than 100 million wireless subscribers."" 
‚ÄúIt‚Äôs commercial surveillance,‚Äù Chester told the Beast, noting that through mobile devices, AT&T can even pinpoint the geographical locations of users.",rt.com,1
Adding digital forensic readiness to electronic communication using a security monitoring tool ,"I. Spam is an inconvenience to electronic communication, but where is the harm? Since most anti-spam strategies are implemented either on the user’s mail box or on the company’s mail servers, the spam needs to be downloaded from an Internet Service Provider (ISP) before it can be scanned. This downloading has a direct impact on the bandwidth use of a company. The harmfulness of spam can H.S. Venter Department of Computer Science University of Pretoria Pretoria, South Africa hventer@cs.up.ac.za readiness thus be calculated in monetary value. The implementation of anti-spam strategies also has its own cost implications. Spam can clog up the electronic communication lines of a company to such an extent that there is a loss of service and therefore a loss of revenue. According to Microsoft (1) 95.4 % of all email delivered in the first half of 2010 to Microsoft Exchange® servers was blocked as spam. However, the problem is that blocking spam does not lead to any consequences for spammers. Employing digital forensic techniques to gather and analyze messaging information provides a new dimension to the fight against spam, which will lead to consequences for spammers. The authors investigated the possibilities of adding digital forensic communications by augmenting SMTP and IP. Adding digital forensic readiness to SMTP and IP did not solve the problem of spoofing. The paper introduces the design for a security monitoring system that adds digital forensic readiness to electronic communication. Probes are used to collect data from different areas of the electronic communication system. The data collected from the probes is stored for later digital forensic analysis. The remainder of the paper is constructed as follows; Section two gives backround information on SMTP, IP, IPSec, Spoofing and Digital Forensic Readiness. Section three looks at the addition of digital forensic information to SMTP and IP. Section four looks at the security monitering system section five is the conclusion of the paper, where future work that will be needed, is discussed. electronic to II. BACKGROUND SMTP, IP, IPSec, Spoofing and Digital Forensic Readiness are discussed in the background section. SMTP and IP are discussed to show the development of the different protocols. The discussion on spoofing shows the developments that were proposed to combat spoofing. The digital forensic readiness sections to make electronic communication digital forensically ready. A. Simple Message Transfer Protocol The Simple Mail Transfer Protocol, known as SMTP, was first proposed in the Request for Comments (RFC) 821 (2) in show what is needed August of 1982. RFC 1425 , (3) accepted in February 1993, described a way to extend the services SMTP offers, so that calling clients can ask what services are available on the server. SMTP Service Extensions are added to the core specification as the service extension becomes more popular. RFC 821 was made obsolete in April 2001 by RFC 2821 (4).The new specification included some service extensions and updates that were in use at the time. The latest RFC that describes SMTP is RFC 5321, released October 2008 (5). Information in the SMTP headers is stored in clear text. Therefore the information can easily be edited. The possibility that the mail headers could have been edited makes the information in the headers suspect. Editing of the mail headers is done to hide information, like the origin of the email, from the receiving email box. The action of editing the mail headers to hide information is called spoofing and is discussed later in the paper. SMTP describes a header called the trace header. The next section gives a short description of the SMTP trace header. B. SMTP trace header The trace header consists of two sub headers, return-path and received headers. The return path header is used to store the address where error reports should be sent. The received header stores the delivery path with a data stamp for each delivery entry. The format of the trace header is defined in RFC 5322 (6) as the trace rule. The trace rules are defined in Augmented Backus-Naur Form (ABNF) which is defined in STD0068 (7). The usage of the trace header is defined in RFC 5321 (5) for delivering error reports to the sender, as well as to create delivery reports that can be used as input information when doing trouble shooting. Klensin (5) proposes that the trace header should be made compulsory for all SMTP servers that implement the RFC 5321 standard. The augmentation that will be discussed later is defined for the received header, therefore only the received rule for the received header is discussed. The received rule indicates that the received header must start with the word “Received” followed by a possible empty list of received-tokens. The list of received-tokens is followed by a date-time stamp and a Carriage Return Line Feed (CRLF) character that indicates the end of the received header entry. The received-token rule indicates that the received-token must start with the word “from” followed by the address of the sending host. The received-token ends with the word “by”, followed by the receiving host’s address. Section C gives a short overview of the Internet Protocol (IP) and the Security Architecture for the Internet Protocol (IPSec). C. Internet Protocol Internet Protocol (IP) version 4 (IPv4) was first published in RFC 791 (8). RFC 2460 (9), published in December 1998, described the next generation Internet Protocol, IP version 6. IPv4 is still widely in use, although some network devices now support IPv4 and IPv6 and the ability to convert between the two protocol versions. The specification for IPv4 and IPv6 describes an optional header called the Routing header. Inside the routing header there is an option called the record route. The record route option is updated for each datagram, by every router that the datagram passes through. The length of the Routing Header is set by the origin. When the maximum length is reached, no more addresses are added but an ICMP parameter problem message is sent to the origin of the message. The Security Architecture for the Internet Protocol (IPSec) provides security functions for IPv4 and IPv6. IPSec was published in August 1995 in RFC 1825 (10) and describes the security services offered and how these services can be employed in the IP environment. The specification discusses the use of the IP Authentication header (AH), as defined in RFC 1826 (11) and the IP Encapsulating Security Payload (ESP), as described in RFC 1827 (12). IPSec was upgraded in November 1998, with the publication of RFC 2401 (9). Section D discusses email spoofing. D. Spoofing Email spoofing is the act of editing or falsifying the SMTP header information to hide the true origin or root of an email (13). Spoofing is also used to add fake validity to the content of an email by using a well known and trusted domain as the originating domain in order to perpetrate a phishing attack. RFC 4406 (14) is an experimental RFC that describes two tests for SMTP servers to perform, to verify that a mail header has not been spoofed. The first test is the Purported Responsible Address (PRA) test (15). Lyon (15) describes a way to try and find the PRA inside the SMTP headers. If no PRA can be found, the email has a high probability of being spoofed. If the PRA can be established, it is still not proof that the SMTP header has not been spoofed, since the address used for the PRA, is the first well formed address the PRA algorithm found. The PRA needs to be tested further, to establish its validity. The second test uses a Sender Policy Framework (SPF) (16) to authenticate if a SMTP client is allowed to act on behalf of the originating domain. Wong (16) proposes the SPF as a method to detect a spoofed email that uses valid domain information to appear legitimate. The supposed sender domain and the routing information in the header is authenticated by the DNS of the domain owner, to determine if the SMTP client’s domain has the authority to act on behalf of the supposed sending domain. If the DNS returns a failed authentication, the email is marked as possibly spoofed. RFC 4871 (17) proposes Domain Key Identified Mail (DKIM) Signatures. DKIM defines a domain-level process that domain owners can use to verify that a message that was supposedly sent by the domain owner was actually sent by the domain owner. The verification process uses public-key cryptography and key sever technology to authenticate the message sender. Section E discusses digital forensic readiness. E. Digital Forensic Readiness Digital forensic science is a relatively new field of study that evolved from forensic science. According to the Oxford Dictionary (18), digital forensic science is the systematic derived proven methods towards gathering of information about electronic devices, that can be used in a court of law. Digital forensic science is more popularly called digital forensics and sometimes also called computer forensics. Palmer (19) defines digital forensics as “the use of scientifically the preservation, collection, validation, identification, analysis, interpretation, documentation and presentation of digital evidence derived from digital sources for the purpose of facilitation or furthering the reconstruction of events”. Palmer’s definition describes the digital forensic process whereas Oxford describes digital forensic science. The Digital Forensic Process Model (DFPM) by Kohn, et al. (20) states that “any digital forensic process must have an outcome that is acceptable by law”. Rowlingson (21) defines digital readiness as consisting of two objectives. The first objective is to maximise the environment’s capability of collecting digital forensic information and the second objective is to minimize the cost of a forensic investigation. Preparing any enviroment to be digital forensically ready, a mechanism will need to be added to preserve, collect and validate the information contained in the enviroment. The the enviroment can then be used as part of a digital forensic investigation. The remainder of the paper elaborates on the research conducted by the authors. Section three discusses the addition of digital forensic information to the Simple Message Transfer Protocol (SMTP) and the Internet Protocol (IP). Section four discusses the design of the security monitoring system. information gathered forensic from III. ADDING DIGITAL FORENSIC READINESS TO SMTP AND IP This section discusses two approaches to adding digital forensic readiness to electronic communications, to show the previous work done. Although the addition of the validated token and the messaging header did not produce the desired results as originally anticipated by the authors, we feel that it is still an important discussion to include in the paper because it leads to our reasoning for introducing the security monitoring system as discussed in section four. Section A and B discuss the augmentation of the SMTP trace header with a validated token and the issues that were identified. Section C and D discuss the addition of the messaging header to IP and the issues that were encountered. A. Augmenting the SMTP trace header Adding digital forensic information to SMTP requires the addition of a hash value to verify the trace information. According to the background discussion of section II.E, digital forensic information must be verifiable. Looking at the trace header, the best place to store the hash value would be the received header. Two hash values are added to the original received rule. The first hash value appears after the “from” line that contains the value of the sending domain hashed together with the DNS lookup IP of the sending domain. The second hash value appears after the “by” line that contains the value of the host’s domain hashed together with the DNS lookup IP address of the host’s domain. Creation of the hash value is done using MD5 hashing to preserve the integrity of the received-token. The received header was changed to contain at least one and only one received-token entry per line. The received header must contain at least one received token so that the digital forensic information is always present. Only one received- token entry is allowed per line to simplify the information extraction process. Section B discusses the issues encountered with the addition of the hash value to the received header. B. Issues with augmenting the SMTP trace header SMTP is an open protocol, which means that the information contained in the headers are human readable and easy to edit. The authors wanted to keep the token creation process as simple as possible, so that it fits with SMTP. The simple hash value creation process is easy to be copied and therefore does not give protection against spoofing. The authors could not find a simple way of safeguarding the token information in the header but decided that safeguarding the information would have added too much complexity to SMTP. There was, therefore, no way to ensure that the tokens were not removed during message transportation. Thus the authors proved that it is possible to add digital forensic information to the received header although the information could not be safeguarded and therefore not truly trusted. Section C discusses the addition of digital forensic readiness by adding a hash token to the Internet Protocol. C. Adding the Messaging header to IP The digital forensic information is captured in a new optional header called the messaging header. Hash data is added to the routing header to create the messaging header. Information in the messaging header is updated by each router before the message is sent to the next router. At the origin, the messaging header is loaded with the origins address and a hash value. Generating the hash value is done by using the MD5 hashing function. Initially the hash value is generated using the origin and destination address. After the initial entry, every hash value is generated using the sending router address and the receiving router address. The output is reconfigured according to the IP version of the datagram. If the datagram is IPv4 the hash value is reduced from 128 bits to 32 bits. If the datagram is IPv6, the hash value is kept at 128 bits. Before a router forwards the message, the message header is loaded with the current router’s address and the hash value. When a message is delivered to the final host, the IP header and messaging header information is saved for further processing. If a message is received without the messaging header, the message is marked with a high spam probability. Section D discusses the issues that were encountered by adding the messaging header to IP. D. Issues encountered when adding the messaging header The size of the messaging header could become a problem. The hash value is 32 bits for IPv4 and 128 bits for IPv6. If a message is routed through 15 hops, the number of added bits is 960 bits for IPv4 and 3840 bits for IPv6. Depending on the frame size for a given network, the size of the messaging header might mean that it becomes detrimental to the efficiency of the transportation layer. The presence of the messaging header is the only indication, to the routers, that the packets being sent belong to a message. The messaging header cannot be restarted during routing if the messaging header is removed. Unless the messaging header is created at the origin, the messaging header will not exist when the message is delivered. All the routers in the route must be set up to maintain the message header. The need for all the routers to maintain the messaging header implies a change to all routers. According to (22) the rate of user adoption is directly proportional to the perceived value that is gained from the adoption. Since the addition of the messaging header cannot guarantee that the message is not spam, the cost might still be perceived as too high. IV. ADDING DIGITAL FORENSIC READINESS USING A SECURITY MONITORING SYSTEM The security monitoring system makes use of different types of probes that collect data and send it to the security monitoring system to be stored. Each probe communicates with the monitoring system using private key encryption so that the probe data is secure and verifiable. The rest of the section discusses the DNS probe, network probe, SMTP probe and the anti-spam probe. A. The DNS probe The DNS probe collects data with regard to DNS requests for the SMTP server address. A record is made containing the address of the requesting server and the date-time that the request was received. It is assumed that the requesting server will use the correct address as the origin of a DNS request because the requested information must be returned to the requesters address. The next section discusses the network probe. B. The network probe The network probe collects data on all the SMTP traffic that passes through the network core. Data generated by the network probe contains the sender and receiver address of each SMTP interaction. A date-time stamp is added to the data set to enable data sets to be matched to data sets collected by the SMTP probe. The next section discusses the SMTP probe. C. The SMTP probe Data surrounding the activity of the SMTP servers are collected by the SMTP probe. The data set that is recorded contains the sender and receiver address as well as a date- timestamp. Data sets from the SMTP probe and the network probe have the same data structure. The next section discusses the anti-spam probe. D. The anti-spam probe The anti-spam probe collects data about spam that have been filtered by anti-spam software. Anti-spam data is generated by stripping the SMTP information, like the sender and receiver address, and storing it with a timestamp of when the spam was received. Information like the anti-spam rating of the email is also stored but in a different data set. E. Using the data The data collected will be used forensic investigations of the electronic communication system. The data can also be analyzed to find known or new patterns that indicate some sort of event. An example would be detecting SMTP traffic on the network probe that cannot be detected on the SMTP probe might indicate a secondary SMTP server on the network. It could also indicate that there might be a botnet operating on the network. Matching the data from the different probes will at least give us a better understanding of how the electronic communication systems are used. in digital V. CONCLUSION The paper discussed previous work of the authors to add digital forensic readiness to electronic communication. The previous work was not as successful because the proposed solutions could not combat spoofing. Trying to change the way SMTP and IP worked was not a cost effective idea. The authors decided to look into a way to still add digital forensic readiness to electronic communication that would not rely on changing SMTP or IP. The paper proposes a probe based security monitoring system that collects digital data about the electronic communication system. Future work will include implementing the monitoring system and analyzing the data to see what can truly be learnt from the data. ACKNOWLEDGMENT to I would like to take this opportunity to express my respectful thanks to my supervisor Prof Venter for his continued input and guidance during this project. I would also like to thank my wife Isobel van Huyssteen for her help, support and patience. Lastly I offer my gratitude the National Research Foundation and the University of Pretoria for the opportunity that was afforded me. This work is based on research supported by the National Research Foundation of South Africa (NRF) as part of a SA/Germany Research cooperation programme. Any opinion, findings and conclusions or recommendations expressed in this material are those of the author(s) and therefore the NRF does not accept any liability in regard thereto. ",Scientific Journal,0
Email Pandemonium and the Perpetuation of Rape Culture,"By Robert Franek on Sun, Oct 30th, 2016 at 9:01 am Imagine if working to end rape culture were treated with the same attention as Hillary Clinton‚Äôs emails. This is one fire that is burning strong and is a true threat to personal and public safety, security, and well-being. Share on Twitter Print This Post 
The following post, written by The Rev. Robert A. Franek, is a part of Politicus Policy Discussion, in which writers draw connections between real lives and public policy. 
Even after months and months of coverage blown out of proportion and at great expense to the public in dollars, psychic stress, and issue coverage ,it seems all one has to do is put Hillary Clinton and emails together in the same sentence with claims of lying and threatening national security and pandemonium continues to break loose throughout the mass media and social media worlds. And yet despite multiple investigations and repeated explanations the pandemonium persists, so much so that every time Republicans stir up some smoke the media assumes there must be a blazing fire. And sadly this is not going away anytime soon, as Republicans are promising more sham investigations in lieu of actually governing as Sarah Jones reported. 
Republicans are already planning how to avoid being grown ups who do their jobs if Hillary Clinton is elected president. Yes, there will be more toy-throwing and tantrums and best of all for the party of fiscal humiliations, more wasteful spending on political witch hunts. ‚Äì Sarah Jones 
Meanwhile, women continue to come forward with painful, heartbreaking stories of Donald Trump‚Äôs sexual harassment and assault. Yet, instead of being portrayed as survivors who have finally had enough of the Republican presidential nominee and his lies and as women who have the strength and courage to go public with their stories, they are being made into opportunistic victims and challenged for not coming forward sooner much sooner. Woefully, no mention is made of the many intersecting reasons that lead many women (and men) not to report these crimes, including that this is the only time when the victim is placed on trial. 
After walking her readers through one incident of Donald Trump‚Äôs public sexual humiliation of a woman for revenge , Sarah Jones makes the following observation: 
If there is any good to come out of the total crapfest of the Trump candidacy, perhaps it is a raised awareness that women are people and that this kind of thing is horrific but it‚Äôs not all Trump‚Äôs fault. It‚Äôs the culture‚Äôs fault because it takes a willing audience to successfully publicly sexually shame a woman. 
She is right. Donald Trump is despicable. Still, there is cultural culpability in the perpetuation of rape culture where women are treated as objects, forced into silence, and placed on trial for their assailant‚Äôs crimes. 
This must change. 
It is unacceptable in a country that places such a high value on freedom that so many are not free from sexual harassment, assault, exploitation, and humiliation. More it is appalling that such behavior is often cheered, unchallenged, and dismissed. It is also inexcusable that survivors face unparalleled levels of scrutiny when they do finally come forward. 
It is also beyond tragic that people of faith who read the first chapter of Genesis can‚Äôt bear out the implications of men and women being created simultaneously and bearing equally the divine image. 
Supposed Christian values champion and Republican Vice Presidential nominee, Mike Pence is more upset over an article he didn‚Äôt read regarding voter suppression efforts by the Trump campaign than any of the horrifying things Donald Trump has said including bragging about sexual assault. 
These are not Christian values nor are they values any civil society should hold. Failing to call out Donald Trump‚Äôs abhorrent behavior and speech is beyond deplorable. Additionally, supporting candidates like Donald Trump and Mike Pence gives validity to their wretched views and is especially shocking when done by people of faith and ‚Äúfamily values.‚Äù 
As a society, we must challenge the pervasive and systemic sexism and misogyny that persists in our culture. As much as we need changes in our laws (and lawmakers!) at every level of government to reflect the equality and humanity of women, we need a moral revival in our collective conscience as a nation that decries every facet of rape culture from how we raise our children to the victim blaming and shaming that happens each time a survivor goes public with their story. 
Imagine if working to end rape culture were treated with the same attention as Hillary Clinton‚Äôs emails. This is one fire that is burning strong and is a true threat to personal and public safety, security, and well-being. 
Email Pandemonium and the Perpetuation of Rape Culture added by Robert Franek on Sun, Oct 30th, 2016",politicususa.com,1